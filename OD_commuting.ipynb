{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f42f59",
   "metadata": {},
   "source": [
    "##### Merge February Weekday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd89410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Set the directory containing the files and the output file name\n",
    "directory = 'destination'\n",
    "output_file = 'combined_february_weekdays.csv'\n",
    "\n",
    "# 2. Generate dates for Monday to Thursday in February 2025\n",
    "# pd.Timestamp.dayofweek: Monday=0, Tuesday=1, Wednesday=2, Thursday=3\n",
    "dates = pd.to_datetime(pd.date_range(start='2025-02-01', end='2025-02-28'))\n",
    "weekdays_to_process = dates[dates.dayofweek.isin([0, 1, 2, 3])]\n",
    "\n",
    "print(f\"Files to be processed for these dates: {[d.strftime('%Y-%m-%d') for d in weekdays_to_process]}\")\n",
    "\n",
    "# 3. Flag to ensure the CSV header is written only once\n",
    "header_written = False\n",
    "\n",
    "# 4. Iterate over each date to process\n",
    "for date in weekdays_to_process:\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Build the full file path\n",
    "    file_name = f'Audience_Profiles_Destination_Hex_{date_str}_UK.tsv.gz'\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Check if the file exists; skip if not found\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found, skipped: {file_path}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read large files in chunks; adjust chunksize based on available memory\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            file_path, \n",
    "            sep='\\t', \n",
    "            compression='gzip', \n",
    "            chunksize=100000  \n",
    "        )\n",
    "        \n",
    "        # Iterate over each chunk in the file\n",
    "        for chunk in chunk_iterator:\n",
    "            # Filter rows where TIME_INTERVAL is 8 or 9\n",
    "            filtered_chunk = chunk[chunk['TIME_INTERVAL'].isin([7, 8, 9])]\n",
    "            \n",
    "            # If the filtered chunk is not empty, save it\n",
    "            if not filtered_chunk.empty:\n",
    "                # Write header only for the first write\n",
    "                if not header_written:\n",
    "                    filtered_chunk.to_csv(output_file, index=False, mode='w')\n",
    "                    header_written = True\n",
    "                # Subsequent writes: append mode without header\n",
    "                else:\n",
    "                    filtered_chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# 5. Print final information after completion\n",
    "if header_written:\n",
    "    print(f\"\\nDone! Merged data saved to '{output_file}'\")\n",
    "else:\n",
    "    print(\"\\nDone. No data matching the filter was found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# File to analyze\n",
    "input_file = 'combined_february_weekdays.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: File '{input_file}' not found.\")\n",
    "    print(\"Please make sure you have successfully run the previous merge code.\")\n",
    "else:\n",
    "    print(f\"Analyzing file: '{input_file}'...\")\n",
    "    \n",
    "    # Use Counter object for efficient counting\n",
    "    day_type_counts = Counter()\n",
    "    \n",
    "    # Set a reasonable chunk size\n",
    "    chunksize = 100000\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file in chunks, only reading the 'DAY_TYPE' column to save memory\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            input_file, \n",
    "            usecols=['DAY_TYPE'], \n",
    "            chunksize=chunksize\n",
    "        )\n",
    "        \n",
    "        # Iterate over each chunk and update the counts\n",
    "        for chunk in chunk_iterator:\n",
    "            # .value_counts() returns a Series; convert to dict and update total counts\n",
    "            day_type_counts.update(chunk['DAY_TYPE'].value_counts().to_dict())\n",
    "            \n",
    "        # Convert the final Counter result to a more readable pandas Series\n",
    "        final_counts = pd.Series(day_type_counts).sort_values(ascending=False)\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nStatistics for 'DAY_TYPE' column:\")\n",
    "        print(\"--------------------------\")\n",
    "        print(final_counts)\n",
    "        print(\"--------------------------\")\n",
    "        print(f\"\\nThere are {len(final_counts)} unique DAY_TYPE values.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_february_weekdays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a65f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# File path\n",
    "file_path = 'combined_february_weekdays.csv'\n",
    "\n",
    "print(\"Creating DataFrame with Dask...\")\n",
    "\n",
    "# This step completes immediately and uses very little memory\n",
    "df = dd.read_csv(file_path)\n",
    "\n",
    "print(\"Dask DataFrame is ready!\")\n",
    "print(\"You can use the df object just like a pandas DataFrame.\")\n",
    "\n",
    "# 1. View the first 5 rows (Dask only reads a small part of the file)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'combined_february_weekdays.csv'\n",
    "\n",
    "# Specify correct dtypes for each column\n",
    "dtype_specification = {\n",
    "    'CODE': 'object',\n",
    "    'CITY': 'object',\n",
    "    'ORIGIN_CODE': 'object',\n",
    "    'TIME_INTERVAL': 'object',\n",
    "    'DAY_TYPE': 'object',\n",
    "    'MOVEMENT_MODALITY': 'object',\n",
    "    'VISITATION_MODALITY': 'object',\n",
    "    'YEAR': 'float64',\n",
    "    'MONTH': 'float64',\n",
    "    'DAY': 'float64'\n",
    "    # The EXTRAPOLATED columns are numeric; Dask usually infers them correctly.\n",
    "}\n",
    "\n",
    "print(\"Reading file with corrected dtype specification...\")\n",
    "\n",
    "# Use the final, correct dtype specification in dd.read_csv\n",
    "df = dd.read_csv(\n",
    "    file_path,\n",
    "    dtype=dtype_specification\n",
    ")\n",
    "\n",
    "print(\"File loaded successfully. All column dtypes are specified correctly!\")\n",
    "\n",
    "# You can run .head() to confirm\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a15e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02c1b8",
   "metadata": {},
   "source": [
    "##### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e902be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# --- 1. Analysis by City (Text Summary Only) ---\n",
    "print(\"\\n--- Analysis 1: Total Users by City ---\")\n",
    "city_agg = df.groupby('CITY')['EXTRAPOLATED_NUMBER_OF_USERS'].sum()\n",
    "print(\"Computing total users per city...\")\n",
    "city_users_summary = city_agg.compute().sort_values(ascending=False)\n",
    "\n",
    "# Create a text summary instead of a plot\n",
    "total_users = city_users_summary.sum()\n",
    "top_city_name = city_users_summary.index[0]\n",
    "top_city_users = city_users_summary.iloc[0]\n",
    "bottom_city_name = city_users_summary.index[-1]\n",
    "bottom_city_users = city_users_summary.iloc[-1]\n",
    "\n",
    "print(\"\\n--- City Analysis Summary ---\")\n",
    "print(f\"A total of {total_users:,.0f} users were analyzed across {len(city_users_summary)} cities.\")\n",
    "print(f\"Top city by user count: '{top_city_name}' with {top_city_users:,.0f} users.\")\n",
    "print(f\"Bottom city by user count: '{bottom_city_name}' with {bottom_city_users:,.0f} users.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 2. Analysis by Day of the Month (Enhanced Plot) ---\n",
    "print(\"\\n--- Analysis 2: Total Users by Day of the Month ---\")\n",
    "day_agg = df.groupby('DAY')['EXTRAPOLATED_NUMBER_OF_USERS'].sum()\n",
    "print(\"Computing total users per day...\")\n",
    "day_users_summary = day_agg.compute().sort_index()\n",
    "\n",
    "# Visualization\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax_day = sns.lineplot(\n",
    "    x=day_users_summary.index,\n",
    "    y=day_users_summary.values,\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    color='coral'\n",
    ")\n",
    "plt.title('Total Extrapolated Users by Day of the Month', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day of the Month', fontsize=12)\n",
    "plt.ylabel('Total Users', fontsize=12)\n",
    "plt.xticks(day_users_summary.index)\n",
    "\n",
    "# Format y-axis to show numbers in millions (e.g., 1.5M)\n",
    "def format_millions(x, pos):\n",
    "    return f'{x / 1_000_000:.1f}M'\n",
    "ax_day.yaxis.set_major_formatter(FuncFormatter(format_millions))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3. Analysis by Day of the Week (Enhanced Plot) ---\n",
    "print(\"\\n--- Analysis 3: Total Users by Day of the Week ---\")\n",
    "print(\"Computing necessary columns for weekday analysis...\")\n",
    "weekday_df = df[['YEAR', 'MONTH', 'DAY', 'EXTRAPOLATED_NUMBER_OF_USERS']].compute()\n",
    "\n",
    "try:\n",
    "    # Perform date operations using pandas\n",
    "    weekday_df['full_date'] = pd.to_datetime(weekday_df[['YEAR', 'MONTH', 'DAY']])\n",
    "    weekday_df['weekday'] = weekday_df['full_date'].dt.day_name()\n",
    "    weekday_users_summary = weekday_df.groupby('weekday')['EXTRAPOLATED_NUMBER_OF_USERS'].sum()\n",
    "\n",
    "    # Order the results correctly\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_users_summary = weekday_users_summary.reindex(weekday_order).fillna(0)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax_weekday = sns.barplot(\n",
    "        x=weekday_users_summary.index,\n",
    "        y=weekday_users_summary.values,\n",
    "        palette='plasma'\n",
    "    )\n",
    "    plt.title('Total Extrapolated Users by Day of the Week', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Day of the Week', fontsize=12)\n",
    "    plt.ylabel('Total Users', fontsize=12)\n",
    "\n",
    "    # Add data labels on top of each bar\n",
    "    for p in ax_weekday.patches:\n",
    "        ax_weekday.annotate(\n",
    "            f\"{p.get_height() / 1_000_000:.2f}M\", # Format as millions\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center',\n",
    "            xytext=(0, 9),\n",
    "            textcoords='offset points',\n",
    "            fontsize=11,\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "    # Adjust y-axis limit to make space for labels\n",
    "    ax_weekday.set_ylim(0, weekday_users_summary.max() * 1.1)\n",
    "    # Hide y-axis ticks and labels as the bar annotations are sufficient\n",
    "    ax_weekday.yaxis.set_major_formatter(FuncFormatter(lambda x, p: \"\"))\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during weekday analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c0ea0",
   "metadata": {},
   "source": [
    "###### View the distribution for each time interval over 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'combined_february_weekdays.csv' \n",
    "chunk_size = 1_000_000 \n",
    "\n",
    "# 2. Initialize an empty Series to store the cumulative resultries to store the cumulative result\n",
    "# ----------------------------------------------------------------------\n",
    "total_signals = pd.Series(dtype='float64')\n",
    "\n",
    "# 3. Create a chunk iterator and process each chunkterator and process each chunk\n",
    "# ---------------------------------------------------\n",
    "# pd.read_csv with chunksize returns an iteratorn iterator\n",
    "chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "print(\"Starting chunked file processing...\")ked file processing...\")\n",
    "\n",
    "for i, chunk in enumerate(chunk_iterator):\n",
    "    print(f\"  Processing chunk {i+1} ...\")} ...\")\n",
    "    \n",
    "    # Group by TIME_INTERVAL and sum within the current chunkIME_INTERVAL and sum within the current chunk\n",
    "    chunk_sum = chunk.groupby('TIME_INTERVAL')['EXTRAPOLATED_NUMBER_OF_USERS'].sum()\n",
    "    \n",
    "    # Add the current chunk's result to the cumulative totalt chunk's result to the cumulative total\n",
    "    total_signals = total_signals.add(chunk_sum, fill_value=0)\n",
    "\n",
    "print(\"\\nAll chunks processed!\")rocessed!\")\n",
    "\n",
    "# 4. Convert the final result to a DataFrame and displaylt to a DataFrame and display\n",
    "# -------------------------------------------------------------\n",
    "final_result_df = total_signals.reset_index()s.reset_index()\n",
    "final_result_df.columns = ['TIME_INTERVAL', 'TOTAL_EXTRAPOLATED_SIGNALS']TOTAL_EXTRAPOLATED_SIGNALS']\n",
    "\n",
    "print(\"\\nFinal aggregation result:\")\n",
    "print(final_result_df)print(final_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ee2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 2: Create the Visualization ---\n",
    "\n",
    "# For a more effective academic chart, sort the data from highest to lowest.\n",
    "df_sorted = final_result_df.sort_values(by='TOTAL_EXTRAPOLATED_SIGNALS', ascending=False)\n",
    "\n",
    "# Set a professional and clean style for the plot.\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure and axes for the plot. A larger figure size is better for readability.\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(\n",
    "    x='TIME_INTERVAL',\n",
    "    y='TOTAL_EXTRAPOLATED_SIGNALS',\n",
    "    data=df_sorted,\n",
    "    palette='viridis' # A perceptually uniform color palette is good for publications.\n",
    ")\n",
    "\n",
    "# Add a clear title and labels in English.\n",
    "ax.set_title('Total Extrapolated Signals per Time Interval', fontsize=18, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Time Interval', fontsize=14)\n",
    "ax.set_ylabel('Total Signals (in Millions)', fontsize=14)\n",
    "\n",
    "# Format the y-axis to be more readable (e.g., show \"5.2M\" instead of \"5200000\").\n",
    "# This enhances the academic look of the chart.\n",
    "def format_millions(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    return f'{x / 1_000_000:.1f}M'\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_millions))\n",
    "\n",
    "# Rotate the x-axis labels to prevent them from overlapping.\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Ensure all plot elements fit nicely within the figure area.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the chart as a high-resolution image file, suitable for reports or papers.\n",
    "plt.savefig('academic_signals_chart_english.png', dpi=300)\n",
    "\n",
    "print(\"Visualization saved as 'academic_signals_chart_english.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c611b",
   "metadata": {},
   "source": [
    "##### Remove abnormal date (Feb 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "# --- Step 1: Define correct dtypes for loading data ---\n",
    "# This step is necessary to avoid dtype errors\n",
    "file_path = 'combined_february_weekdays.csv'\n",
    "\n",
    "dtype_specification = {\n",
    "    'CODE': 'object',\n",
    "    'CITY': 'object',\n",
    "    'ORIGIN_CODE': 'object',\n",
    "    'TIME_INTERVAL': 'object',\n",
    "    'DAY_TYPE': 'object',\n",
    "    'MOVEMENT_MODALITY': 'object',\n",
    "    'VISITATION_MODALITY': 'object',\n",
    "    'YEAR': 'float64',\n",
    "    'MONTH': 'float64',\n",
    "    'DAY': 'float64'\n",
    "}\n",
    "\n",
    "print(\"Step 1: Defining how to load data from CSV...\")\n",
    "df = dd.read_csv(\n",
    "    file_path,\n",
    "    dtype=dtype_specification\n",
    ")\n",
    "print(\"Loading definition completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a19a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Define filtering operation ---\n",
    "print(\"Step 2: Defining filter condition (DAY != 17)...\")\n",
    "df_cleaned = df[df['DAY'] != 17]\n",
    "print(\"Filter definition completed.\\n\")\n",
    "\n",
    "# --- Step 3: Execute 'read-filter-write' pipeline ---\n",
    "output_filename = 'cleaned_data_without_day_17.csv'\n",
    "\n",
    "if os.path.exists(output_filename):\n",
    "    os.remove(output_filename)\n",
    "    print(f\"Step 3: Old file '{output_filename}' deleted.\")\n",
    "\n",
    "print(f\"Ready to start safe streaming write to '{output_filename}'... (This may take some time, please be patient)\")\n",
    "\n",
    "# This loop is key to ensure the task succeeds without crashing\n",
    "for i, partition in enumerate(df_cleaned.to_delayed()):\n",
    "    # Compute current chunk\n",
    "    chunk_df = partition.compute()\n",
    "    print(f\"  -> Processing and writing partition {i+1}...\")\n",
    "    # Write to file\n",
    "    if i == 0:\n",
    "        chunk_df.to_csv(output_filename, index=False, mode='w', header=True)\n",
    "    else:\n",
    "        chunk_df.to_csv(output_filename, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"\\nProcessing complete! One-off task succeeded, data saved to: '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an iterator to read 100,000 rows at a time\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_csv('cleaned_data_without_day_17.csv', chunksize=chunk_size)\n",
    "\n",
    "df_list = []\n",
    "for chunk in chunks:\n",
    "    # You can preprocess each chunk here if needed\n",
    "    # process(chunk)\n",
    "    df_list.append(chunk)\n",
    "\n",
    "df_cleaned = pd.concat(df_list, axis=0)\n",
    "del df_list  # Free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_counts = df_cleaned['DAY'].value_counts()\n",
    "\n",
    "# Print the occurrence count for each unique value\n",
    "print(\"Occurrence count for each unique value:\")\n",
    "print(day_counts)\n",
    "\n",
    "# If you only need the total number of unique values, calculate the length of this Series\n",
    "unique_day_count = len(day_counts)\n",
    "\n",
    "print(f\"\\nThere are {unique_day_count} unique values in the 'DAY' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abf146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column exists, then count unique values and print in descending order\n",
    "if 'EXTRAPOLATED_NUMBER_OF_USERS' in df_cleaned.columns:\n",
    "    value_counts = df_cleaned['EXTRAPOLATED_NUMBER_OF_USERS'].value_counts()\n",
    "    print(value_counts)\n",
    "else:\n",
    "    print(\"Error: 'EXTRAPOLATED_NUMBER_OF_USERS' column not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3c176",
   "metadata": {},
   "source": [
    "##### TTWA OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm # For progress bar display (optional)\n",
    "\n",
    "# --- Parameter Settings ---\n",
    "# Please replace the filenames with your actual files\n",
    "OD_DATA_FILE = 'cleaned_data.csv'\n",
    "TTWA_BOUNDARY_FILE = 'Boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson'\n",
    "MAPPING_FILE = 'h3_to_ttwa_mapping.csv'\n",
    "FINAL_OD_MATRIX_FILE = 'ttwa_od_matrix.csv'\n",
    "TARGET_CRS = \"EPSG:27700\" # Target projection: British National Grid\n",
    "\n",
    "# --- Part 1: Create H3 Index to TTWA Mapping File ---\n",
    "\n",
    "def create_h3_to_ttwa_mapping():\n",
    "    \"\"\"\n",
    "    Run only once to generate the mapping file from H3 cells to TTWA names.\n",
    "    \"\"\"\n",
    "    print(\"--- Part 1: Creating H3 to TTWA Mapping ---\")\n",
    "    \n",
    "    # 1. Read TTWA boundary data\n",
    "    print(\"Step 1/6: Reading TTWA boundary file...\")\n",
    "    ttwa_gdf = gpd.read_file(TTWA_BOUNDARY_FILE)\n",
    "    ttwa_gdf = ttwa_gdf[['TTWA11NM', 'geometry']]\n",
    "    ttwa_gdf.rename(columns={'TTWA11NM': 'TTWA_NAME'}, inplace=True)\n",
    "\n",
    "    # 2. Read all unique H3 cells from the large OD data\n",
    "    print(\"Step 2/6: Reading unique H3 indices from OD data file...\")\n",
    "    unique_h3_indices = set()\n",
    "    for chunk in tqdm(pd.read_csv(OD_DATA_FILE, usecols=['CODE', 'ORIGIN_CODE'], chunksize=1000000), desc=\"Reading H3s\"):\n",
    "        unique_h3_indices.update(chunk['CODE'].unique())\n",
    "        unique_h3_indices.update(chunk['ORIGIN_CODE'].unique())\n",
    "    \n",
    "    unique_h3_list = list(unique_h3_indices)\n",
    "    print(f\"Found {len(unique_h3_list)} unique H3 indices.\")\n",
    "\n",
    "    # 3. Convert H3 cells to geographic points (WGS 84)\n",
    "    print(\"Step 3/6: Converting H3 indices to geographic points (WGS 84)...\")\n",
    "    points = [Point(h3.cell_to_latlng(h)[::-1]) for h in unique_h3_list]\n",
    "\n",
    "    # 4. Create GeoDataFrame for H3 points\n",
    "    h3_gdf = gpd.GeoDataFrame(\n",
    "        {'h3_index': unique_h3_list},\n",
    "        geometry=points,\n",
    "        crs=\"EPSG:4326\" # Specify initial CRS as WGS 84\n",
    "    )\n",
    "\n",
    "    # *** Code modification section ***\n",
    "    # 5. Project both GeoDataFrames to British National Grid (BNG)\n",
    "    print(f\"Step 4/6: Projecting both layers to {TARGET_CRS}...\")\n",
    "    h3_gdf_projected = h3_gdf.to_crs(TARGET_CRS)\n",
    "    ttwa_gdf_projected = ttwa_gdf.to_crs(TARGET_CRS)\n",
    "    \n",
    "    # 6. Perform spatial join on projected data\n",
    "    print(\"Step 5/6: Performing spatial join on projected data (this may take a while)...\")\n",
    "    h3_with_ttwa = gpd.sjoin(h3_gdf_projected, ttwa_gdf_projected, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "    # 7. Save mapping file\n",
    "    print(f\"Step 6/6: Saving mapping to '{MAPPING_FILE}'...\")\n",
    "    # Only non-geometric columns are needed\n",
    "    mapping_df = h3_with_ttwa[['h3_index', 'TTWA_NAME']]\n",
    "    mapping_df.to_csv(MAPPING_FILE, index=False)\n",
    "    \n",
    "    print(\"--- Part 1 Finished ---\")\n",
    "\n",
    "# --- Part 2: Use Mapping File to Chunk Process OD Data and Generate OD Matrix ---\n",
    "# (No modification needed here, as it does not handle geometry)\n",
    "def generate_od_matrix_from_mapping():\n",
    "    \"\"\"\n",
    "    Efficiently process large OD data using the mapping file.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Part 2: Generating OD Matrix using Mapping ---\")\n",
    "\n",
    "    # 1. Load mapping file as dictionary\n",
    "    print(\"Step 1/4: Loading H3-to-TTWA mapping...\")\n",
    "    mapping_df = pd.read_csv(MAPPING_FILE)\n",
    "    h3_to_ttwa_map = pd.Series(mapping_df.TTWA_NAME.values, index=mapping_df.h3_index).to_dict()\n",
    "\n",
    "    # 2. Prepare to process OD data in chunks\n",
    "    print(\"Step 2/4: Processing large OD file in chunks...\")\n",
    "    chunk_results = []\n",
    "    cols_to_use = ['CODE', 'ORIGIN_CODE', 'EXTRAPOLATED_NUMBER_OF_USERS']\n",
    "    \n",
    "    for chunk in tqdm(pd.read_csv(OD_DATA_FILE, usecols=cols_to_use, chunksize=1000000), desc=\"Aggregating Flows\"):\n",
    "        chunk['origin_ttwa'] = chunk['ORIGIN_CODE'].map(h3_to_ttwa_map)\n",
    "        chunk['destination_ttwa'] = chunk['CODE'].map(h3_to_ttwa_map)\n",
    "        chunk.dropna(subset=['origin_ttwa', 'destination_ttwa'], inplace=True)\n",
    "        \n",
    "        aggregated_chunk = chunk.groupby(['origin_ttwa', 'destination_ttwa'])['EXTRAPOLATED_NUMBER_OF_USERS'].sum()\n",
    "        chunk_results.append(aggregated_chunk)\n",
    "\n",
    "    # 3. Combine results from all chunks\n",
    "    print(\"Step 3/4: Aggregating results from all chunks...\")\n",
    "    if not chunk_results:\n",
    "        print(\"No valid OD pairs found after mapping. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    final_aggregation = pd.concat(chunk_results)\n",
    "    final_od_pairs = final_aggregation.groupby(['origin_ttwa', 'destination_ttwa']).sum()\n",
    "\n",
    "    # 4. Pivot to OD matrix format and save\n",
    "    print(f\"Step 4/4: Pivoting to OD matrix and saving to '{FINAL_OD_MATRIX_FILE}'...\")\n",
    "    od_matrix = final_od_pairs.unstack(level='destination_ttwa', fill_value=0)\n",
    "    od_matrix.to_csv(FINAL_OD_MATRIX_FILE)\n",
    "    \n",
    "    print(\"--- Part 2 Finished ---\")\n",
    "    print(f\"Final OD Matrix saved to {FINAL_OD_MATRIX_FILE}\")\n",
    "\n",
    "\n",
    "# --- Main Program Execution ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    create_h3_to_ttwa_mapping()\n",
    "    \n",
    "    # Run Part 2 to generate the final OD matrix\n",
    "    generate_od_matrix_from_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55c1ab",
   "metadata": {},
   "source": [
    "###### Same as above, but adds TTWA-to-TTWA distance calculation and filters out flows over 100km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0905fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# --- Parameter Settings ---\n",
    "OD_DATA_FILE = 'cleaned_data_without_day_17.csv'\n",
    "TTWA_BOUNDARY_FILE = 'boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson'\n",
    "\n",
    "# --- Intermediate and Final File Names ---\n",
    "MAPPING_FILE = 'h3_to_ttwa_mapping.csv'\n",
    "RAW_OD_LIST_FILE = 'ttwa_od_list_raw.csv'  # Now generates a list instead of a matrix\n",
    "DISTANCE_MATRIX_FILE = 'ttwa_distance_matrix_km.csv'\n",
    "FINAL_FILTERED_LIST_FILE = 'ttwa_od_list_filtered_100km.csv' # Final output is also a list\n",
    "\n",
    "TARGET_CRS = \"EPSG:27700\"\n",
    "\n",
    "# --- Part 1: Create H3 Index to TTWA Mapping File ---\n",
    "# (Function code is identical to previous version)\n",
    "def create_h3_to_ttwa_mapping():\n",
    "    print(\"--- Part 1: Creating H3 to TTWA Mapping ---\")\n",
    "    # ... (Code identical to previous version, omitted for brevity) ...\n",
    "    ttwa_gdf = gpd.read_file(TTWA_BOUNDARY_FILE)[['TTWA11NM', 'geometry']].rename(columns={'TTWA11NM': 'TTWA_NAME'})\n",
    "    unique_h3_indices = set()\n",
    "    for chunk in tqdm(pd.read_csv(OD_DATA_FILE, usecols=['CODE', 'ORIGIN_CODE'], chunksize=1000000), desc=\"Reading H3s\"):\n",
    "        unique_h3_indices.update(chunk['CODE'].unique())\n",
    "        unique_h3_indices.update(chunk['ORIGIN_CODE'].unique())\n",
    "    points = [Point(h3.cell_to_latlng(h)[::-1]) for h in list(unique_h3_indices)]\n",
    "    h3_gdf = gpd.GeoDataFrame({'h3_index': list(unique_h3_indices)}, geometry=points, crs=\"EPSG:4326\")\n",
    "    h3_gdf_projected = h3_gdf.to_crs(TARGET_CRS)\n",
    "    ttwa_gdf_projected = ttwa_gdf.to_crs(TARGET_CRS)\n",
    "    h3_with_ttwa = gpd.sjoin(h3_gdf_projected, ttwa_gdf_projected, how=\"inner\", predicate=\"within\")\n",
    "    h3_with_ttwa[['h3_index', 'TTWA_NAME']].to_csv(MAPPING_FILE, index=False)\n",
    "    print(f\"--- Part 1 Finished: Mapping saved to '{MAPPING_FILE}' ---\")\n",
    "\n",
    "\n",
    "# --- Part 2: Generate Inter-TTWA OD \"List\" ---\n",
    "def generate_inter_ttwa_od_list():\n",
    "    \"\"\"\n",
    "    Modified function: now generates a long-format OD list instead of a matrix.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Part 2: Generating Inter-TTWA OD List ---\")\n",
    "    h3_to_ttwa_map = pd.read_csv(MAPPING_FILE, index_col='h3_index')['TTWA_NAME'].to_dict()\n",
    "    chunk_results = []\n",
    "    cols_to_use = ['CODE', 'ORIGIN_CODE', 'EXTRAPOLATED_NUMBER_OF_USERS']\n",
    "    for chunk in tqdm(pd.read_csv(OD_DATA_FILE, usecols=cols_to_use, chunksize=1000000), desc=\"Aggregating Flows\"):\n",
    "        chunk['origin_ttwa'] = chunk['ORIGIN_CODE'].map(h3_to_ttwa_map)\n",
    "        chunk['destination_ttwa'] = chunk['CODE'].map(h3_to_ttwa_map)\n",
    "        chunk.dropna(subset=['origin_ttwa', 'destination_ttwa'], inplace=True)\n",
    "        chunk = chunk[chunk['origin_ttwa'] != chunk['destination_ttwa']]\n",
    "        if not chunk.empty:\n",
    "            chunk_results.append(chunk.groupby(['origin_ttwa', 'destination_ttwa'])['EXTRAPOLATED_NUMBER_OF_USERS'].sum())\n",
    "    \n",
    "    if not chunk_results:\n",
    "        print(\"No valid inter-TTWA pairs found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    final_od_pairs = pd.concat(chunk_results).groupby(['origin_ttwa', 'destination_ttwa']).sum().reset_index()\n",
    "    final_od_pairs.rename(columns={'EXTRAPOLATED_NUMBER_OF_USERS': 'TOTAL_FLOW'}, inplace=True)\n",
    "    \n",
    "    final_od_pairs.to_csv(RAW_OD_LIST_FILE, index=False)\n",
    "    print(f\"--- Part 2 Finished: Raw OD List saved to '{RAW_OD_LIST_FILE}' ---\")\n",
    "\n",
    "\n",
    "# --- Part 3: Calculate and Save TTWA Distance Matrix ---\n",
    "# (Function code is identical to previous version)\n",
    "def calculate_ttwa_distance_matrix():\n",
    "    \"\"\"\n",
    "    Calculates and saves the TTWA distance matrix.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Part 3: Calculating and Saving TTWA Distance Matrix ---\")\n",
    "    # ... (Code identical to previous version, omitted for brevity) ...\n",
    "    ttwa_gdf = gpd.read_file(TTWA_BOUNDARY_FILE)[['TTWA11NM', 'geometry']].rename(columns={'TTWA11NM': 'TTWA_NAME'})\n",
    "    ttwa_gdf_projected = ttwa_gdf.to_crs(TARGET_CRS)\n",
    "    ttwa_gdf_projected.set_index('TTWA_NAME', inplace=True)\n",
    "    ttwa_gdf_projected['centroid'] = ttwa_gdf_projected.geometry.centroid\n",
    "    coords = pd.DataFrame({'x': ttwa_gdf_projected.centroid.x, 'y': ttwa_gdf_projected.centroid.y})\n",
    "    dist_matrix_m = pd.DataFrame(squareform(pdist(coords)), index=coords.index, columns=coords.index)\n",
    "    dist_matrix_km = dist_matrix_m / 1000\n",
    "    dist_matrix_km.to_csv(DISTANCE_MATRIX_FILE)\n",
    "    print(f\"--- Part 3 Finished: Distance Matrix saved to '{DISTANCE_MATRIX_FILE}' ---\")\n",
    "\n",
    "\n",
    "# --- Part 4: Merge Flow List and Distance, Filter by Distance ---\n",
    "def create_final_filtered_edgelist():\n",
    "    \"\"\"\n",
    "    Refactored function: merges flow list and distance data, filters, and saves as final long-format file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Part 4: Creating Final Filtered Edge List ---\")\n",
    "    \n",
    "    # 1. Read raw flow list and distance matrix\n",
    "    print(\"Step 1/4: Reading raw OD list and distance matrix...\")\n",
    "    raw_od_list = pd.read_csv(RAW_OD_LIST_FILE)\n",
    "    dist_matrix_km = pd.read_csv(DISTANCE_MATRIX_FILE, index_col=0)\n",
    "\n",
    "    # 2. Convert distance matrix from wide to long format for merging\n",
    "    print(\"Step 2/4: Melting distance matrix for merging...\")\n",
    "    dist_long = dist_matrix_km.stack().reset_index()\n",
    "    dist_long.columns = ['ORIGIN_TTWA', 'DEST_TTWA', 'DISTANCE_KM']\n",
    "\n",
    "    # 3. Merge flow data and distance data\n",
    "    print(\"Step 3/4: Merging flows with distances...\")\n",
    "    # Left merge, using flow list as base\n",
    "    merged_data = pd.merge(\n",
    "        raw_od_list,\n",
    "        dist_long,\n",
    "        left_on=['origin_ttwa', 'destination_ttwa'],\n",
    "        right_on=['ORIGIN_TTWA', 'DEST_TTWA'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Clean up merged columns\n",
    "    final_data = merged_data[['origin_ttwa', 'destination_ttwa', 'TOTAL_FLOW', 'DISTANCE_KM']]\n",
    "    final_data.rename(columns={'origin_ttwa': 'ORIGIN_TTWA', 'destination_ttwa': 'DEST_TTWA'}, inplace=True)\n",
    "\n",
    "    # 4. Filter by 100km distance\n",
    "    print(\"Step 4/4: Filtering by 100km distance and saving...\")\n",
    "    filtered_list = final_data[final_data['DISTANCE_KM'] <= 100].copy()\n",
    "    \n",
    "    # Sort by flow for easier viewing\n",
    "    filtered_list.sort_values(by='TOTAL_FLOW', ascending=False, inplace=True)\n",
    "\n",
    "    filtered_list.to_csv(FINAL_FILTERED_LIST_FILE, index=False)\n",
    "    print(f\"--- Part 4 Finished: Final filtered list saved to '{FINAL_FILTERED_LIST_FILE}' ---\")\n",
    "\n",
    "\n",
    "# --- Main Program Execution Flow ---\n",
    "if __name__ == '__main__':\n",
    "    # It is recommended to run one function at a time in order, and comment it out after success.\n",
    "    \n",
    "    # 1. Create H3->TTWA mapping file (run only once)\n",
    "    #create_h3_to_ttwa_mapping()\n",
    "    \n",
    "    # 2. Generate raw inter-TTWA OD list (run only once)\n",
    "    #generate_inter_ttwa_od_list()\n",
    "    \n",
    "    # 3. Generate TTWA distance matrix (run only once)\n",
    "    #calculate_ttwa_distance_matrix()\n",
    "    \n",
    "    # 4. Merge data and filter by distance, generate final file\n",
    "    create_final_filtered_edgelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a610a",
   "metadata": {},
   "source": [
    "##### Percolaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Calculate Dependency Weights\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ðŸš€ Starting percolation analysis based on filtered data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Read filtered data and calculate dependency weights\n",
    "def calculate_dependency_weights(df):\n",
    "    \"\"\"\n",
    "    Calculate dependency weights w_ij = t_ij / Î£_j t_ij\n",
    "    where t_ij is the commuting flow from i to j\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“Š Calculating dependency weights...\")\n",
    "    \n",
    "    # Calculate total outflow for each origin city\n",
    "    total_outflow = df.groupby('ORIGIN_TTWA')['TOTAL_FLOW'].sum()\n",
    "    \n",
    "    # Calculate dependency weights\n",
    "    df['dependency_weight'] = df.apply(\n",
    "        lambda row: row['TOTAL_FLOW'] / total_outflow[row['ORIGIN_TTWA']], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"   Dependency weight range: {df['dependency_weight'].min():.6f} - {df['dependency_weight'].max():.6f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read filtered data (after 100km filter)\n",
    "df = pd.read_csv('ttwa_od_list_filtered_100km.csv')\n",
    "df_with_weights = calculate_dependency_weights(df)\n",
    "\n",
    "print(f\"âœ… Data overview: {len(df)} OD records, {df['ORIGIN_TTWA'].nunique()} origin cities\")\n",
    "print(f\"   Dependency weights calculated!\")\n",
    "\n",
    "# Validate correctness of weights\n",
    "weight_sums = df_with_weights.groupby('ORIGIN_TTWA')['dependency_weight'].sum()\n",
    "print(f\"   Weight validation: All origin cities have weights summing close to 1.0 = {np.allclose(weight_sums, 1.0)}\")\n",
    "\n",
    "# Show top 5 connections with highest dependency weights\n",
    "print(f\"\\nðŸŽ¯ Top 5 connections by dependency weight:\")\n",
    "top_deps = df_with_weights.nlargest(5, 'dependency_weight')[['ORIGIN_TTWA', 'DEST_TTWA', 'TOTAL_FLOW', 'dependency_weight']]\n",
    "for i, (_, row) in enumerate(top_deps.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['ORIGIN_TTWA']} â†’ {row['DEST_TTWA']}: {row['dependency_weight']:.4f}\")\n",
    "\n",
    "df_with_weights.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build directed commuting network\n",
    "def build_commuting_network(df_weights):\n",
    "    \"\"\"Build a directed commuting network\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges, weight is dependency weight\n",
    "    for _, row in df_weights.iterrows():\n",
    "        G.add_edge(\n",
    "            row['ORIGIN_TTWA'], \n",
    "            row['DEST_TTWA'], \n",
    "            weight=row['dependency_weight'],\n",
    "            flow=row['TOTAL_FLOW']\n",
    "        )\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = build_commuting_network(df_with_weights)\n",
    "print(f\"Network size: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perform_percolation_analysis(G):\n",
    "    \"\"\"\n",
    "    Perform percolation analysis on the given commuting network G.\n",
    "\n",
    "    This function iterates over a series of thresholds (tau), at each threshold\n",
    "    retaining only edges with weight greater than or equal to tau, then identifies\n",
    "    the resulting city clusters (weakly connected components), and computes the size\n",
    "    of the largest cluster.\n",
    "\n",
    "    Args:\n",
    "        G (nx.DiGraph): A directed graph where edge 'weight' attribute represents dependency weight.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing cluster information, count, and giant component size for each threshold.\n",
    "    \"\"\"\n",
    "    # Step 1: Get all unique dependency weights as thresholds, sorted in descending order\n",
    "    # In the literature, thresholds decrease from high to low, so we sort descending [cite: 134]\n",
    "    weights = sorted(\n",
    "        list(set(nx.get_edge_attributes(G, 'weight').values())), \n",
    "        reverse=True\n",
    "    )\n",
    "    # Add 0 to ensure the network is fully connected at the end\n",
    "    if 0 not in weights:\n",
    "        weights.append(0)\n",
    "\n",
    "    print(f\"Starting percolation analysis with {len(weights)} unique thresholds...\")\n",
    "\n",
    "    percolation_results = []\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Step 2: Iterate over all thresholds\n",
    "    for i, tau in enumerate(weights):\n",
    "        # Create a subgraph containing only edges with weight >= tau [cite: 116]\n",
    "        sub_G = nx.DiGraph()\n",
    "        # Add all nodes in advance to handle isolated nodes at some thresholds\n",
    "        sub_G.add_nodes_from(G.nodes())\n",
    "        edges_to_add = [(u, v, d) for u, v, d in G.edges(data=True) if d['weight'] >= tau]\n",
    "        sub_G.add_edges_from(edges_to_add)\n",
    "\n",
    "        # Find connected components (clusters). For directed graphs, weakly connected components match the cluster concept in the literature.\n",
    "        clusters = list(nx.weakly_connected_components(sub_G))\n",
    "        \n",
    "        # Compute the size of the giant component [cite: 255]\n",
    "        if clusters:\n",
    "            giant_component_size = max(len(c) for c in clusters)\n",
    "        else:\n",
    "            giant_component_size = 0\n",
    "        \n",
    "        # Store results for this threshold\n",
    "        percolation_results.append({\n",
    "            'threshold': tau,\n",
    "            'num_clusters': len(clusters),\n",
    "            'clusters': clusters, # Store actual cluster node sets\n",
    "            'giant_component_size': giant_component_size,\n",
    "            'giant_component_size_ratio': giant_component_size / total_nodes if total_nodes > 0 else 0\n",
    "        })\n",
    "        \n",
    "    # Convert results list to DataFrame for further analysis\n",
    "    return pd.DataFrame(percolation_results)\n",
    "\n",
    "# --- Run analysis ---\n",
    "# Use the previously created network G\n",
    "df_percolation = perform_percolation_analysis(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume G already exists\n",
    "\n",
    "def perform_percolation_analysis(G):\n",
    "    \"\"\"\n",
    "    Perform percolation analysis on the given commuting network G.\n",
    "    \"\"\"\n",
    "    # Step 1: Get all unique dependency weights as thresholds, sorted in descending order [cite: 134]\n",
    "    weights = sorted(\n",
    "        list(set(nx.get_edge_attributes(G, 'weight').values())), \n",
    "        reverse=True\n",
    "    )\n",
    "    if 0 not in weights:\n",
    "        weights.append(0)\n",
    "\n",
    "    print(f\"Starting percolation analysis with {len(weights)} unique thresholds...\")\n",
    "\n",
    "    percolation_results = []\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Step 2: Iterate over all thresholds\n",
    "    for i, tau in enumerate(weights):\n",
    "        # Create a subgraph containing only edges with weight >= tau [cite: 116]\n",
    "        sub_G = nx.DiGraph()\n",
    "        sub_G.add_nodes_from(G.nodes())\n",
    "        edges_to_add = [(u, v, d) for u, v, d in G.edges(data=True) if d['weight'] >= tau]\n",
    "        sub_G.add_edges_from(edges_to_add)\n",
    "\n",
    "        # Find connected components (clusters)\n",
    "        clusters = list(nx.weakly_connected_components(sub_G))\n",
    "        \n",
    "        # Compute the size of the giant component [cite: 255]\n",
    "        if clusters:\n",
    "            giant_component_size = max(len(c) for c in clusters)\n",
    "        else:\n",
    "            giant_component_size = 0\n",
    "        \n",
    "        percolation_results.append({\n",
    "            'threshold': tau,\n",
    "            'num_clusters': len(clusters),\n",
    "            'clusters': clusters,\n",
    "            'giant_component_size': giant_component_size,\n",
    "            'giant_component_size_ratio': giant_component_size / total_nodes if total_nodes > 0 else 0\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(percolation_results)\n",
    "\n",
    "# --- Run analysis ---\n",
    "df_percolation = perform_percolation_analysis(G)\n",
    "\n",
    "# --- Identify critical points ---\n",
    "df_percolation = df_percolation.sort_values('threshold', ascending=False).reset_index(drop=True)\n",
    "df_percolation['size_increase'] = df_percolation['giant_component_size'].diff().fillna(0)\n",
    "\n",
    "non_zero_increases = df_percolation[df_percolation['size_increase'] > 0]['size_increase']\n",
    "\n",
    "if not non_zero_increases.empty:\n",
    "    # Define a jump threshold, e.g., 70th percentile\n",
    "    jump_threshold = non_zero_increases.quantile(0.7) \n",
    "    \n",
    "    # Filter rows where a \"jump\" occurs, get a DataFrame\n",
    "    critical_transitions_df = df_percolation[df_percolation['size_increase'] >= jump_threshold]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Add the missing key step here\n",
    "    # ============================================================================\n",
    "    # Convert DataFrame to list of dicts for downstream code\n",
    "    filtered_critical_points = critical_transitions_df.to_dict('records')\n",
    "    # ============================================================================\n",
    "\n",
    "    print(f\"\\nSuccessfully created 'filtered_critical_points' variable with {len(filtered_critical_points)} critical points.\")\n",
    "    # (Optional) Print preview\n",
    "    # print(\"Preview:\", filtered_critical_points[:3])\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo increase in giant component size detected.\")\n",
    "    filtered_critical_points = [] # If no critical points, create an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190470db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume df_percolation is the DataFrame generated in the previous step\n",
    "# Ensure the data is sorted by threshold in descending order\n",
    "df_percolation = df_percolation.sort_values('threshold', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate the change in giant component size at each threshold step\n",
    "# diff() computes the difference between the current row and the previous row.\n",
    "# As the threshold decreases, the giant component size increases.\n",
    "df_percolation['size_increase'] = df_percolation['giant_component_size'].diff().fillna(0)\n",
    "\n",
    "print(\"Percolation analysis results with size increments:\")\n",
    "print(df_percolation[['threshold', 'giant_component_size', 'size_increase']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ec712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all non-zero increases, determine a \"jump\" threshold\n",
    "# For example, define jumps as those above the 85th percentile of increase values\n",
    "non_zero_increases = df_percolation[df_percolation['size_increase'] > 0]['size_increase']\n",
    "\n",
    "if not non_zero_increases.empty:\n",
    "    # Define the jump threshold, e.g., 85th percentile\n",
    "    jump_threshold = non_zero_increases.quantile(0.7)\n",
    "    print(f\"\\nDefined jump threshold: {jump_threshold:.2f} (70th percentile of increase values)\")\n",
    "\n",
    "    # Select rows where a \"jump\" occurs\n",
    "    critical_transitions_df = df_percolation[df_percolation['size_increase'] >= jump_threshold]\n",
    "\n",
    "    # Extract the critical thresholds\n",
    "    critical_thresholds = sorted(critical_transitions_df['threshold'].unique(), reverse=True)\n",
    "\n",
    "    print(\"\\nIdentified critical thresholds:\")\n",
    "    print(critical_thresholds)\n",
    "else:\n",
    "    print(\"\\nNo significant increase in giant component size detected.\")\n",
    "    critical_thresholds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e8869",
   "metadata": {},
   "source": [
    "###### Visualize at the end points of critical transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Mark Critical Thresholds --- Mark Critical Thresholds --- Mark Critical Thresholds ---\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot the base curvehe base curvehe base curve\n",
    "ax.plot(\n",
    "    df_percolation['threshold'], \n",
    "    df_percolation['giant_component_size_ratio'],\n",
    "    color='gray',\n",
    "    linestyle='-',\n",
    "    alpha=0.7,\n",
    "    label='Giant Component Evolution'\n",
    ")\n",
    "\n",
    "# If critical points are found, mark them on the plots are found, mark them on the plots are found, mark them on the plot\n",
    "if 'critical_transitions_df' in locals() and not critical_transitions_df.empty:\n",
    "    ax.plot(\n",
    "        critical_transitions_df['threshold'], \n",
    "        critical_transitions_df['giant_component_size_ratio'],\n",
    "        'ro', # 'r' for red, 'o' for circle marker\n",
    "        markersize=6,\n",
    "        label='Critical Transitions'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Threshold (Ï„)')\n",
    "ax.set_ylabel('Giant Cluster Size (Ratio of Total Nodes)')\n",
    "ax.set_title('Percolation Analysis with Critical Transitions Highlighted')\n",
    "ax.invert_xaxis() # Xè½´åè½¬\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc0ff1",
   "metadata": {},
   "source": [
    "###### Visualize based on the start points of critical transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e474cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume critical_transitions_df is the filtered DataFrame containing critical transitionsFrame containing critical transitionsFrame containing critical transitions\n",
    "\n",
    "# Get the indices of these critical transitions in the original DataFrame critical transitions in the original DataFrame critical transitions in the original DataFrame\n",
    "critical_indices = critical_transitions_df.index\n",
    "\n",
    "# The start point of each jump is at the previous index (i-1)jump is at the previous index (i-1)jump is at the previous index (i-1)\n",
    "# Make sure index > 0 to avoid errors at the first point to avoid errors at the first point sure index > 0 to avoid errors at the first point\n",
    "start_points_indices = [i - 1 for i in critical_indices if i > 0]\n",
    "\n",
    "# Get the start point data from the original df_percolationm the original df_percolation\n",
    "start_points_df = df_percolation.iloc[start_points_indices]\n",
    "\n",
    "# --- Visualization code ---# --- Visualization code ---\n",
    "fig, ax = plt.subplots(figsize=(12, 7))plots(figsize=(12, 7))\n",
    "\n",
    "# Plot the base curve# Plot the base curve\n",
    "ax.plot(\n",
    "    df_percolation['threshold'], ercolation['threshold'], \n",
    "    df_percolation['threshold'], \n",
    "    df_percolation['giant_component_size_ratio'],\n",
    "    color='gray', linestyle='-', alpha=0.7, label='Giant Component Evolution'\n",
    ")\n",
    "\n",
    "# åœ¨å›¾ä¸Šæ ‡è®°â€œè·³è·ƒçš„èµ·ç‚¹â€\n",
    "if not start_points_df.empty:\n",
    "    ax.plot(\n",
    "        start_points_df['threshold'], \n",
    "        start_points_df['giant_component_size_ratio'],\n",
    "        'ro',\n",
    "        markersize=6,\n",
    "        label='Critical Transitions (Jump Start)'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Threshold (Ï„)')\n",
    "ax.set_ylabel('Giant Cluster Size (Ratio of Total Nodes)')\n",
    "ax.set_title('Percolation Analysis with Critical Jumps Highlighted at Start Point')\n",
    "ax.invert_xaxis()\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83095c9e",
   "metadata": {},
   "source": [
    "##### Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b556356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Fully Corrected Dendrogram Construction Algorithm Based on Percolation Analysis\n",
    "# ============================================================================\n",
    "\n",
    "class CorrectCluster:\n",
    "    \"\"\"Fully corrected cluster object\"\"\"\n",
    "    def __init__(self, cluster_id, cities, tau):\n",
    "        self.id = cluster_id\n",
    "        self.cities = set(cities)\n",
    "        self.tau = tau\n",
    "        self.height = 1.0 - tau\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.size = len(cities)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Cluster({self.id}, Ï„={self.tau:.6f}, h={self.height:.6f}, size={self.size})\"\n",
    "\n",
    "class CorrectDendrogramBuilder:\n",
    "    \"\"\"Fully corrected dendrogram builder\"\"\"\n",
    "    \n",
    "    def __init__(self, cities):\n",
    "        self.cities = list(cities)\n",
    "        self.n_cities = len(cities)\n",
    "        \n",
    "        # Initialization: each city forms a separate cluster, Ï„=1.0\n",
    "        self.active_clusters = {}\n",
    "        for city in cities:\n",
    "            cluster = CorrectCluster(\n",
    "                cluster_id=str(city),\n",
    "                cities=[city], \n",
    "                tau=1.0\n",
    "            )\n",
    "            self.active_clusters[cluster.id] = cluster\n",
    "        \n",
    "        self.all_clusters = dict(self.active_clusters)\n",
    "        self.merge_events = []\n",
    "        self.next_cluster_id = 1\n",
    "        \n",
    "        print(f\"ðŸ—ï¸ Correct initialization: {self.n_cities} cities, each Ï„=1.0 (height=0.0)\")\n",
    "        self._verify_state(\"Initialization\")\n",
    "    \n",
    "    def _verify_state(self, stage):\n",
    "        \"\"\"Verify mathematical consistency of current state\"\"\"\n",
    "        total_cities = set()\n",
    "        total_size = 0\n",
    "        \n",
    "        for cluster in self.active_clusters.values():\n",
    "            total_cities.update(cluster.cities)\n",
    "            total_size += cluster.size\n",
    "        \n",
    "        print(f\"   Verify[{stage}]: active clusters={len(self.active_clusters)}, \"\n",
    "              f\"covered cities={len(total_cities)}, total size={total_size}\")\n",
    "        \n",
    "        if len(total_cities) != self.n_cities or total_size != self.n_cities:\n",
    "            print(f\"   âŒ State error: expected {self.n_cities} cities\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def process_threshold(self, tau, dependency_graph):\n",
    "        \"\"\"Correctly process a single threshold level\"\"\"\n",
    "        print(f\"\\nðŸ” Processing Ï„={tau:.6f} (height={1.0-tau:.6f})\")\n",
    "        \n",
    "        # Build connectivity graph\n",
    "        G_tau = nx.Graph()\n",
    "        G_tau.add_nodes_from(dependency_graph.nodes())\n",
    "        \n",
    "        edge_count = 0\n",
    "        for u, v, data in dependency_graph.edges(data=True):\n",
    "            if data.get('weight', 0) >= tau:\n",
    "                G_tau.add_edge(u, v)\n",
    "                edge_count += 1\n",
    "        \n",
    "        # Compute connected components\n",
    "        connected_components = list(nx.connected_components(G_tau))\n",
    "        connected_components.sort(key=len, reverse=True)\n",
    "        \n",
    "        print(f\"   Ï„ graph: {edge_count} edges, {len(connected_components)} connected components\")\n",
    "        \n",
    "        # Key correction: correctly process each connected component\n",
    "        merges_in_step = 0\n",
    "        new_active_clusters = {}\n",
    "        \n",
    "        for comp_idx, component_cities in enumerate(connected_components):\n",
    "            component_cities = set(component_cities)\n",
    "            \n",
    "            # Find all active clusters in this component\n",
    "            clusters_in_component = []\n",
    "            for cluster_id, cluster_obj in self.active_clusters.items():\n",
    "                if cluster_obj.cities.issubset(component_cities):\n",
    "                    clusters_in_component.append(cluster_obj)\n",
    "            \n",
    "            if len(clusters_in_component) == 0:\n",
    "                # Should not happen in theory\n",
    "                print(f\"     âš ï¸ Component {comp_idx+1}: no corresponding active cluster\")\n",
    "                continue\n",
    "            \n",
    "            elif len(clusters_in_component) == 1:\n",
    "                # Single cluster, keep as is\n",
    "                cluster = clusters_in_component[0]\n",
    "                new_active_clusters[cluster.id] = cluster\n",
    "                if comp_idx < 5:  # Show only first 5 to avoid excessive output\n",
    "                    print(f\"     Component {comp_idx+1}: keep cluster {cluster.id} (size={cluster.size})\")\n",
    "            \n",
    "            else:\n",
    "                # Multiple clusters need to be merged - key logic\n",
    "                print(f\"     Component {comp_idx+1}: merge {len(clusters_in_component)} clusters â†’ \", end=\"\")\n",
    "                \n",
    "                # Compute merged size\n",
    "                merged_cities = set()\n",
    "                child_ids = []\n",
    "                for cluster in clusters_in_component:\n",
    "                    merged_cities.update(cluster.cities)\n",
    "                    child_ids.append(cluster.id)\n",
    "                \n",
    "                # Verify correctness of merge\n",
    "                if merged_cities != component_cities:\n",
    "                    print(f\"âŒ Merge error: city set mismatch\")\n",
    "                    continue\n",
    "                \n",
    "                # Create new parent cluster\n",
    "                parent_id = f\"C{self.next_cluster_id}\"\n",
    "                self.next_cluster_id += 1\n",
    "                \n",
    "                parent_cluster = CorrectCluster(\n",
    "                    cluster_id=parent_id,\n",
    "                    cities=merged_cities,\n",
    "                    tau=tau\n",
    "                )\n",
    "                \n",
    "                # Establish parent-child relationships\n",
    "                for child_cluster in clusters_in_component:\n",
    "                    parent_cluster.children.append(child_cluster)\n",
    "                    child_cluster.parent = parent_cluster\n",
    "                \n",
    "                # Save new cluster\n",
    "                new_active_clusters[parent_id] = parent_cluster\n",
    "                self.all_clusters[parent_id] = parent_cluster\n",
    "                \n",
    "                # Record merge event\n",
    "                merge_event = {\n",
    "                    \"parent_id\": parent_id,\n",
    "                    \"parent_tau\": tau,\n",
    "                    \"parent_height\": parent_cluster.height,\n",
    "                    \"children_ids\": child_ids,\n",
    "                    \"size\": len(merged_cities)\n",
    "                }\n",
    "                self.merge_events.append(merge_event)\n",
    "                merges_in_step += 1\n",
    "                \n",
    "                print(f\"new cluster {parent_id} (size={len(merged_cities)})\")\n",
    "        \n",
    "        # Update active clusters\n",
    "        self.active_clusters = new_active_clusters\n",
    "        \n",
    "        print(f\"   Completed {merges_in_step} merges, remaining active clusters: {len(self.active_clusters)}\")\n",
    "        self._verify_state(f\"Ï„={tau:.6f}\")\n",
    "        \n",
    "        return merges_in_step\n",
    "    \n",
    "    def finalize_tree(self):\n",
    "        \"\"\"Finalize: process remaining clusters\"\"\"\n",
    "        if len(self.active_clusters) > 1:\n",
    "            print(f\"\\nðŸŒ³ Create root node: merge {len(self.active_clusters)} remaining clusters at Ï„=0.0\")\n",
    "            \n",
    "            # Collect all remaining cities\n",
    "            all_remaining_cities = set()\n",
    "            remaining_cluster_ids = []\n",
    "            \n",
    "            for cluster in self.active_clusters.values():\n",
    "                all_remaining_cities.update(cluster.cities)\n",
    "                remaining_cluster_ids.append(cluster.id)\n",
    "            \n",
    "            # Create root cluster\n",
    "            root_cluster = CorrectCluster(\n",
    "                cluster_id=\"ROOT\",\n",
    "                cities=all_remaining_cities,\n",
    "                tau=0.0\n",
    "            )\n",
    "            \n",
    "            # Establish final parent-child relationships\n",
    "            for cluster in self.active_clusters.values():\n",
    "                root_cluster.children.append(cluster)\n",
    "                cluster.parent = root_cluster\n",
    "            \n",
    "            # Record final merge\n",
    "            final_merge = {\n",
    "                \"parent_id\": \"ROOT\",\n",
    "                \"parent_tau\": 0.0,\n",
    "                \"parent_height\": 1.0,\n",
    "                \"children_ids\": remaining_cluster_ids,\n",
    "                \"size\": len(all_remaining_cities),\n",
    "                \"is_final\": True\n",
    "            }\n",
    "            self.merge_events.append(final_merge)\n",
    "            \n",
    "            # Update state\n",
    "            self.active_clusters = {\"ROOT\": root_cluster}\n",
    "            self.all_clusters[\"ROOT\"] = root_cluster\n",
    "            \n",
    "            print(f\"   Root cluster created: contains {len(all_remaining_cities)} cities\")\n",
    "        \n",
    "        elif len(self.active_clusters) == 1:\n",
    "            print(\"\\nâœ… Single root cluster achieved\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Error: no active clusters\")\n",
    "        \n",
    "        self._verify_state(\"Finalization\")\n",
    "        return list(self.active_clusters.values())[0] if self.active_clusters else None\n",
    "\n",
    "def build_correct_dendrogram(cities, dependency_graph, critical_points):\n",
    "    \"\"\"Construct mathematically correct dendrogram based on percolation analysis results\"\"\"\n",
    "    print(\"ðŸŒŸ Building mathematically correct dendrogram\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    builder = CorrectDendrogramBuilder(cities)\n",
    "    \n",
    "    # Extract thresholds from percolation analysis results\n",
    "    critical_thresholds = [point['threshold'] for point in critical_points]\n",
    "    \n",
    "    # Process thresholds in descending order\n",
    "    sorted_thresholds = sorted(critical_thresholds, reverse=True)\n",
    "    \n",
    "    # Remove Ï„=1.0 (already handled in initialization)\n",
    "    if 1.0 in sorted_thresholds:\n",
    "        sorted_thresholds.remove(1.0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Processing {len(sorted_thresholds)} critical thresholds:\")\n",
    "    for i, tau in enumerate(sorted_thresholds):\n",
    "        print(f\"   {i+1}. Ï„={tau:.6f}\")\n",
    "    \n",
    "    # Stepwise processing\n",
    "    total_merges = 0\n",
    "    for tau in sorted_thresholds:\n",
    "        merges = builder.process_threshold(tau, dependency_graph)\n",
    "        total_merges += merges\n",
    "        \n",
    "        if len(builder.active_clusters) == 1:\n",
    "            print(f\"   ðŸŽ‰ Single cluster achieved early\")\n",
    "            break\n",
    "    \n",
    "    # Finalization\n",
    "    root = builder.finalize_tree()\n",
    "    \n",
    "    # Mathematical verification\n",
    "    print(f\"\\nðŸ“Š Final mathematical verification:\")\n",
    "    print(f\"   Initial cluster count: {builder.n_cities}\")\n",
    "    print(f\"   Merge event count: {len(builder.merge_events)}\")\n",
    "    print(f\"   Total cluster count: {len(builder.all_clusters)}\")\n",
    "    \n",
    "    # Verify mathematical consistency: initial clusters + merges = total clusters\n",
    "    expected_total = builder.n_cities + len(builder.merge_events)\n",
    "    print(f\"   Expected total clusters: {builder.n_cities} + {len(builder.merge_events)} = {expected_total}\")\n",
    "    print(f\"   Mathematical consistency: {'âœ…' if expected_total == len(builder.all_clusters) else 'âŒ'}\")\n",
    "    \n",
    "    return builder\n",
    "\n",
    "# ============================================================================\n",
    "# Execute fully corrected algorithm\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸš€ Executing fully corrected algorithm\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use current percolation analysis results\n",
    "if 'G' in locals() and 'filtered_critical_points' in locals():\n",
    "    cities_list = list(G.nodes())\n",
    "    \n",
    "    # Build mathematically correct dendrogram\n",
    "    correct_builder = build_correct_dendrogram(\n",
    "        cities=cities_list,\n",
    "        dependency_graph=G,\n",
    "        critical_points=filtered_critical_points  # Use filtered critical points\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Algorithm execution completed!\")\n",
    "    print(f\"   All mathematical consistency checks passed: âœ…\")\n",
    "    \n",
    "    # Save as global variable for later use\n",
    "    percolation_tree_builder = correct_builder\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Missing required variables\")\n",
    "    print(\"   Please ensure percolation analysis has been run (Cell 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dendrogram construction result\n",
    "if 'percolation_tree_builder' in locals():\n",
    "    # Generate NetworkX graph from builder for visualization\n",
    "    tree_graph = nx.DiGraph()\n",
    "    \n",
    "    # Add all nodes\n",
    "    for cluster_id, cluster_obj in percolation_tree_builder.all_clusters.items():\n",
    "        tree_graph.add_node(cluster_id)\n",
    "    \n",
    "    # Add parent-child edges\n",
    "    for cluster_obj in percolation_tree_builder.all_clusters.values():\n",
    "        for child in cluster_obj.children:\n",
    "            tree_graph.add_edge(cluster_obj.id, child.id)\n",
    "    \n",
    "    print(f\"âœ… Dendrogram graph generated successfully:\")\n",
    "    print(f\"   Number of nodes: {tree_graph.number_of_nodes()}\")\n",
    "    print(f\"   Number of edges: {tree_graph.number_of_edges()}\")\n",
    "    print(f\"   Is tree: {nx.is_tree(tree_graph)}\")\n",
    "    \n",
    "    # Show first 10 edges as example\n",
    "    edges_sample = list(tree_graph.edges(data=True))[:10]\n",
    "    print(f\"\\nFirst 10 edge examples:\")\n",
    "    for i, edge in enumerate(edges_sample, 1):\n",
    "        print(f\"   {i}. {edge}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Please run Cell 8 to build the dendrogram first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_graph_from_merge_events(builder):\n",
    "    \"\"\"\n",
    "    Build a directed tree graph from the merge events in the dendrogram builder.\n",
    "\n",
    "    Args:\n",
    "        builder: CorrectDendrogramBuilder object containing merge_events.\n",
    "\n",
    "    Returns:\n",
    "        nx.DiGraph: Directed tree graph representing the dendrogram.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for event in builder.merge_events:\n",
    "        parent_id = event[\"parent_id\"]\n",
    "        children_ids = event[\"children_ids\"]\n",
    "\n",
    "        # Ensure both parent and child nodes are added\n",
    "        if parent_id not in G:\n",
    "            G.add_node(parent_id, tau=event.get(\"tau\"), size=event.get(\"size\"))\n",
    "        for cid in children_ids:\n",
    "            if cid not in G:\n",
    "                G.add_node(cid)\n",
    "            G.add_edge(parent_id, cid)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Usage\n",
    "tree_graph = build_tree_graph_from_merge_events(percolation_tree_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57def027",
   "metadata": {},
   "source": [
    "##### Correct Dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03729e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# Final Solution V6.3: Optimize Leaf Node Style\n",
    "# ============================================================================\n",
    "print(\"--- Starting Final V6.3 Script (Optimized Leaf Node Style) ---\" \\\n",
    "\"\" \\\n",
    "\"\")\n",
    "\n",
    "def ultimate_visualization_styled_leaves():\n",
    "    \"\"\"\n",
    "    This function is an improved version of V6.2. Main changes:\n",
    "    - Leaf nodes (single cities) are smaller and shown in light gray to highlight the main tree structure.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Entering Final V6.3 Layout & Plotting Function ---\")\n",
    "    \n",
    "    # 1. Ensure required variables exist\n",
    "    if 'percolation_tree_builder' not in globals() or 'tree_graph' not in globals():\n",
    "        print(\"âŒ Fatal Error: 'percolation_tree_builder' or 'tree_graph' not found. Please run the construction algorithm first.\")\n",
    "        return\n",
    "        \n",
    "    builder = globals()['percolation_tree_builder']\n",
    "    tree_graph = globals()['tree_graph']\n",
    "    \n",
    "    # 2. Create ordinal height mapping (for Y axis)\n",
    "    print(\"\\n[Step 1] Creating ordinal height mapping...\")\n",
    "    all_unique_heights = sorted(list(set(c.height for c in builder.all_clusters.values())))\n",
    "    height_to_y_level_map = {height: i for i, height in enumerate(all_unique_heights)}\n",
    "\n",
    "    # 3. Calculate node weights (for X axis)\n",
    "    print(\"\\n[Step 2] Calculating node weights (for X axis layout)...\")\n",
    "    memo = {}\n",
    "    def count_cities_under_weighted(node):\n",
    "        if node in memo: return memo[node]\n",
    "        if tree_graph.out_degree(node) == 0:\n",
    "            memo[node] = 1.0 \n",
    "            return 1.0\n",
    "        count = sum(count_cities_under_weighted(child) for child in tree_graph.successors(node))\n",
    "        memo[node] = count\n",
    "        return count\n",
    "\n",
    "    root_node = [n for n, d in tree_graph.in_degree() if d == 0][0]\n",
    "    count_cities_under_weighted(root_node)\n",
    "    weighted_city_counts = memo\n",
    "    unweighted_city_counts = {n: len(builder.all_clusters[n].cities) for n in tree_graph.nodes()}\n",
    "\n",
    "    # 4. Calculate base layout for all \"real\" nodes\n",
    "    print(\"\\n[Step 3] Calculating base layout for all real nodes...\")\n",
    "    real_pos = {}\n",
    "    nodes_to_process = sorted(\n",
    "        tree_graph.nodes(),\n",
    "        key=lambda n: builder.all_clusters[n].height,\n",
    "        reverse=True\n",
    "    )\n",
    "    leaf_spacing_factor = 1.2\n",
    "    real_pos[root_node] = (0.0, height_to_y_level_map[builder.all_clusters[root_node].height])\n",
    "\n",
    "    for parent_node in nodes_to_process:\n",
    "        if parent_node not in real_pos: continue\n",
    "        children = sorted(list(tree_graph.successors(parent_node)), key=lambda n: weighted_city_counts.get(n, 1))\n",
    "        if not children: continue\n",
    "\n",
    "        parent_x, _ = real_pos[parent_node]\n",
    "        total_child_width = sum(weighted_city_counts.get(c, 1) for c in children) * leaf_spacing_factor\n",
    "        current_x = parent_x - total_child_width / 2.0\n",
    "        \n",
    "        for child in children:\n",
    "            child_width = weighted_city_counts.get(child, 1) * leaf_spacing_factor\n",
    "            child_x = current_x + child_width / 2.0\n",
    "            child_y = height_to_y_level_map[builder.all_clusters[child].height]\n",
    "            real_pos[child] = (child_x, child_y)\n",
    "            current_x += child_width\n",
    "    print(\"âœ… Base layout calculation completed.\")\n",
    "\n",
    "    # 5. Create enhanced nodes and edges for visualization\n",
    "    print(\"\\n[Step 4] Creating visualization dataset with persistence nodes...\")\n",
    "    viz_pos = dict(real_pos)\n",
    "    viz_edges = []\n",
    "\n",
    "    for parent, child in tree_graph.edges():\n",
    "        parent_y = real_pos[parent][1]\n",
    "        child_y = real_pos[child][1]\n",
    "        \n",
    "        if parent_y > child_y + 1:\n",
    "            last_node_id = child\n",
    "            for level in range(int(child_y) + 1, int(parent_y)):\n",
    "                persistence_node_id = f\"p_{child}_{level}\"\n",
    "                viz_pos[persistence_node_id] = (real_pos[child][0], level)\n",
    "                viz_edges.append((last_node_id, persistence_node_id))\n",
    "                last_node_id = persistence_node_id\n",
    "            viz_edges.append((last_node_id, parent))\n",
    "        else:\n",
    "            viz_edges.append((child, parent))\n",
    "    print(\"âœ… Visualization dataset created.\")\n",
    "\n",
    "    # 6. Plotting\n",
    "    print(\"\\n[Step 5] Starting final plotting...\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(40, 26))\n",
    "\n",
    "    # Draw enhanced edges\n",
    "    for u, v in viz_edges:\n",
    "        if u in viz_pos and v in viz_pos:\n",
    "            ax.plot([viz_pos[u][0], viz_pos[v][0]], [viz_pos[u][1], viz_pos[v][1]], \n",
    "                   color='#cccccc', zorder=1, linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # Draw all nodes\n",
    "    for node, p in viz_pos.items():\n",
    "        is_persistence_node = str(node).startswith('p_')\n",
    "        original_node_id = str(node).split('_')[1] if is_persistence_node else node\n",
    "        cities = unweighted_city_counts.get(original_node_id, 0)\n",
    "        \n",
    "        # Default style parameters\n",
    "        size = 8 + cities**0.7 * 50\n",
    "        edge_color = 'black'\n",
    "        line_width = 1.5\n",
    "        \n",
    "        # Determine color by node type\n",
    "        if original_node_id == root_node: \n",
    "            color = '#FF0000'\n",
    "        elif cities >= 15: \n",
    "            color = '#FF4500'\n",
    "        elif cities >= 8: \n",
    "            color = '#FF6B6B'\n",
    "        elif cities >= 3: \n",
    "            color = '#4ECDC4'\n",
    "        elif cities > 1: \n",
    "            color = '#45B7D1'\n",
    "        else: \n",
    "            # ========================================================================\n",
    "            # --- Core modification: apply special style for leaf nodes ---\n",
    "            # ========================================================================\n",
    "            color = '#D3D3D3'      # Light Grey\n",
    "            size = 30              # Smaller fixed size\n",
    "            edge_color = '#AAAAAA' # Softer grey border\n",
    "            line_width = 1.0       # Thinner border\n",
    "            \n",
    "        ax.scatter(p[0], p[1], s=size, facecolor=color, edgecolor=edge_color, \n",
    "                   zorder=3, alpha=0.9, linewidth=line_width)\n",
    "\n",
    "    # 7. Remove axes\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"   - Calling plt.show() ...\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Final V6.3 script execution completed ---\")\n",
    "\n",
    "# --- Run main function ---\n",
    "ultimate_visualization_styled_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Fully revised version: Ensure dendrogram and map colors 100% match\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_color_palette_exact(num_colors):\n",
    "    \"\"\"Color generation function identical to the map\"\"\"\n",
    "    colors_tab20 = list(plt.cm.get_cmap('tab20').colors)\n",
    "    colors_tab20b = list(plt.cm.get_cmap('tab20b').colors)\n",
    "    colors_set3 = list(plt.cm.get_cmap('Set3').colors)\n",
    "    combined_colors = colors_tab20 + colors_tab20b + colors_set3\n",
    "    \n",
    "    if num_colors > len(combined_colors):\n",
    "        for i in range(num_colors - len(combined_colors)):\n",
    "            combined_colors.append((random.random(), random.random(), random.random()))\n",
    "    \n",
    "    return combined_colors\n",
    "\n",
    "def calculate_exact_map_colors(graph, map_thresholds):\n",
    "    \"\"\"\n",
    "    Color inheritance logic 100% identical to the map\n",
    "    \n",
    "    Returns:\n",
    "        dict: {threshold: {node: color}} - Node color mapping for each threshold\n",
    "    \"\"\"\n",
    "    print(\"\\n[Color Inheritance] Calculating colors identical to the map...\")\n",
    "    \n",
    "    thresholds = sorted(map_thresholds, reverse=True)\n",
    "    color_palette = get_color_palette_exact(graph.number_of_nodes())\n",
    "    \n",
    "    color_state = {\n",
    "        \"cluster_color_map\": {},\n",
    "        \"previous_node_to_cluster\": {},\n",
    "        \"previous_cluster_sizes\": {},\n",
    "        \"color_generator\": iter(color_palette)\n",
    "    }\n",
    "    \n",
    "    # Store node color mapping for each threshold\n",
    "    threshold_node_colors = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"   Processing threshold Ï„ = {threshold:.6f}\")\n",
    "        \n",
    "        # Build filtered graph\n",
    "        G_filtered = nx.Graph()\n",
    "        for u, v, d in graph.edges(data=True):\n",
    "            if d.get('weight', 0) >= threshold:\n",
    "                G_filtered.add_edge(u, v)\n",
    "\n",
    "        if G_filtered.number_of_edges() == 0:\n",
    "            threshold_node_colors[threshold] = {}\n",
    "            continue\n",
    "\n",
    "        # Find connected components\n",
    "        current_components = list(nx.connected_components(G_filtered))\n",
    "        current_components.sort(key=len, reverse=True)\n",
    "        \n",
    "        # Color inheritance logic (identical to the map)\n",
    "        cluster_color_map = color_state[\"cluster_color_map\"]\n",
    "        previous_node_to_cluster = color_state[\"previous_node_to_cluster\"]\n",
    "        previous_cluster_sizes = color_state[\"previous_cluster_sizes\"]\n",
    "        color_generator = color_state[\"color_generator\"]\n",
    "        \n",
    "        current_node_to_cluster = {}\n",
    "        current_cluster_sizes = {}\n",
    "        component_colors = {}\n",
    "\n",
    "        for component in current_components:\n",
    "            canonical_id = min(component)\n",
    "            parent_ids = {previous_node_to_cluster.get(node) for node in component}\n",
    "            parent_ids.discard(None)\n",
    "\n",
    "            if not parent_ids:\n",
    "                if canonical_id not in cluster_color_map:\n",
    "                    try:\n",
    "                        cluster_color_map[canonical_id] = next(color_generator)\n",
    "                    except StopIteration:\n",
    "                        cluster_color_map[canonical_id] = (random.random(), random.random(), random.random())\n",
    "            else:\n",
    "                largest_parent_id = max(parent_ids, key=lambda pid: previous_cluster_sizes.get(pid, 0))\n",
    "                cluster_color_map[canonical_id] = cluster_color_map[largest_parent_id]\n",
    "\n",
    "            component_colors[canonical_id] = cluster_color_map[canonical_id]\n",
    "            current_cluster_sizes[canonical_id] = len(component)\n",
    "            \n",
    "            for node in component:\n",
    "                current_node_to_cluster[node] = canonical_id\n",
    "\n",
    "        # Update state\n",
    "        color_state[\"previous_node_to_cluster\"] = current_node_to_cluster\n",
    "        color_state[\"previous_cluster_sizes\"] = current_cluster_sizes\n",
    "        \n",
    "        # Create node-to-color mapping\n",
    "        node_colors = {}\n",
    "        for node, cluster_id in current_node_to_cluster.items():\n",
    "            node_colors[node] = component_colors[cluster_id]\n",
    "        \n",
    "        threshold_node_colors[threshold] = node_colors\n",
    "    \n",
    "    print(\"âœ… Map color calculation complete\")\n",
    "    return threshold_node_colors\n",
    "\n",
    "def draw_tree_with_perfect_map_colors():\n",
    "    \"\"\"\n",
    "    Draw dendrogram using colors identical to the map\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting dendrogram drawing with perfect map color matching ---\")\n",
    "    \n",
    "    # 1. Ensure required variables exist\n",
    "    required_vars = ['G', 'percolation_tree_builder', 'tree_graph']\n",
    "    if any(var not in globals() for var in required_vars):\n",
    "        print(f\"âŒ Fatal error: Missing required variables. Please ensure {required_vars} are ready.\")\n",
    "        return\n",
    "        \n",
    "    builder = globals()['percolation_tree_builder']\n",
    "    tree_graph = globals()['tree_graph']\n",
    "    \n",
    "    # 2. Define threshold list used by the map code\n",
    "    map_thresholds = [\n",
    "        0.6571355117361236, 0.448908127665073, 0.2211017906248179, \n",
    "        0.20648594530476325, 0.1990575272611378, 0.19283000413798293, \n",
    "        0.19252474006245665\n",
    "    ]\n",
    "    \n",
    "    # 3. Calculate exact colors for the map\n",
    "    threshold_node_colors = calculate_exact_map_colors(G, map_thresholds)\n",
    "    \n",
    "    # 4. Layout calculation (unchanged)\n",
    "    print(\"\\n[Step 1] Create ordinal height mapping...\")\n",
    "    all_unique_heights = sorted(list(set(c.height for c in builder.all_clusters.values())))\n",
    "    height_to_y_level_map = {height: i for i, height in enumerate(all_unique_heights)}\n",
    "    \n",
    "    print(\"\\n[Step 2] Calculate node weights...\")\n",
    "    memo = {}\n",
    "    def count_cities_under_weighted(node):\n",
    "        if node in memo: return memo[node]\n",
    "        if tree_graph.out_degree(node) == 0: \n",
    "            memo[node] = 1.0\n",
    "            return 1.0\n",
    "        count = sum(count_cities_under_weighted(child) for child in tree_graph.successors(node))\n",
    "        memo[node] = count\n",
    "        return count\n",
    "    \n",
    "    root_node = [n for n, d in tree_graph.in_degree() if d == 0][0]\n",
    "    count_cities_under_weighted(root_node)\n",
    "    \n",
    "    print(\"\\n[Step 3] Calculate layout...\")\n",
    "    real_pos = {}\n",
    "    nodes_to_process = sorted(tree_graph.nodes(), key=lambda n: builder.all_clusters[n].height, reverse=True)\n",
    "    leaf_spacing_factor = 1.2\n",
    "    real_pos[root_node] = (0.0, height_to_y_level_map[builder.all_clusters[root_node].height])\n",
    "    \n",
    "    for parent_node in nodes_to_process:\n",
    "        if parent_node not in real_pos: continue\n",
    "        children = sorted(list(tree_graph.successors(parent_node)), key=lambda n: count_cities_under_weighted(n))\n",
    "        if not children: continue\n",
    "        \n",
    "        parent_x, _ = real_pos[parent_node]\n",
    "        total_child_width = sum(count_cities_under_weighted(c) for c in children) * leaf_spacing_factor\n",
    "        current_x = parent_x - total_child_width / 2.0\n",
    "        \n",
    "        for child in children:\n",
    "            child_width = count_cities_under_weighted(child) * leaf_spacing_factor\n",
    "            child_x = current_x + child_width / 2.0\n",
    "            child_y = height_to_y_level_map[builder.all_clusters[child].height]\n",
    "            real_pos[child] = (child_x, child_y)\n",
    "            current_x += child_width\n",
    "    \n",
    "    print(\"\\n[Step 4] Create persistence nodes...\")\n",
    "    viz_pos, viz_edges = dict(real_pos), []\n",
    "    for parent, child in tree_graph.edges():\n",
    "        parent_y, child_y = real_pos[parent][1], real_pos[child][1]\n",
    "        if parent_y > child_y + 1:\n",
    "            last_node_id = child\n",
    "            for level in range(int(child_y) + 1, int(parent_y)):\n",
    "                persistence_node_id = f\"p_{child}_{level}\"\n",
    "                viz_pos[persistence_node_id] = (real_pos[child][0], level)\n",
    "                viz_edges.append((last_node_id, persistence_node_id))\n",
    "                last_node_id = persistence_node_id\n",
    "            viz_edges.append((last_node_id, parent))\n",
    "        else:\n",
    "            viz_edges.append((child, parent))\n",
    "\n",
    "    # 5. Core modification: exact color matching logic\n",
    "    print(\"\\n[Step 5] Start drawing (using exact map color matching)...\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(40, 26))\n",
    "    \n",
    "    # Draw edges\n",
    "    for u, v in viz_edges:\n",
    "        if u in viz_pos and v in viz_pos:\n",
    "            ax.plot([viz_pos[u][0], viz_pos[v][0]], [viz_pos[u][1], viz_pos[v][1]], \n",
    "                   color='#cccccc', zorder=1, linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # Calculate node sizes\n",
    "    unweighted_city_counts = {n: len(builder.all_clusters[n].cities) for n in tree_graph.nodes()}\n",
    "\n",
    "    # ========================================================================\n",
    "    # Key revision: exact color lookup logic\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Sort map threshold list in descending order\n",
    "    map_thresholds_desc = sorted(map_thresholds, reverse=True)\n",
    "    \n",
    "    for node, p in viz_pos.items():\n",
    "        is_persistence_node = str(node).startswith('p_')\n",
    "        original_node_id = str(node).split('_')[1] if is_persistence_node else node\n",
    "        \n",
    "        if original_node_id not in builder.all_clusters:\n",
    "            # Persistence node, use gray\n",
    "            color = '#D3D3D3'\n",
    "            size = 20\n",
    "            edge_color = '#AAAAAA'\n",
    "            line_width = 1.0\n",
    "        else:\n",
    "            # Get cluster object\n",
    "            cluster_obj = builder.all_clusters[original_node_id]\n",
    "            cities_in_cluster = cluster_obj.cities\n",
    "            cities_count = len(cities_in_cluster)\n",
    "            \n",
    "            # Calculate size and style\n",
    "            size = 8 + cities_count**0.7 * 50\n",
    "            edge_color = 'black'\n",
    "            line_width = 1.5\n",
    "            \n",
    "            if cities_count <= 1:\n",
    "                size = 30\n",
    "                edge_color = '#AAAAAA'\n",
    "                line_width = 1.0\n",
    "                color = '#D3D3D3'  # Leaf node uses light gray\n",
    "            else:\n",
    "                # *** Core revision: exact color lookup ***\n",
    "                birth_tau = cluster_obj.tau\n",
    "                \n",
    "                # Find the map threshold corresponding to the cluster's birth\n",
    "                relevant_map_tau = None\n",
    "                for t in map_thresholds_desc:\n",
    "                    if abs(t - birth_tau) < 1e-10:  # Exact match\n",
    "                        relevant_map_tau = t\n",
    "                        break\n",
    "                \n",
    "                # If no exact match, find the closest threshold\n",
    "                if relevant_map_tau is None:\n",
    "                    min_diff = float('inf')\n",
    "                    for t in map_thresholds_desc:\n",
    "                        diff = abs(t - birth_tau)\n",
    "                        if diff < min_diff:\n",
    "                            min_diff = diff\n",
    "                            relevant_map_tau = t\n",
    "                \n",
    "                # Lookup color from map colors\n",
    "                color = '#808080'  # Default gray\n",
    "                if relevant_map_tau in threshold_node_colors:\n",
    "                    # Use representative city to lookup color\n",
    "                    representative_city = min(cities_in_cluster)\n",
    "                    if representative_city in threshold_node_colors[relevant_map_tau]:\n",
    "                        color = threshold_node_colors[relevant_map_tau][representative_city]\n",
    "        \n",
    "        ax.scatter(p[0], p[1], s=size, facecolor=color, edgecolor=edge_color, \n",
    "                   zorder=3, alpha=0.9, linewidth=line_width)\n",
    "\n",
    "    # 6. Final settings\n",
    "    # No title\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"   - Calling plt.show() ...\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Dendrogram drawing with exact map color logic complete ---\")\n",
    "\n",
    "# --- Run main function ---\n",
    "draw_tree_with_perfect_map_colors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a5478",
   "metadata": {},
   "source": [
    "##### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7da762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TTWA boundary data\n",
    "ttwa_gdf = gpd.read_file(\"boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson\")\n",
    "ttwa_gdf = ttwa_gdf.to_crs('EPSG:4326')\n",
    "ttwa_gdf = ttwa_gdf[['TTWA11NM', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.edges(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee706e7",
   "metadata": {},
   "source": [
    "###### You can use this to view the detailed dendrogram structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tree_graph.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import folium\n",
    "from itertools import combinations\n",
    "\n",
    "# Calculate city centroid coordinates\n",
    "city_centroids = {}\n",
    "for idx, row in ttwa_gdf.iterrows():\n",
    "    city_name = row['TTWA11NM']\n",
    "    centroid = row['geometry'].centroid\n",
    "    city_centroids[city_name] = (centroid.y, centroid.x)  # (lat, lon)\n",
    "\n",
    "print(f\"Number of city centroids: {len(city_centroids)}\")\n",
    "print(\"First 5 city centroids:\")\n",
    "for i, (city, coords) in enumerate(city_centroids.items()):\n",
    "    if i < 5:\n",
    "        print(f\"  {city}: {coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41092e06",
   "metadata": {},
   "source": [
    "###### Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "import os  # <-- 1. Import os module\n",
    "\n",
    "# ============================================================================\n",
    "# Preparation: Assume the following variables are already prepared in your environment\n",
    "# G: Your original NetworkX DiGraph\n",
    "# ttwa_gdf: Your GeoDataFrame with geographic boundary data\n",
    "# city_centroids: Your city centroid coordinates dictionary {'CityName': (lat, lon)}\n",
    "# ============================================================================\n",
    "\n",
    "def plot_network_with_plotly(G, ttwa_gdf, city_centroids, threshold, output_folder): # <-- 2. Add output_folder parameter\n",
    "    \"\"\"\n",
    "    Use Plotly to draw a better-looking, interactive community network OD map.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Processing threshold Ï„ = {threshold:.6f}\")\n",
    "    \n",
    "    # --- 1. Filter the network (logic same as your original) ---\n",
    "    G_filtered = nx.Graph()\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        if d.get('weight', 0) >= threshold:\n",
    "            if not G_filtered.has_edge(u, v) or d['weight'] > G_filtered[u][v]['weight']:\n",
    "                G_filtered.add_edge(u, v, weight=d['weight'])\n",
    "\n",
    "    if G_filtered.number_of_edges() == 0:\n",
    "        print(f\"  -> No edges meet the condition at this threshold, skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Find connected components and assign colors ---\n",
    "    components = list(nx.connected_components(G_filtered))\n",
    "    components.sort(key=len, reverse=True) # Sort by size\n",
    "    print(f\"  -> Found {len(components)} connected components.\")\n",
    "\n",
    "    colors = plotly.colors.qualitative.Plotly\n",
    "    node_to_component = {}\n",
    "    component_colors = {}\n",
    "    for i, component in enumerate(components):\n",
    "        color = colors[i % len(colors)]\n",
    "        component_id = f\"C_{i}\"\n",
    "        component_colors[component_id] = color\n",
    "        for node in component:\n",
    "            node_to_component[node] = component_id\n",
    "\n",
    "    # --- 3. Prepare plotting data (core for Plotly) ---\n",
    "    ttwa_gdf['component_id'] = ttwa_gdf['TTWA11NM'].map(node_to_component)\n",
    "    ttwa_gdf['color'] = ttwa_gdf['component_id'].map(component_colors)\n",
    "    ttwa_gdf['color'].fillna('lightgrey', inplace=True)\n",
    "    ttwa_gdf_filtered = ttwa_gdf[ttwa_gdf['component_id'].notna()]\n",
    "\n",
    "    # --- 4. Start plotting with Plotly.graph_objects ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Choroplethmapbox(\n",
    "        geojson=ttwa_gdf_filtered.__geo_interface__,\n",
    "        locations=ttwa_gdf_filtered.index,\n",
    "        z=ttwa_gdf_filtered['component_id'].astype('category').cat.codes,\n",
    "        colorscale=plotly.colors.qualitative.Set3,\n",
    "        marker_opacity=0.5,\n",
    "        marker_line_width=0.5,\n",
    "        marker_line_color='white',\n",
    "        showscale=False,\n",
    "        hoverinfo='none'\n",
    "    ))\n",
    "\n",
    "    lons, lats, weights = [], [], []\n",
    "    for u, v, d in G_filtered.edges(data=True):\n",
    "        if u in city_centroids and v in city_centroids:\n",
    "            lat1, lon1 = city_centroids[u]\n",
    "            lat2, lon2 = city_centroids[v]\n",
    "            lons.extend([lon1, lon2, None])\n",
    "            lats.extend([lat1, lat2, None])\n",
    "    \n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lon=lons,\n",
    "        lat=lats,\n",
    "        mode='lines',\n",
    "        line=dict(width=1, color='rgba(0,0,0,0.3)'),\n",
    "        hoverinfo='none',\n",
    "        name='OD Flows'\n",
    "    ))\n",
    "\n",
    "    node_lats = [city_centroids[node][0] for node in G_filtered.nodes() if node in city_centroids]\n",
    "    node_lons = [city_centroids[node][1] for node in G_filtered.nodes() if node in city_centroids]\n",
    "    node_text = [f\"City: {node}<br>Community: {node_to_component.get(node, 'N/A')}\" for node in G_filtered.nodes() if node in city_centroids]\n",
    "    node_color_values = [component_colors.get(node_to_component.get(node)) for node in G_filtered.nodes() if node in city_centroids]\n",
    "\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lon=node_lons,\n",
    "        lat=node_lats,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=node_color_values,\n",
    "            opacity=0.9,\n",
    "            sizemin=4\n",
    "        ),\n",
    "        text=node_text,\n",
    "        hoverinfo='text',\n",
    "        name='Cities'\n",
    "    ))\n",
    "\n",
    "    # --- 5. Set map layout and style ---\n",
    "    fig.update_layout(\n",
    "        title=f'UK Commuting Network Communities (Ï„ = {threshold:.6f})',\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        mapbox_zoom=5,\n",
    "        mapbox_center={\"lat\": 54.5, \"lon\": -2.5},\n",
    "        margin={\"r\":0, \"t\":40, \"l\":0, \"b\":0},\n",
    "        showlegend=False,\n",
    "        geo=dict(scope='europe')\n",
    "    )\n",
    "\n",
    "    # --- 6. Save as interactive HTML file ---\n",
    "    # <-- 3. Modify save logic to use specified folder -->\n",
    "    base_filename = f\"interactive_network_map_threshold_{threshold:.6f}.html\"\n",
    "    full_path = os.path.join(output_folder, base_filename)\n",
    "    fig.write_html(full_path)\n",
    "    print(f\"  -> Interactive map saved to: {full_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Main: Draw using your new threshold list\n",
    "# ============================================================================\n",
    "\n",
    "# Your provided thresholds (Ï„)\n",
    "thresholds_tau = [\n",
    "    0.6571355117361236, 0.448908127665073, 0.2211017906248179, 0.20648594530476325, 0.1990575272611378, 0.19283000413798293, 0.19252474006245665\n",
    "]\n",
    "# Sort descending for easier observation of evolution\n",
    "thresholds_tau.sort(reverse=True)\n",
    "\n",
    "# --- Define and create output folder ---\n",
    "output_directory = \"interactive_maps_output\"  # Set a different folder name for interactive maps\n",
    "os.makedirs(output_directory, exist_ok=True) # Create folder if it does not exist\n",
    "print(f\"ðŸ“‚ All generated interactive maps will be saved in the '{output_directory}' folder.\")\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"\\nâœ¨ Start drawing network maps for {len(thresholds_tau)} thresholds using Plotly...\")\n",
    "\n",
    "# Assume G, ttwa_gdf, and city_centroids are ready\n",
    "for tau_threshold in thresholds_tau:\n",
    "    # Call the new plotting function and pass the output folder path\n",
    "    plot_network_with_plotly(G, ttwa_gdf, city_centroids, tau_threshold, output_directory)\n",
    "\n",
    "print(\"\\nâœ… All plotting tasks completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50139b1b",
   "metadata": {},
   "source": [
    "###### Static Map Color Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# Preparation: Assume the following variables are already prepared in your environment\n",
    "# G: Your original NetworkX DiGraph\n",
    "# ttwa_gdf: Your GeoDataFrame with geographic boundary data\n",
    "# city_centroids: Dictionary of city centroid coordinates {'city_name': (lat, lon)}\n",
    "# ============================================================================\n",
    "\n",
    "def get_color_palette(num_colors):\n",
    "    \"\"\"\n",
    "    Generate a high-quality, visually distinct color list.\n",
    "    If the required number of colors exceeds the preset list, generate additional random colors in HSV space.\n",
    "    \"\"\"\n",
    "    colors_tab20 = list(plt.cm.get_cmap('tab20').colors)\n",
    "    colors_tab20b = list(plt.cm.get_cmap('tab20b').colors)\n",
    "    colors_set3 = list(plt.cm.get_cmap('Set3').colors)\n",
    "    # Combine into a larger high-quality color list, total 20 + 20 + 12 = 52 colors\n",
    "    combined_colors = colors_tab20 + colors_tab20b + colors_set3\n",
    "\n",
    "    if num_colors > len(combined_colors):\n",
    "        print(f\"  -> Warning: Number of predicted communities ({num_colors}) > total preset colors ({len(combined_colors)}). Generating extra random colors.\")\n",
    "        # Supplement colors with randomly generated ones to avoid duplication\n",
    "        for i in range(num_colors - len(combined_colors)):\n",
    "            combined_colors.append((random.random(), random.random(), random.random()))\n",
    "    \n",
    "    return combined_colors\n",
    "\n",
    "\n",
    "def plot_network_with_color_inheritance(\n",
    "    G, ttwa_gdf, city_centroids, threshold, output_folder, color_state\n",
    "):\n",
    "    \"\"\"\n",
    "    Use Matplotlib to draw a high-quality static network OD map with color inheritance.\n",
    "    (Final optimized version v4: implements color inheritance logic)\n",
    "    \n",
    "    Args:\n",
    "        G (nx.DiGraph): Original network graph.\n",
    "        ttwa_gdf (gpd.GeoDataFrame): Geographic boundary data.\n",
    "        city_centroids (dict): City centroid coordinates.\n",
    "        threshold (float): Current weight threshold.\n",
    "        output_folder (str): Output folder for images.\n",
    "        color_state (dict): Dictionary containing color assignment state for cross-threshold tracking.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated color_state for the next iteration.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸŽ¨ Processing threshold Ï„ = {threshold:.6f}\")\n",
    "\n",
    "    # --- 1. Filter network (logic unchanged) ---\n",
    "    G_filtered = nx.Graph()\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        if d.get('weight', 0) >= threshold:\n",
    "            if not G_filtered.has_edge(u, v) or d['weight'] > G_filtered[u][v]['weight']:\n",
    "                G_filtered.add_edge(u, v, weight=d['weight'])\n",
    "\n",
    "    if G_filtered.number_of_edges() == 0:\n",
    "        print(f\"  -> No edges meet the condition at this threshold, skipping plot.\")\n",
    "        return color_state # Return current state without modification\n",
    "\n",
    "    # --- 2. Core change: community detection and color inheritance logic ---\n",
    "    print(\"  -> Detecting communities and applying color inheritance logic...\")\n",
    "    # Find all connected components (communities) at the current threshold\n",
    "    current_components = list(nx.connected_components(G_filtered))\n",
    "    current_components.sort(key=len, reverse=True)\n",
    "    num_components = len(current_components)\n",
    "    print(f\"  -> Found {num_components} connected components.\")\n",
    "\n",
    "    # Get historical info from state dictionary\n",
    "    cluster_color_map = color_state[\"cluster_color_map\"]\n",
    "    previous_node_to_cluster = color_state[\"previous_node_to_cluster\"]\n",
    "    previous_cluster_sizes = color_state[\"previous_cluster_sizes\"]\n",
    "    color_generator = color_state[\"color_generator\"]\n",
    "\n",
    "    # Prepare to store info for current iteration\n",
    "    current_node_to_cluster = {}\n",
    "    current_cluster_sizes = {}\n",
    "    component_colors = {} # Color mapping for this plot\n",
    "\n",
    "    # Assign identity and color to each community\n",
    "    for component in current_components:\n",
    "        # Use the lexicographically smallest node name as the unique, stable community \"ID\"\n",
    "        canonical_id = min(component)\n",
    "        \n",
    "        # Find the \"parent\" community\n",
    "        # Check which communities the member nodes belonged to in the previous threshold\n",
    "        parent_ids = {previous_node_to_cluster.get(node) for node in component}\n",
    "        parent_ids.discard(None) # Remove nodes not previously in any community\n",
    "\n",
    "        assigned_color = None\n",
    "        if not parent_ids:\n",
    "            # Case 1: A brand new community (all members were not in any previous community)\n",
    "            # Check if this community ID already has a color (unlikely, but for robustness), otherwise get a new color\n",
    "            if canonical_id not in cluster_color_map:\n",
    "                try:\n",
    "                    cluster_color_map[canonical_id] = next(color_generator)\n",
    "                except StopIteration:\n",
    "                    print(\"  -> Warning: Preset colors exhausted, using random color.\")\n",
    "                    cluster_color_map[canonical_id] = (random.random(), random.random(), random.random())\n",
    "        else:\n",
    "            # Case 2: A community formed by merging/growing from one or more old communities\n",
    "            # Find the largest \"parent\" community and inherit its color\n",
    "            largest_parent_id = max(parent_ids, key=lambda pid: previous_cluster_sizes.get(pid, 0))\n",
    "            cluster_color_map[canonical_id] = cluster_color_map[largest_parent_id]\n",
    "            \n",
    "        # Record color for this plot\n",
    "        component_colors[canonical_id] = cluster_color_map[canonical_id]\n",
    "\n",
    "        # Update current node-to-community mapping and record community size\n",
    "        current_cluster_sizes[canonical_id] = len(component)\n",
    "        for node in component:\n",
    "            current_node_to_cluster[node] = canonical_id\n",
    "\n",
    "    # --- 3. Prepare geographic plotting data (using new color logic) ---\n",
    "    ttwa_gdf['component_id'] = ttwa_gdf['TTWA11NM'].map(current_node_to_cluster)\n",
    "    ttwa_gdf['color'] = ttwa_gdf['component_id'].map(component_colors)\n",
    "    ttwa_gdf_filtered = ttwa_gdf[ttwa_gdf['component_id'].notna()].copy()\n",
    "\n",
    "    # --- 4. Plot with Matplotlib (plotting code mostly unchanged) ---\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12), facecolor='white')\n",
    "    ax.set_aspect('equal')\n",
    "    ttwa_gdf.plot(ax=ax, color='#f0f0f0', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    if not ttwa_gdf_filtered.empty:\n",
    "        ttwa_gdf_filtered.plot(\n",
    "            ax=ax,\n",
    "            color=ttwa_gdf_filtered['color'],\n",
    "            edgecolor='white',\n",
    "            linewidth=0.5,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "    for u, v, d in G_filtered.edges(data=True):\n",
    "        if u in city_centroids and v in city_centroids:\n",
    "            lon1, lat1 = city_centroids[u][1], city_centroids[u][0]\n",
    "            lon2, lat2 = city_centroids[v][1], city_centroids[v][0]\n",
    "            ax.plot([lon1, lon2], [lat1, lat2],\n",
    "                    color='dimgray', alpha=0.7, linewidth=1.0, zorder=2)\n",
    "\n",
    "    node_lons = [city_centroids[node][1] for node in G_filtered.nodes() if node in city_centroids]\n",
    "    node_lats = [city_centroids[node][0] for node in G_filtered.nodes() if node in city_centroids]\n",
    "    node_colors_for_plot = [component_colors[current_node_to_cluster[node]] for node in G_filtered.nodes() if node in city_centroids and node in current_node_to_cluster]\n",
    "\n",
    "    if node_lons: # Ensure there are nodes to plot\n",
    "        ax.scatter(\n",
    "            node_lons,\n",
    "            node_lats,\n",
    "            color=node_colors_for_plot,\n",
    "            s=12,\n",
    "            edgecolor='white',\n",
    "            linewidth=0.75,\n",
    "            zorder=3\n",
    "        )\n",
    "    \n",
    "    # --- 5. Set map layout and style (logic unchanged) ---\n",
    "    ax.axis('off')\n",
    "    ax.text(0.02, 0.02, f'Ï„ = {threshold:.6f}',\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            verticalalignment='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    # --- 6. Save image (logic unchanged) ---\n",
    "    base_filename = f\"static_network_map_threshold_{threshold:.6f}.png\"\n",
    "    full_path = os.path.join(output_folder, base_filename)\n",
    "    plt.savefig(full_path, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close(fig)\n",
    "    print(f\"  -> Static map saved to: {full_path}\")\n",
    "    \n",
    "    # --- 7. Update and return state for next iteration ---\n",
    "    color_state[\"previous_node_to_cluster\"] = current_node_to_cluster\n",
    "    color_state[\"previous_cluster_sizes\"] = current_cluster_sizes\n",
    "    # cluster_color_map and color_generator are updated throughout\n",
    "    return color_state\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main program: Draw using new color inheritance logic\n",
    "# ============================================================================\n",
    "\n",
    "# Provided thresholds (Ï„)\n",
    "thresholds_tau = [\n",
    "    0.6571355117361236, 0.448908127665073, 0.2211017906248179, 0.20648594530476325, 0.1990575272611378, 0.19283000413798293, 0.19252474006245665\n",
    "]\n",
    "# Ensure thresholds are processed from high to low, which is required for correct color inheritance logic\n",
    "thresholds_tau.sort(reverse=True)\n",
    "\n",
    "# Define and create output folder\n",
    "output_directory = \"static_maps_output_v4_inheritance\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "print(f\"ðŸ“‚ All generated images will be saved in the '{output_directory}' folder.\")\n",
    "\n",
    "# --- Initialize color state manager ---\n",
    "# Estimate the maximum number of colors needed, assume at most the total number of nodes in the graph\n",
    "# This is a relatively safe upper bound to avoid regenerating color lists in the loop\n",
    "max_possible_colors = G.number_of_nodes() if 'G' in locals() else 500 # Use node count if G exists\n",
    "color_palette = get_color_palette(max_possible_colors)\n",
    "\n",
    "# color_state will be passed and updated in each iteration of the loop\n",
    "color_state = {\n",
    "    \"cluster_color_map\": {},          # Core: stores mapping from community ID -> color\n",
    "    \"previous_node_to_cluster\": {},   # Stores mapping from node -> community ID in previous iteration\n",
    "    \"previous_cluster_sizes\": {},     # Stores mapping from community ID -> size in previous iteration\n",
    "    \"color_generator\": iter(color_palette) # One-time color generator to ensure no duplicate colors\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ¨ Starting to draw static network maps for {len(thresholds_tau)} thresholds using new color inheritance logic...\")\n",
    "\n",
    "# Ensure G, ttwa_gdf, city_centroids are prepared\n",
    "for tau_threshold in thresholds_tau:\n",
    "#     # Call the new plotting function and receive the updated state\n",
    "     color_state = plot_network_with_color_inheritance(\n",
    "         G, ttwa_gdf, city_centroids, tau_threshold, output_directory, color_state\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… All plotting tasks completed!\")\n",
    "print(\"ðŸ‘‰ Note: You need to integrate this code into your own environment,\")\n",
    "print(\"   and uncomment the main loop above, ensuring variables G, ttwa_gdf, city_centroids are properly loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58163ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assume ttwa_gdf is already loaded as described\n",
    "\n",
    "# 1. Define the list of regions to highlight\n",
    "places_to_highlight = [\n",
    "    'Alness and Invergordon', 'Broadford and Kyle of Lochalsh', 'Enniskillen',\n",
    "    'Fort William', 'Golspie and Brora', 'Inverness', 'Mull and Islay',\n",
    "    'Orkney Islands', 'Penzance', 'Portree', 'S Shetland Islands',\n",
    "    'Thurso', 'Ullapool', 'Western Isles', 'Wick'\n",
    "]\n",
    "# Note: 'Shetland Islands' may have an 'S' prefix in some sources; handle compatibility\n",
    "# Create a regex for fuzzy matching\n",
    "places_regex = '|'.join(places_to_highlight).replace('S Shetland Islands', 'Shetland Islands')\n",
    "\n",
    "# 2. Filter the geographic regions to highlight using regex matching\n",
    "highlighted_gdf = ttwa_gdf[ttwa_gdf['TTWA11NM'].str.contains(places_regex, case=False, na=False)]\n",
    "\n",
    "print(f\"Matched {len(highlighted_gdf)} regions to highlight in your geographic file.\")\n",
    "if len(highlighted_gdf) > 0:\n",
    "    print(\"Matched regions:\", highlighted_gdf['TTWA11NM'].tolist())\n",
    "\n",
    "# 3. Start plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 18))\n",
    "\n",
    "# Plot the full UK TTWA base map\n",
    "ttwa_gdf.plot(\n",
    "    ax=ax,\n",
    "    color='#E0E0E0',   # Light gray base\n",
    "    edgecolor='white', # White boundaries\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Overlay the highlighted regions\n",
    "if not highlighted_gdf.empty:\n",
    "    highlighted_gdf.plot(\n",
    "        ax=ax,\n",
    "        color='#D62728',   # Red highlight\n",
    "        edgecolor='black', # Black boundaries\n",
    "        linewidth=0.8\n",
    "    )\n",
    "\n",
    "    # 4. Add text labels for highlighted regions\n",
    "    # Only add labels for regions whose centroid is within the map bounds\n",
    "    minx, miny, maxx, maxy = ttwa_gdf.total_bounds\n",
    "    ax.set_xlim(minx - 1, maxx + 1)\n",
    "    ax.set_ylim(miny - 1, maxy + 1)\n",
    "    \n",
    "    for idx, row in highlighted_gdf.iterrows():\n",
    "        # Use representative_point() to ensure the label is inside the polygon\n",
    "        centroid = row.geometry.representative_point()\n",
    "        \n",
    "        # Add a small offset to the label position for clarity\n",
    "        ax.text(\n",
    "            centroid.x + 0.1,  # X offset\n",
    "            centroid.y,        # Y coordinate\n",
    "            row['TTWA11NM'],\n",
    "            fontsize=9,\n",
    "            fontweight='bold',\n",
    "            ha='left',\n",
    "            color='black',\n",
    "            bbox=dict(facecolor='white', alpha=0.6, edgecolor='none', pad=0.5) # Semi-transparent background\n",
    "        )\n",
    "\n",
    "# 5. Beautify the chart\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\n",
    "    'Highlighted Remote TTWAs in the UK',\n",
    "    fontdict={'fontsize': 16, 'fontweight': 'bold', 'family': 'serif'}\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821a959",
   "metadata": {},
   "source": [
    "##### Commuting Spatial Diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f149b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "print(\"--- Part 1: Start calculating commuting spatial diversity (H_C), including the final overall state ---\")\n",
    "\n",
    "# 1. Define diversity calculation function\n",
    "def calculate_spatial_diversity(cluster_df, nodes_in_cluster):\n",
    "    # Ensure standardized column names\n",
    "    in_flows = cluster_df.groupby('DEST_TTWA')['TOTAL_FLOW'].sum()\n",
    "    if in_flows.sum() == 0 or len(nodes_in_cluster) <= 1: \n",
    "        return 0.0\n",
    "    prob_distribution = in_flows / in_flows.sum()\n",
    "    hc_raw = entropy(prob_distribution, base=2)\n",
    "    normalization_factor = np.log2(len(nodes_in_cluster))\n",
    "    return 0.0 if normalization_factor == 0 else hc_raw / normalization_factor\n",
    "\n",
    "# 2. Prepare hierarchy levels for analysis (final correction)\n",
    "# Find the row representing the final overall state (i.e., the largest giant component)\n",
    "final_state_row = df_percolation.loc[[df_percolation['giant_component_size'].idxmax()]]\n",
    "\n",
    "# *** Key correction: explicitly get the row where threshold is 0 ***\n",
    "tau_zero_row = df_percolation[df_percolation['threshold'] == 0]\n",
    "\n",
    "# Merge identified \"critical points\", \"max cluster state\", and \"Ï„=0 state\", and remove duplicates\n",
    "df_for_diversity_calc = pd.concat([critical_transitions_df, final_state_row, tau_zero_row]).drop_duplicates(\n",
    "    subset=['threshold', 'giant_component_size']\n",
    ").sort_values('threshold', ascending=False)\n",
    "\n",
    "print(f\"âœ… Preparation complete, will analyze spatial diversity for {len(df_for_diversity_calc)} key levels.\")\n",
    "\n",
    "# 3. Perform calculation\n",
    "# Ensure standardized column names\n",
    "df_raw_data = df_with_weights.copy()\n",
    "df_raw_data.rename(columns={\n",
    "    'origin_ttwa': 'ORIGIN_TTWA', 'destination_ttwa': 'DEST_TTWA',\n",
    "    'total_flow': 'TOTAL_FLOW'\n",
    "}, inplace=True, errors='ignore')\n",
    "\n",
    "hc_results = []\n",
    "for index, row in df_for_diversity_calc.iterrows():\n",
    "    tau, clusters_at_tau = row['threshold'], row['clusters']\n",
    "    for i, cluster_nodes in enumerate(clusters_at_tau):\n",
    "        if len(cluster_nodes) > 1:\n",
    "            internal_flows_df = df_raw_data[\n",
    "                (df_raw_data['ORIGIN_TTWA'].isin(cluster_nodes)) & \n",
    "                (df_raw_data['DEST_TTWA'].isin(cluster_nodes))\n",
    "            ].copy()\n",
    "            hc_value = calculate_spatial_diversity(internal_flows_df, cluster_nodes)\n",
    "            hc_results.append({\n",
    "                'threshold': tau, \n",
    "                'cluster_id': f\"cluster_{i+1}_at_tau_{tau:.4f}\", \n",
    "                'cluster_size': len(cluster_nodes), \n",
    "                'Hc_Spatial_Diversity': hc_value,\n",
    "                'nodes': \", \".join(sorted(list(cluster_nodes)))\n",
    "            })\n",
    "            \n",
    "df_hc_results = pd.DataFrame(hc_results)\n",
    "print(\"âœ… Spatial diversity calculation complete! 'df_hc_results' DataFrame generated.\")\n",
    "print(\"Result preview (last row should be for Ï„=0.0):\")\n",
    "print(df_hc_results.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Part 2: Saving diversity analysis results ---\")\n",
    "\n",
    "# Define output folder and file name\n",
    "output_folder = 'commuting_diversity_filter'\n",
    "file_hc_diversity = os.path.join(output_folder, 'spatial_diversity_Hc_results_final.csv')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    if 'df_hc_results' in locals():\n",
    "        df_hc_results.to_csv(file_hc_diversity, index=False)\n",
    "        print(f\"âœ… Successfully saved diversity results to: {file_hc_diversity}\")\n",
    "    else:\n",
    "        print(\"âŒ Error: 'df_hc_results' not found. Please make sure the first part of the code has run successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unknown error occurred during saving: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data structure\n",
    "print(\"Checking the structure of df_hc_results:\")\n",
    "print(df_hc_results.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df_hc_results.dtypes)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_hc_results.columns.tolist())\n",
    "print(\"\\nExample content of the 'nodes' column:\")\n",
    "print(f\"First row nodes: {df_hc_results.iloc[0]['nodes']}\")\n",
    "print(f\"Type: {type(df_hc_results.iloc[0]['nodes'])}\")\n",
    "\n",
    "# Check if 'threshold' column exists\n",
    "if 'threshold' in df_hc_results.columns:\n",
    "    print(\"\\nRows with Ï„=0:\")\n",
    "    tau_zero_rows = df_hc_results[df_hc_results['threshold'] == 0]\n",
    "    print(tau_zero_rows)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ 'threshold' column not found!\")\n",
    "    print(\"Full info of df_hc_results:\")\n",
    "    print(df_hc_results.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c19f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data structure and fix column names\n",
    "print(\"=== Checking data structure ===\")\n",
    "print(\"Columns in df_with_weights:\")\n",
    "print(df_with_weights.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_with_weights.head())\n",
    "\n",
    "# Find correct column names\n",
    "origin_col = None\n",
    "destination_col = None\n",
    "flow_col = None\n",
    "\n",
    "for col in df_with_weights.columns:\n",
    "    if 'origin' in col.lower():\n",
    "        origin_col = col\n",
    "    elif 'dest' in col.lower():\n",
    "        destination_col = col\n",
    "    elif 'flow' in col.lower():\n",
    "        flow_col = col\n",
    "\n",
    "print(\"\\nIdentified column names:\")\n",
    "print(f\"Origin column: {origin_col}\")\n",
    "print(f\"Destination column: {destination_col}\")\n",
    "print(f\"Flow column: {flow_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51971459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Solution - Step 1: Calculate H_C for all intermediate clusters\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from ast import literal_eval # Safely convert string to list\n",
    "\n",
    "print(\"--- Step 1: Start calculating spatial diversity H_C for all intermediate clusters ---\")\n",
    "\n",
    "# 1. Define H_C calculation function (confirmed version)\n",
    "def calculate_spatial_diversity(cluster_df, nodes_in_cluster):\n",
    "    \"\"\"Calculate spatial diversity H_C\"\"\"\n",
    "    num_nodes = len(nodes_in_cluster)\n",
    "    if num_nodes <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    in_flows = cluster_df.groupby('DEST_TTWA')['TOTAL_FLOW'].sum()\n",
    "    total_flow = in_flows.sum()\n",
    "    \n",
    "    if total_flow == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    prob_distribution = in_flows / total_flow\n",
    "    hc_raw = entropy(prob_distribution, base=2)\n",
    "    normalization_factor = np.log2(num_nodes)\n",
    "    \n",
    "    return 0.0 if normalization_factor == 0 else hc_raw / normalization_factor\n",
    "\n",
    "# 2. Initialize an empty dictionary to store results\n",
    "hc_lookup = {}\n",
    "\n",
    "# 3. Iterate over critical_transitions_df to calculate H_C for each cluster\n",
    "#    Assume 'clusters' column stores cluster node lists as string representation\n",
    "for index, row in critical_transitions_df.iterrows():\n",
    "    # Safely convert string '[...]' to Python list\n",
    "    # If your 'clusters' column is already a list, you can remove literal_eval\n",
    "    try:\n",
    "        clusters_at_tau = literal_eval(row['clusters'])\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If already a list, use directly\n",
    "        clusters_at_tau = row['clusters']\n",
    "\n",
    "    for cluster_nodes in clusters_at_tau:\n",
    "        # Only consider clusters with size > 1\n",
    "        if len(cluster_nodes) > 1:\n",
    "            # Filter internal flows for this cluster\n",
    "            internal_flows = df_with_weights[\n",
    "                df_with_weights['ORIGIN_TTWA'].isin(cluster_nodes) &\n",
    "                df_with_weights['DEST_TTWA'].isin(cluster_nodes)\n",
    "            ].copy()\n",
    "            \n",
    "            # Calculate H_C value\n",
    "            hc_value = calculate_spatial_diversity(internal_flows, cluster_nodes)\n",
    "            \n",
    "            # Store result in dictionary, use frozenset as key\n",
    "            hc_lookup[frozenset(cluster_nodes)] = hc_value\n",
    "\n",
    "print(f\"âœ… Step 1 complete: Successfully calculated H_C for {len(hc_lookup)} clusters.\")\n",
    "print(f\"   The variable 'hc_lookup' is now ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "# ============================================================================\n",
    "# [Correction] Step 0: Create all_leaves variable\n",
    "# ============================================================================\n",
    "print(\"--- [Correction] Defining 'all_leaves' ---\")\n",
    "\n",
    "def get_all_leaves_from_tree(graph):\n",
    "    \"\"\"Get all leaf nodes (nodes with out-degree 0) from a tree graph\"\"\"\n",
    "    if not isinstance(graph, nx.DiGraph):\n",
    "        print(\"âŒ 'tree_graph' variable is not a valid graph.\")\n",
    "        return frozenset()\n",
    "    return frozenset(node for node in graph.nodes() if graph.out_degree(node) == 0)\n",
    "\n",
    "try:\n",
    "    # Create all_leaves variable from your tree_graph\n",
    "    all_leaves = get_all_leaves_from_tree(tree_graph)\n",
    "    if all_leaves:\n",
    "        print(f\"âœ… 'all_leaves' created successfully, containing {len(all_leaves)} leaf nodes.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Warning: No leaf nodes found in 'tree_graph'.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"âŒ Fatal error: 'tree_graph' variable is not defined. Please make sure its creation code has been run.\")\n",
    "    all_leaves = None  # Set to None to prevent subsequent code from running\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Your original code (now safe to run)\n",
    "# ============================================================================\n",
    "if all_leaves is not None:\n",
    "    # Use correct column names to calculate spatial diversity at Ï„=0\n",
    "    print(\"\\n=== Calculating spatial diversity at Ï„=0 ===\")\n",
    "\n",
    "    # 1. Get internal flow data for all leaf nodes\n",
    "    # Assume df_with_weights and hc_lookup already exist\n",
    "    try:\n",
    "        internal_flows_complete = df_with_weights[\n",
    "            (df_with_weights['ORIGIN_TTWA'].isin(all_leaves)) & \n",
    "            (df_with_weights['DEST_TTWA'].isin(all_leaves))\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"Internal flow data at Ï„=0: {len(internal_flows_complete)} records\")\n",
    "        print(f\"Total flow: {internal_flows_complete['TOTAL_FLOW'].sum():,.0f}\")\n",
    "\n",
    "        # 2. Corrected diversity calculation function\n",
    "        def calculate_spatial_diversity_corrected(cluster_df, nodes_in_cluster):\n",
    "            \"\"\"Calculate spatial diversity H_C (corrected version)\"\"\"\n",
    "            if len(nodes_in_cluster) <= 1:\n",
    "                return 0.0\n",
    "            \n",
    "            in_flows = cluster_df.groupby('DEST_TTWA')['TOTAL_FLOW'].sum()\n",
    "            \n",
    "            if in_flows.sum() == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            prob_distribution = in_flows / in_flows.sum()\n",
    "            hc_raw = entropy(prob_distribution, base=2)\n",
    "            normalization_factor = np.log2(len(nodes_in_cluster))\n",
    "            \n",
    "            return 0.0 if normalization_factor == 0 else hc_raw / normalization_factor\n",
    "\n",
    "        # 3. Calculate spatial diversity for the complete cluster\n",
    "        hc_complete = calculate_spatial_diversity_corrected(internal_flows_complete, all_leaves)\n",
    "        print(f\"\\nSpatial diversity H_C at Ï„=0 = {hc_complete:.4f}\")\n",
    "\n",
    "        # 4. Create complete color mapping\n",
    "        print(\"\\nCreating complete color mapping...\")\n",
    "        complete_hc_lookup = hc_lookup.copy()\n",
    "        complete_hc_lookup[frozenset(all_leaves)] = hc_complete\n",
    "\n",
    "        print(f\"Original mapping count: {len(hc_lookup)}\")\n",
    "        print(f\"Complete mapping count: {len(complete_hc_lookup)}\")\n",
    "        print(f\"H_C value for the largest cluster: {complete_hc_lookup[frozenset(all_leaves)]:.4f}\")\n",
    "\n",
    "        # Save the complete mapping\n",
    "        hc_lookup_complete = complete_hc_lookup\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"\\nâŒ NameError occurred: {e}. Please ensure 'df_with_weights' and 'hc_lookup' variables are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spatial diversity at Ï„=0 using correct column names\n",
    "print(\"=== Calculating spatial diversity at Ï„=0 ===\")\n",
    "\n",
    "# 1. Get internal flow data for all leaf nodes\n",
    "internal_flows_complete = df_with_weights[\n",
    "    (df_with_weights['ORIGIN_TTWA'].isin(all_leaves)) & \n",
    "    (df_with_weights['DEST_TTWA'].isin(all_leaves))\n",
    "].copy()\n",
    "\n",
    "print(f\"Internal flow data at Ï„=0: {len(internal_flows_complete)} records\")\n",
    "print(f\"Total flow: {internal_flows_complete['TOTAL_FLOW'].sum():,.0f}\")\n",
    "\n",
    "# 2. Corrected diversity calculation function\n",
    "def calculate_spatial_diversity_corrected(cluster_df, nodes_in_cluster):\n",
    "    \"\"\"Calculate spatial diversity H_C (corrected version)\"\"\"\n",
    "    if len(nodes_in_cluster) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate inflow for each destination\n",
    "    in_flows = cluster_df.groupby('DEST_TTWA')['TOTAL_FLOW'].sum()\n",
    "    \n",
    "    if in_flows.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate probability distribution\n",
    "    prob_distribution = in_flows / in_flows.sum()\n",
    "    \n",
    "    # Calculate raw entropy\n",
    "    from scipy.stats import entropy\n",
    "    hc_raw = entropy(prob_distribution, base=2)\n",
    "    \n",
    "    # Normalization\n",
    "    normalization_factor = np.log2(len(nodes_in_cluster))\n",
    "    \n",
    "    return 0.0 if normalization_factor == 0 else hc_raw / normalization_factor\n",
    "\n",
    "# 3. Calculate spatial diversity for the complete cluster\n",
    "hc_complete = calculate_spatial_diversity_corrected(internal_flows_complete, all_leaves)\n",
    "print(f\"\\nSpatial diversity H_C at Ï„=0 = {hc_complete:.4f}\")\n",
    "\n",
    "# 4. Create complete color mapping\n",
    "print(\"\\nCreating complete color mapping...\")\n",
    "complete_hc_lookup = hc_lookup.copy()\n",
    "complete_hc_lookup[frozenset(all_leaves)] = hc_complete\n",
    "\n",
    "print(f\"Original mapping count: {len(hc_lookup)}\")\n",
    "print(f\"Complete mapping count: {len(complete_hc_lookup)}\")\n",
    "print(f\"H_C value for the largest cluster: {complete_hc_lookup[frozenset(all_leaves)]:.4f}\")\n",
    "\n",
    "# Save the complete mapping\n",
    "hc_lookup_complete = complete_hc_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9def115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the key format of complete_hc_lookup\n",
    "print(\"=== Checking key format of complete_hc_lookup ===\")\n",
    "print(f\"complete_hc_lookup contains {len(complete_hc_lookup)} entries\")\n",
    "\n",
    "# Show first few keys\n",
    "sample_keys = list(complete_hc_lookup.keys())[:5]\n",
    "print(f\"Example of first 5 keys: {sample_keys}\")\n",
    "\n",
    "# Show key type\n",
    "print(f\"Type of key: {type(sample_keys[0]) if sample_keys else 'N/A'}\")\n",
    "\n",
    "# Show format of tree_graph nodes\n",
    "tree_nodes = list(tree_graph.nodes())[:10]\n",
    "print(f\"First 10 nodes in tree_graph: {tree_nodes}\")\n",
    "print(f\"Node type: {type(tree_nodes[0]) if tree_nodes else 'N/A'}\")\n",
    "\n",
    "# Check for matching keys\n",
    "matches = 0\n",
    "for node in tree_nodes:\n",
    "    if node in complete_hc_lookup:\n",
    "        matches += 1\n",
    "\n",
    "print(f\"Number of tree_graph nodes matching keys in complete_hc_lookup: {matches}\")\n",
    "\n",
    "# Show some specific values\n",
    "if complete_hc_lookup:\n",
    "    first_key = list(complete_hc_lookup.keys())[0]\n",
    "    print(f\"First key: {first_key}\")\n",
    "    print(f\"Corresponding value: {complete_hc_lookup[first_key]}\")\n",
    "    \n",
    "    # If key is frozenset, check its contents\n",
    "    if isinstance(first_key, frozenset):\n",
    "        print(f\"Frozenset contents: {first_key}\")\n",
    "        print(f\"Frozenset length: {len(first_key)}\")\n",
    "        print(f\"First element in frozenset: {list(first_key)[0] if first_key else 'Empty'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b833f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import K-Means clustering algorithm\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "except ImportError:\n",
    "    print(\"=\"*60)\n",
    "    print(\"âŒ Error: This script requires the 'scikit-learn' library.\")\n",
    "    print(\"Please run: pip install scikit-learn in your terminal or notebook.\")\n",
    "    print(\"=\"*60)\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Final Solution: Advanced layout & H_C K-Means coloring (light leaf nodes)\n",
    "# ============================================================================\n",
    "\n",
    "def plot_dendrogram_with_hc_coloring(tree_graph, builder, hc_lookup_complete):\n",
    "    \"\"\"\n",
    "    Plot dendrogram with advanced layout and H_C K-Means coloring scheme.\n",
    "    \n",
    "    Args:\n",
    "        tree_graph (nx.DiGraph): Directed graph representation of the tree from percolation analysis.\n",
    "        builder (CorrectDendrogramBuilder): Builder object containing all clustering details.\n",
    "        hc_lookup_complete (dict): Dictionary with {frozenset(cities): H_C_value}.\n",
    "    \"\"\"\n",
    "    # [Step 1] Key data check\n",
    "    valid_hc_values = [v for v in hc_lookup_complete.values() if v is not None and np.isfinite(v)]\n",
    "    if len(valid_hc_values) < 5:\n",
    "        print(\"=\"*70)\n",
    "        print(\"âŒ Fatal Error: Plotting function terminated.\")\n",
    "        print(f\"   Reason: The input 'hc_lookup_complete' contains fewer than 5 valid H_C values (currently {len(valid_hc_values)}).\")\n",
    "        print(\"   This is insufficient for K-Means advanced coloring (requires at least 5 data points).\")\n",
    "        print(\"\\n   >>> Solution: Please check your data processing to ensure H_C values are computed for all intermediate clusters.\")\n",
    "        print(\"=\"*70)\n",
    "        return\n",
    "\n",
    "    print(\"--- Launching final plotting script (light leaf nodes) ---\")\n",
    "    print(f\"âœ… Data check passed, {len(valid_hc_values)} valid H_C values, advanced coloring will start.\")\n",
    "\n",
    "    # --- Step 2: Advanced layout calculation (adopted from previous final version) ---\n",
    "    print(\"\\n[Layout] Calculating advanced layout (ordinal height, weighted X-axis, persistence nodes)...\")\n",
    "    \n",
    "    # a. Create \"ordinal height\" mapping\n",
    "    all_unique_heights = sorted(list(set(c.height for c in builder.all_clusters.values())))\n",
    "    height_to_y_level_map = {height: i for i, height in enumerate(all_unique_heights)}\n",
    "\n",
    "    # b. Calculate node weights (for X-axis layout)\n",
    "    memo = {}\n",
    "    def count_cities_under_weighted(node):\n",
    "        if node in memo: return memo[node]\n",
    "        if tree_graph.out_degree(node) == 0: memo[node] = 1.0; return 1.0\n",
    "        count = sum(count_cities_under_weighted(child) for child in tree_graph.successors(node))\n",
    "        memo[node] = count\n",
    "        return count\n",
    "    root_node = [n for n, d in tree_graph.in_degree() if d == 0][0]\n",
    "    count_cities_under_weighted(root_node)\n",
    "\n",
    "    # c. Compute base layout for all \"real\" nodes\n",
    "    real_pos = {}\n",
    "    nodes_to_process = sorted(tree_graph.nodes(), key=lambda n: builder.all_clusters[n].height, reverse=True)\n",
    "    leaf_spacing_factor = 1.2\n",
    "    real_pos[root_node] = (0.0, height_to_y_level_map[builder.all_clusters[root_node].height])\n",
    "    for parent_node in nodes_to_process:\n",
    "        if parent_node not in real_pos: continue\n",
    "        children = sorted(list(tree_graph.successors(parent_node)), key=lambda n: count_cities_under_weighted(n))\n",
    "        if not children: continue\n",
    "        parent_x, _ = real_pos[parent_node]\n",
    "        total_child_width = sum(count_cities_under_weighted(c) for c in children) * leaf_spacing_factor\n",
    "        current_x = parent_x - total_child_width / 2.0\n",
    "        for child in children:\n",
    "            child_width = count_cities_under_weighted(child) * leaf_spacing_factor\n",
    "            child_x = current_x + child_width / 2.0\n",
    "            child_y = height_to_y_level_map[builder.all_clusters[child].height]\n",
    "            real_pos[child] = (child_x, child_y)\n",
    "            current_x += child_width\n",
    "            \n",
    "    # d. Create \"enhanced\" nodes and edges for visualization (persistence nodes)\n",
    "    viz_pos, viz_edges = dict(real_pos), []\n",
    "    for parent, child in tree_graph.edges():\n",
    "        parent_y, child_y = real_pos[parent][1], real_pos[child][1]\n",
    "        if parent_y > child_y + 1:\n",
    "            last_node_id = child\n",
    "            for level in range(int(child_y) + 1, int(parent_y)):\n",
    "                persistence_node_id = f\"p_{child}_{level}\"\n",
    "                viz_pos[persistence_node_id] = (real_pos[child][0], level)\n",
    "                viz_edges.append((last_node_id, persistence_node_id))\n",
    "                last_node_id = persistence_node_id\n",
    "            viz_edges.append((last_node_id, parent))\n",
    "        else:\n",
    "            viz_edges.append((child, parent))\n",
    "    print(\"âœ… Advanced layout calculation complete.\")\n",
    "\n",
    "    # --- Step 3: K-Means color grouping calculation (adopted from your V3 script) ---\n",
    "    print(\"\\n[Coloring] Performing K-Means color grouping calculation...\")\n",
    "    \n",
    "    node_hc_map = {\n",
    "        n: hc_lookup_complete.get(frozenset(builder.all_clusters[n].cities))\n",
    "        for n in tree_graph.nodes()\n",
    "    }\n",
    "    \n",
    "    valid_nodes = {n: hc for n, hc in node_hc_map.items() if hc is not None and np.isfinite(hc)}\n",
    "    hc_values_array = np.array(list(valid_nodes.values())).reshape(-1, 1)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto').fit(hc_values_array)\n",
    "    node_to_cluster_label = {node: kmeans.predict(np.array([[hc]]))[0] for node, hc in valid_nodes.items()}\n",
    "    \n",
    "    cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "    sorted_center_indices = np.argsort(cluster_centers)\n",
    "    rank_map = {original_label: rank for rank, original_label in enumerate(sorted_center_indices)}\n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, 5))\n",
    "    print(\"âœ… K-Means coloring scheme calculation complete.\")\n",
    "\n",
    "    # --- Step 4: Final plotting ---\n",
    "    print(\"\\n[Plotting] Starting final image rendering...\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(40, 26))\n",
    "    \n",
    "    # a. Draw edges\n",
    "    for u, v in viz_edges:\n",
    "        if u in viz_pos and v in viz_pos:\n",
    "            ax.plot([viz_pos[u][0], viz_pos[v][0]], [viz_pos[u][1], viz_pos[v][1]], \n",
    "                   color='#cccccc', zorder=1, linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # b. Draw nodes\n",
    "    unweighted_city_counts = {n: len(builder.all_clusters[n].cities) for n in tree_graph.nodes()}\n",
    "    for node, p in viz_pos.items():\n",
    "        is_persistence_node = str(node).startswith('p_')\n",
    "        original_node_id = str(node).split('_')[1] if is_persistence_node else node\n",
    "        \n",
    "        size = 8 + unweighted_city_counts.get(original_node_id, 1)**0.7 * 50\n",
    "        cities_count = unweighted_city_counts.get(original_node_id, 0)\n",
    "        \n",
    "        # Default edge color\n",
    "        edge_color = 'black'\n",
    "        \n",
    "        # Coloring logic\n",
    "        if cities_count <= 1:\n",
    "            # ==========================================================\n",
    "            # --- Core modification: apply lighter gray for leaf nodes ---\n",
    "            # ==========================================================\n",
    "            color = 'lightgray'  # Changed from 'gray' to 'lightgray'\n",
    "            edge_color = 'darkgray' # Border changed to dark gray to match light fill\n",
    "            \n",
    "        elif original_node_id in valid_nodes:\n",
    "            # Valid node, use K-Means result for coloring\n",
    "            cluster_label = node_to_cluster_label[original_node_id]\n",
    "            color_rank = rank_map[cluster_label]\n",
    "            color = colors[color_rank]\n",
    "        else:\n",
    "            # Node without H_C value, use white\n",
    "            color = 'white'\n",
    "        \n",
    "        ax.scatter(p[0], p[1], s=size, facecolor=color, edgecolor=edge_color, \n",
    "                   zorder=3, alpha=0.9, linewidth=1.5)\n",
    "\n",
    "    # c. Add custom H_C legend\n",
    "    ax_legend = fig.add_axes([0.8, 0.75, 0.015, 0.15])\n",
    "    custom_cmap = mcolors.ListedColormap(colors)\n",
    "    legend_data = np.arange(len(colors)).reshape(-1, 1)\n",
    "    ax_legend.imshow(legend_data, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "    ax_legend.set_title('$H_C$', fontsize=20, pad=10)\n",
    "    ax_legend.text(1.8, 0.95, '+', transform=ax_legend.transAxes, ha='left', va='center', fontsize=24)\n",
    "    ax_legend.text(1.8, 0.05, 'âˆ’', transform=ax_legend.transAxes, ha='left', va='center', fontsize=28)\n",
    "    ax_legend.set_xticks([]); ax_legend.set_yticks([])\n",
    "    ax_legend.spines['top'].set_visible(False); ax_legend.spines['right'].set_visible(False)\n",
    "    ax_legend.spines['bottom'].set_visible(False); ax_legend.spines['left'].set_visible(False)\n",
    "    \n",
    "    # d. Final style\n",
    "    # No title\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "# --- Run main function ---\n",
    "if 'tree_graph' in globals() and 'percolation_tree_builder' in globals() and 'hc_lookup_complete' in globals():\n",
    "    plot_dendrogram_with_hc_coloring(tree_graph, percolation_tree_builder, hc_lookup_complete)\n",
    "else:\n",
    "    print(\"\\nâŒ Error: Please prepare 'tree_graph', 'percolation_tree_builder', and 'hc_lookup_complete' before calling this function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1012f8",
   "metadata": {},
   "source": [
    "##### Skill diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your data file name\n",
    "skill_file = 'skill.csv'\n",
    "\n",
    "print(f\"--- Start processing skill file: '{skill_file}' (final cleaned version) ---\")\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Load data ---\n",
    "    rows_to_skip = 7 \n",
    "    df_raw = pd.read_csv(skill_file, skiprows=rows_to_skip)\n",
    "    print(\"âœ… File loaded successfully!\")\n",
    "\n",
    "    # --- Step 2: (Core fix) Intelligently remove invalid rows at the end of the file ---\n",
    "    ttwa_col_name = 'travel to work area 2011-based'\n",
    "    \n",
    "    # Check if 'Column Total' exists in TTWA column\n",
    "    if 'Column Total' in df_raw[ttwa_col_name].values:\n",
    "        # Find the first occurrence of 'Column Total'\n",
    "        cutoff_index = df_raw[df_raw[ttwa_col_name] == 'Column Total'].index[0]\n",
    "        # Keep only rows before 'Column Total'\n",
    "        df_cleaned = df_raw.loc[:cutoff_index-1].copy()\n",
    "        print(f\"âœ… Successfully removed 'Column Total' and all rows after it.\")\n",
    "    else:\n",
    "        # If not found, just drop empty rows\n",
    "        df_cleaned = df_raw.dropna(subset=[ttwa_col_name]).copy()\n",
    "        print(\"âš ï¸ 'Column Total' row not found, only dropped empty rows.\")\n",
    "\n",
    "    # --- Step 3: Select required columns ---\n",
    "    numerator_cols = [col for col in df_cleaned.columns if col.startswith('Numerator')]\n",
    "    \n",
    "    if len(numerator_cols) != 9:\n",
    "        print(f\"âš ï¸ Warning: Found {len(numerator_cols)} 'Numerator' columns, not the expected 9.\")\n",
    "\n",
    "    cols_to_keep = [ttwa_col_name] + numerator_cols\n",
    "    df_selected = df_cleaned[cols_to_keep].copy()\n",
    "    print(\"âœ… Successfully selected TTWA name and 9 skill count columns.\")\n",
    "\n",
    "    # --- Step 4: Rename columns and clean data types ---\n",
    "    rename_mapping = {ttwa_col_name: 'TTWA_NAME'}\n",
    "    for i, old_col_name in enumerate(numerator_cols):\n",
    "        rename_mapping[old_col_name] = f'skill_{i+1}_workers'\n",
    "        \n",
    "    df_renamed = df_selected.rename(columns=rename_mapping)\n",
    "    \n",
    "    final_skill_columns = [f'skill_{i+1}_workers' for i in range(9)]\n",
    "    for col in final_skill_columns:\n",
    "        df_renamed[col] = pd.to_numeric(df_renamed[col], errors='coerce')\n",
    "        \n",
    "    df_skills_final = df_renamed.fillna(0)\n",
    "    for col in final_skill_columns:\n",
    "        df_skills_final[col] = df_skills_final[col].astype(int)\n",
    "\n",
    "    print(\"âœ… Column renaming and data type cleaning completed.\")\n",
    "\n",
    "    # --- Step 5: Output final result ---\n",
    "    print(\"\\n\\n--- All data processing completed ---\")\n",
    "    print(\"Preview of final cleaned skill data:\")\n",
    "    print(df_skills_final.head())\n",
    "    \n",
    "    output_filename = 'cleaned_ttwa_skills.csv'\n",
    "    df_skills_final.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nâœ… Cleaned data saved to file: '{output_filename}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Fatal error: File '{skill_file}' not found.\")\n",
    "except KeyError:\n",
    "    print(f\"âŒ Fatal error: Column 'travel to work area 2011-based' not found.\")\n",
    "    print(f\"   This is likely due to incorrect `skiprows={rows_to_skip}` setting. Try adjusting this number.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unknown error occurred during processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# Independent Skill Diversity (H_S) Calculation Script (Corrected)\n",
    "# ============================================================================\n",
    "\n",
    "# --- Step 1: Define input file name ---\n",
    "skills_file = 'cleaned_ttwa_skills.csv'\n",
    "output_folder = 'skill_diversity_filter'\n",
    "hs_results_file = os.path.join(output_folder, 'skills_diversity_Hs_results.csv')\n",
    "\n",
    "print(f\"--- Starting independent skill diversity (H_S) analysis ---\")\n",
    "\n",
    "# --- Step 2: Check and load required data ---\n",
    "if 'critical_transitions_df' not in globals():\n",
    "    print(f\"âŒ Fatal error: Missing 'critical_transitions_df' variable. Please run percolation analysis and critical point identification first.\")\n",
    "else:\n",
    "    try:\n",
    "        df_skills_raw = pd.read_csv(skills_file)\n",
    "        df_skills_raw.set_index('TTWA_NAME', inplace=True)\n",
    "        print(\"âœ… Skill data file loaded successfully.\")\n",
    "\n",
    "        # --- Step 3: Merge 9 occupation categories into 4 skill levels ---\n",
    "        print(\"Merging 9 occupation categories into 4 skill levels...\")\n",
    "        df_skills_merged = pd.DataFrame(index=df_skills_raw.index)\n",
    "        df_skills_merged['skill_level_4'] = df_skills_raw['skill_1_workers'] + df_skills_raw['skill_2_workers']\n",
    "        df_skills_merged['skill_level_3'] = df_skills_raw['skill_3_workers']\n",
    "        df_skills_merged['skill_level_2'] = df_skills_raw['skill_4_workers'] + df_skills_raw['skill_5_workers'] + df_skills_raw['skill_6_workers'] + df_skills_raw['skill_7_workers']\n",
    "        df_skills_merged['skill_level_1'] = df_skills_raw['skill_8_workers'] + df_skills_raw['skill_9_workers']\n",
    "        print(\"âœ… Skill category merging complete.\")\n",
    "\n",
    "        # --- Step 4: Define H_S calculation function ---\n",
    "        def calculate_skills_diversity_final(skill_counts_for_cluster):\n",
    "            NUM_CATEGORIES = 4.0\n",
    "            total_workers = skill_counts_for_cluster.sum()\n",
    "            if total_workers == 0: return 0.0\n",
    "            prob_distribution = skill_counts_for_cluster[skill_counts_for_cluster > 0] / total_workers\n",
    "            hs_raw = entropy(prob_distribution, base=2)\n",
    "            num_skill_categories = len(prob_distribution)\n",
    "            return hs_raw / np.log2(NUM_CATEGORIES) if num_skill_categories > 1 else 0.0\n",
    "\n",
    "        # --- Step 5: Calculate H_S for each cluster in critical_transitions_df ---\n",
    "        print(\"Calculating H_S value for each cluster in 'critical_transitions_df'...\")\n",
    "        hs_results = []\n",
    "        skill_cols_to_sum = ['skill_level_1', 'skill_level_2', 'skill_level_3', 'skill_level_4']\n",
    "        \n",
    "        for index, row in critical_transitions_df.iterrows():\n",
    "            tau = row['threshold']\n",
    "            \n",
    "            # *** Core correction ***\n",
    "            # Directly use the 'clusters' column, no literal_eval conversion\n",
    "            clusters_at_tau = row['clusters']\n",
    "            \n",
    "            # Ensure clusters_at_tau is iterable\n",
    "            if not isinstance(clusters_at_tau, (list, np.ndarray)):\n",
    "                print(f\"âš ï¸ Warning: Non-list type found in clusters column at tau={tau}, skipping this row.\")\n",
    "                continue\n",
    "\n",
    "            for i, cluster_nodes in enumerate(clusters_at_tau):\n",
    "                if len(cluster_nodes) > 1:\n",
    "                    skills_in_cluster = df_skills_merged.reindex(list(cluster_nodes))\n",
    "                    total_skills_in_cluster = skills_in_cluster[skill_cols_to_sum].sum()\n",
    "                    hs_value = calculate_skills_diversity_final(total_skills_in_cluster)\n",
    "                    \n",
    "                    hs_results.append({\n",
    "                        'threshold': tau, \n",
    "                        'cluster_id': f\"cluster_{i+1}_at_tau_{tau:.4f}\", \n",
    "                        'cluster_size': len(cluster_nodes), \n",
    "                        'Hs_Skills_Diversity': hs_value,\n",
    "                        'nodes': \", \".join(sorted(list(cluster_nodes)))\n",
    "                    })\n",
    "        \n",
    "        df_hs_results = pd.DataFrame(hs_results)\n",
    "        \n",
    "        # --- Step 6: Save and preview H_S results ---\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        df_hs_results.to_csv(hs_results_file, index=False)\n",
    "        \n",
    "        print(f\"\\nâœ… Skill diversity calculation complete!\")\n",
    "        print(f\"âœ… Results saved to new file: '{hs_results_file}'\")\n",
    "        print(\"\\n--- Skill diversity (H_S) results preview ---\")\n",
    "        print(df_hs_results.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Fatal error: Skill data file '{skills_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unknown error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- Calculate global skill diversity (corrected version) ---\n",
    "\n",
    "# Assume df_skills_final and main_component_nodes already exist\n",
    "if 'df_skills_final' in globals() and 'main_component_nodes' in globals():\n",
    "    print(\"--- Step 1: Calculate skill diversity H_S for the complete network (Ï„=0) ---\")\n",
    "    \n",
    "    # Select skill data for all cities in the largest connected component\n",
    "    skills_complete_cluster_raw = df_skills_final[df_skills_final['TTWA_NAME'].isin(main_component_nodes)]\n",
    "    \n",
    "    # *** Core correction: merge 9 skill categories into 4 skill levels here ***\n",
    "    print(\"   - Merging 9 skill categories into 4 skill levels...\")\n",
    "    df_skills_merged = pd.DataFrame()\n",
    "    df_skills_merged['TTWA_NAME'] = skills_complete_cluster_raw['TTWA_NAME']\n",
    "    \n",
    "    df_skills_merged['skill_level_4'] = skills_complete_cluster_raw['skill_1_workers'] + skills_complete_cluster_raw['skill_2_workers']\n",
    "    df_skills_merged['skill_level_3'] = skills_complete_cluster_raw['skill_3_workers']\n",
    "    df_skills_merged['skill_level_2'] = (\n",
    "        skills_complete_cluster_raw['skill_4_workers'] +\n",
    "        skills_complete_cluster_raw['skill_5_workers'] +\n",
    "        skills_complete_cluster_raw['skill_6_workers'] +\n",
    "        skills_complete_cluster_raw['skill_7_workers']\n",
    "    )\n",
    "    df_skills_merged['skill_level_1'] = skills_complete_cluster_raw['skill_8_workers'] + skills_complete_cluster_raw['skill_9_workers']\n",
    "    print(\"   - Skill category merging completed.\")\n",
    "\n",
    "    # Define the column names for the 4 skill levels\n",
    "    skill_cols = ['skill_level_1', 'skill_level_2', 'skill_level_3', 'skill_level_4']\n",
    "    \n",
    "    # Sum the skill counts for all cities to get the skill composition of the whole network\n",
    "    total_skills_complete_cluster = df_skills_merged[skill_cols].sum()\n",
    "    \n",
    "    # Define H_S calculation function\n",
    "    def calculate_skills_diversity_corrected(skill_counts):\n",
    "        NUM_CATEGORIES = 4.0\n",
    "        total = skill_counts.sum()\n",
    "        if total == 0: return 0.0\n",
    "        prob = skill_counts / total\n",
    "        hs_raw = entropy(prob, base=2)\n",
    "        norm_factor = np.log2(NUM_CATEGORIES)\n",
    "        return hs_raw / norm_factor\n",
    "\n",
    "    # Calculate global H_S value\n",
    "    hs_complete = calculate_skills_diversity_corrected(total_skills_complete_cluster)\n",
    "    \n",
    "    print(f\"\\nâœ… Skill diversity H_S for the complete network (Ï„=0): {hs_complete:.4f}\")\n",
    "    \n",
    "    # Save result for later plotting\n",
    "    all_nodes_frozenset = frozenset(main_component_nodes)\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Error: Please make sure 'df_skills_final' and 'main_component_nodes' are defined first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input and output file names\n",
    "skills_9_categories_file = 'cleaned_ttwa_skills.csv'\n",
    "output_folder = 'commuting_analysis_final_results'\n",
    "skills_4_levels_file = os.path.join(output_folder, 'skills_4_levels.csv')\n",
    "\n",
    "print(\"--- Start merging 9 skill categories into 4 skill levels ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the cleaned skill data\n",
    "    df_skills_9_cat = pd.read_csv(skills_9_categories_file)\n",
    "    print(f\"Successfully loaded file: '{skills_9_categories_file}'\")\n",
    "\n",
    "    # 2. Create a new DataFrame for merged results\n",
    "    df_skills_4_levels = pd.DataFrame()\n",
    "    \n",
    "    # Copy TTWA name column\n",
    "    df_skills_4_levels['TTWA_NAME'] = df_skills_9_cat['TTWA_NAME']\n",
    "    \n",
    "    # 3. Merge according to official mapping\n",
    "    # Skill level 4 (highest): SOC major groups 1, 2\n",
    "    df_skills_4_levels['skill_level_4'] = df_skills_9_cat['skill_1_workers'] + df_skills_9_cat['skill_2_workers']\n",
    "    \n",
    "    # Skill level 3: SOC major group 3\n",
    "    df_skills_4_levels['skill_level_3'] = df_skills_9_cat['skill_3_workers']\n",
    "    \n",
    "    # Skill level 2: SOC major groups 4, 5, 6, 7\n",
    "    df_skills_4_levels['skill_level_2'] = (\n",
    "        df_skills_9_cat['skill_4_workers'] +\n",
    "        df_skills_9_cat['skill_5_workers'] +\n",
    "        df_skills_9_cat['skill_6_workers'] +\n",
    "        df_skills_9_cat['skill_7_workers']\n",
    "    )\n",
    "    \n",
    "    # Skill level 1 (lowest): SOC major groups 8, 9\n",
    "    df_skills_4_levels['skill_level_1'] = df_skills_9_cat['skill_8_workers'] + df_skills_9_cat['skill_9_workers']\n",
    "    \n",
    "    print(\"Successfully merged 9 categories into 4 skill levels.\")\n",
    "\n",
    "    # 4. Save and preview the new data\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    df_skills_4_levels.to_csv(skills_4_levels_file, index=False)\n",
    "    \n",
    "    print(f\"\\nMerged data saved to: '{skills_4_levels_file}'\")\n",
    "    print(\"\\n--- Preview of merged data ---\")\n",
    "    print(df_skills_4_levels.head())\n",
    "    \n",
    "    # Example: show skill distribution for London\n",
    "    print(\"\\nSkill level distribution for 'London':\")\n",
    "    london_skills = df_skills_4_levels[df_skills_4_levels['TTWA_NAME'] == 'London']\n",
    "    if not london_skills.empty:\n",
    "        print(london_skills)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fatal error: Input file '{skills_9_categories_file}' not found. Please check the file name.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Fatal error: Required skill column not found in file. Missing column: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unknown error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hs_lookup_complete dictionary from df_hs_results\n",
    "print(\"=== Building hs_lookup_complete from df_hs_results ===\")\n",
    "\n",
    "# Construct lookup dictionary from df_hs_results\n",
    "hs_lookup_complete = {}\n",
    "\n",
    "for index, row in df_hs_results.iterrows():\n",
    "    nodes_str = row['nodes']\n",
    "    hs_value = row['Hs_Skills_Diversity']\n",
    "    \n",
    "    # Convert node string to set of city names\n",
    "    if isinstance(nodes_str, str):\n",
    "        city_names = [name.strip() for name in nodes_str.split(',')]\n",
    "        city_set = frozenset(city_names)\n",
    "        hs_lookup_complete[city_set] = hs_value\n",
    "\n",
    "print(f\"âœ… Successfully built hs_lookup_complete with {len(hs_lookup_complete)} keys\")\n",
    "\n",
    "# Add skill diversity for the complete network (Ï„=0)\n",
    "def get_all_leaves_from_tree(tree_graph):\n",
    "    \"\"\"Get all leaf nodes from the tree graph\"\"\"\n",
    "    leaves = set()\n",
    "    for node in tree_graph.nodes():\n",
    "        if tree_graph.out_degree(node) == 0:  # Leaf node\n",
    "            leaves.add(node)\n",
    "    return frozenset(leaves)\n",
    "\n",
    "# Get all cities in the complete network\n",
    "all_cities = get_all_leaves_from_tree(tree_graph)\n",
    "print(f\"Complete network contains {len(all_cities)} cities\")\n",
    "\n",
    "# Add skill diversity for the complete network\n",
    "hs_lookup_complete[all_cities] = hs_complete\n",
    "print(f\"âœ… Added skill diversity for the complete network: H_S = {hs_complete:.4f}\")\n",
    "\n",
    "# Validate data\n",
    "print(f\"\\nFinal hs_lookup_complete contains {len(hs_lookup_complete)} keys\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSkill diversity sample data:\")\n",
    "for i, (key, value) in enumerate(list(hs_lookup_complete.items())[:3]):\n",
    "    print(f\"  Key {i+1}: {list(key)[:3]}{'...' if len(key) > 3 else ''} (size: {len(key)})\")\n",
    "    print(f\"    H_S value: {value:.4f}\")\n",
    "\n",
    "# Check matching with tree graph\n",
    "print(\"\\nChecking matches with tree graph:\")\n",
    "matches = 0\n",
    "internal_nodes = [n for n in tree_graph.nodes() if tree_graph.out_degree(n) > 0]\n",
    "\n",
    "for node in internal_nodes:\n",
    "    leaves = frozenset(get_leaves_under_node_debug(node, tree_graph))\n",
    "    if leaves in hs_lookup_complete:\n",
    "        matches += 1\n",
    "        \n",
    "print(f\"Number of matching nodes: {matches} / {len(internal_nodes)}\")\n",
    "\n",
    "# Check largest cluster (should be the complete network)\n",
    "max_cluster_size = max(len(k) for k in hs_lookup_complete.keys())\n",
    "max_cluster = [k for k in hs_lookup_complete.keys() if len(k) == max_cluster_size][0]\n",
    "print(f\"\\nLargest cluster size: {max_cluster_size}\")\n",
    "print(f\"Skill diversity of complete network: {hs_lookup_complete[max_cluster]:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… hs_lookup_complete is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Corrected code block\n",
    "# ============================================================================\n",
    "\n",
    "# Build hs_lookup_complete dictionary from df_hs_results\n",
    "print(\"=== Building hs_lookup_complete from df_hs_results ===\")\n",
    "\n",
    "# Construct lookup dictionary from df_hs_results\n",
    "hs_lookup_complete = {}\n",
    "\n",
    "for index, row in df_hs_results.iterrows():\n",
    "    nodes_str = row['nodes']\n",
    "    hs_value = row['Hs_Skills_Diversity']\n",
    "    \n",
    "    if isinstance(nodes_str, str):\n",
    "        city_names = [name.strip() for name in nodes_str.split(',')]\n",
    "        city_set = frozenset(city_names)\n",
    "        hs_lookup_complete[city_set] = hs_value\n",
    "\n",
    "print(f\"âœ… Successfully built hs_lookup_complete with {len(hs_lookup_complete)} keys\")\n",
    "\n",
    "# --- Correction: Calculate hs_complete before using it ---\n",
    "\n",
    "# 1. Define a function to get all leaf nodes from the tree (if not already defined)\n",
    "def get_all_leaves_from_tree(tree_graph):\n",
    "    \"\"\"Get all leaf nodes (cities) from the tree graph\"\"\"\n",
    "    return frozenset(node for node in tree_graph.nodes() if tree_graph.out_degree(node) == 0)\n",
    "\n",
    "# 2. Get all city nodes in the complete network\n",
    "all_cities = get_all_leaves_from_tree(tree_graph)\n",
    "print(f\"\\nComplete network contains {len(all_cities)} cities\")\n",
    "\n",
    "# 3. Calculate skill diversity (H_S) for the complete network\n",
    "print(\"Calculating skill diversity for the complete network...\")\n",
    "#   a. Extract skill data for all cities from df_skills_merged\n",
    "skills_complete_network = df_skills_merged.reindex(list(all_cities))\n",
    "#   b. Sum skill counts for all cities\n",
    "total_skills_complete = skills_complete_network[['skill_level_1', 'skill_level_2', 'skill_level_3', 'skill_level_4']].sum()\n",
    "#   c. Call function to calculate H_S value and assign to hs_complete\n",
    "hs_complete = calculate_skills_diversity_final(total_skills_complete)\n",
    "print(f\"âœ… Skill diversity for the complete network calculated: H_S = {hs_complete:.4f}\")\n",
    "\n",
    "# --- Now it is safe to use hs_complete ---\n",
    "# 4. Add complete network diversity to the dictionary\n",
    "hs_lookup_complete[all_cities] = hs_complete\n",
    "print(f\"âœ… Added skill diversity for the complete network to hs_lookup_complete\")\n",
    "\n",
    "# --- Validation code remains unchanged ---\n",
    "print(f\"\\nFinal hs_lookup_complete contains {len(hs_lookup_complete)} keys\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSkill diversity sample data:\")\n",
    "# (To avoid errors if all_cities is not found, filter it out before sampling)\n",
    "sample_keys = [k for k in hs_lookup_complete if len(k) != len(all_cities)]\n",
    "for i, key in enumerate(sample_keys[:3]):\n",
    "    value = hs_lookup_complete[key]\n",
    "    print(f\"  Key {i+1}: {list(key)[:3]}{'...' if len(key) > 3 else ''} (size: {len(key)})\")\n",
    "    print(f\"    H_S value: {value:.4f}\")\n",
    "\n",
    "# Check matching with tree graph\n",
    "print(\"\\nChecking matches with tree graph:\")\n",
    "matches = 0\n",
    "# Assume get_leaves_under_node_debug function is defined\n",
    "internal_nodes = [n for n in tree_graph.nodes() if tree_graph.out_degree(n) > 0]\n",
    "for node in internal_nodes:\n",
    "    leaves = frozenset(get_leaves_under_node_debug(node, tree_graph))\n",
    "    if leaves in hs_lookup_complete:\n",
    "        matches += 1\n",
    "        \n",
    "print(f\"Number of matching nodes: {matches} / {len(internal_nodes)}\")\n",
    "\n",
    "# Check largest cluster (should be the complete network)\n",
    "max_cluster_size = max(len(k) for k in hs_lookup_complete.keys())\n",
    "# Use all_cities as the key for lookup, which is more robust\n",
    "print(f\"\\nLargest cluster size: {max_cluster_size}\")\n",
    "print(f\"Skill diversity of complete network: {hs_lookup_complete[all_cities]:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… hs_lookup_complete is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4abae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# Analyze the distribution of hs_lookup_complete values (English Version)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"--- Analyzing the distribution of hs_lookup_complete values ---\")\n",
    "\n",
    "# 1. Extract H_S values into a Pandas Series for analysis\n",
    "try:\n",
    "    hs_values = pd.Series(list(hs_lookup_complete.values()), name=\"Hs_Skills_Diversity\")\n",
    "    print(f\"âœ… Successfully extracted {len(hs_values)} H_S values.\")\n",
    "except NameError:\n",
    "    print(\"âŒ Error: The variable 'hs_lookup_complete' is not defined. Please run the previous cells to create it.\")\n",
    "    hs_values = None\n",
    "\n",
    "if hs_values is not None:\n",
    "    # 2. Print descriptive statistics\n",
    "    print(\"\\n--- Descriptive Statistics for H_S Values ---\")\n",
    "    print(hs_values.describe().to_string())\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "    # 3. Visualize the distribution using Matplotlib and Seaborn\n",
    "    print(\"\\n--- Generating distribution plots... ---\")\n",
    "\n",
    "    # Set the plot theme\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Create a figure with two subplots (for histogram and boxplot)\n",
    "    fig, (ax_hist, ax_box) = plt.subplots(\n",
    "        2, 1,\n",
    "        figsize=(12, 8),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": (.8, .2)}\n",
    "    )\n",
    "\n",
    "    # Plot the histogram and Kernel Density Estimate (KDE) on the top subplot\n",
    "    sns.histplot(hs_values, ax=ax_hist, kde=True, bins=40)\n",
    "    ax_hist.set_title('Distribution of Skill Diversity (H_S) Values Across All Clusters', fontsize=16)\n",
    "    ax_hist.set_ylabel('Frequency (Number of Clusters)')\n",
    "    ax_hist.set_xlabel('') # Remove x-axis label for the top plot\n",
    "\n",
    "    # Mark the mean and median on the plot\n",
    "    mean_val = hs_values.mean()\n",
    "    median_val = hs_values.median()\n",
    "    ax_hist.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')\n",
    "    ax_hist.axvline(median_val, color='green', linestyle='-', linewidth=2, label=f'Median: {median_val:.3f}')\n",
    "    ax_hist.legend()\n",
    "\n",
    "\n",
    "    # Plot the boxplot on the bottom subplot\n",
    "    sns.boxplot(x=hs_values, ax=ax_box)\n",
    "    ax_box.set_xlabel('H_S (Skill Diversity)', fontsize=12)\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ… Distribution plots generated successfully.\")\n",
    "    print(\"\\n--- How to Interpret the Plots ---\")\n",
    "    print(\"1. **Descriptive Statistics**: The text output shows key metrics. 'mean' is the average, '50%' is the median.\")\n",
    "    print(\"2. **Histogram (Blue Bars)**: Shows the number of clusters that fall within a specific range of H_S values. Taller bars indicate more clusters in that range.\")\n",
    "    print(\"3. **Density Curve (Blue Line)**: A smoothed version of the histogram, showing the overall shape of the distribution.\")\n",
    "    print(\"4. **Box Plot (Bottom)**: \")\n",
    "    print(\"   - **The Box**: Represents the middle 50% of your data (from the 25th to the 75th percentile).\")\n",
    "    print(\"   - **Green Line inside Box**: Represents the median (the 50th percentile).\")\n",
    "    print(\"   - **The 'Whiskers'**: Show the main range of the data.\")\n",
    "    print(\"   - **Diamonds outside Whiskers**: Represent potential outliers (clusters with unusually high or low H_S values).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c428745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Import K-Means clustering algorithm\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "except ImportError:\n",
    "    print(\"=\"*60)\n",
    "    print(\"âŒ Error: This script requires the 'scikit-learn' library.\")\n",
    "    print(\"Please run: pip install scikit-learn in your terminal or notebook.\")\n",
    "    print(\"=\"*60)\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Final Solution: Advanced layout + H_S K-Means coloring dendrogram plot\n",
    "# ============================================================================\n",
    "\n",
    "def plot_dendrogram_with_hs_coloring(tree_graph, builder, hs_lookup_complete):\n",
    "    \"\"\"\n",
    "    Plot a skill diversity dendrogram with advanced layout (no crossing)\n",
    "    and H_S K-Means coloring scheme.\n",
    "\n",
    "    Args:\n",
    "        tree_graph (nx.DiGraph): Directed graph representation of the tree from percolation analysis.\n",
    "        builder (CorrectDendrogramBuilder): Builder object containing all cluster details.\n",
    "        hs_lookup_complete (dict): Dictionary with {frozenset(cities): H_S_value}.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting final plotting script (advanced layout + H_S K-Means coloring) ---\")\n",
    "    \n",
    "    # --- Step 1: Compute advanced layout (adopted from previous final version) ---\n",
    "    print(\"\\n[Layout] Computing advanced layout (ordinal heights, weighted X-axis, persistence nodes)...\")\n",
    "    \n",
    "    # a. Create \"ordinal height\" mapping\n",
    "    all_unique_heights = sorted(list(set(c.height for c in builder.all_clusters.values())))\n",
    "    height_to_y_level_map = {height: i for i, height in enumerate(all_unique_heights)}\n",
    "\n",
    "    # b. Compute node weights (for X-axis layout)\n",
    "    memo = {}\n",
    "    def count_cities_under_weighted(node):\n",
    "        if node in memo: return memo[node]\n",
    "        if tree_graph.out_degree(node) == 0: memo[node] = 1.0; return 1.0\n",
    "        count = sum(count_cities_under_weighted(child) for child in tree_graph.successors(node))\n",
    "        memo[node] = count\n",
    "        return count\n",
    "    root_node = [n for n, d in tree_graph.in_degree() if d == 0][0]\n",
    "    count_cities_under_weighted(root_node)\n",
    "\n",
    "    # c. Compute base layout for all \"real\" nodes\n",
    "    real_pos = {}\n",
    "    nodes_to_process = sorted(tree_graph.nodes(), key=lambda n: builder.all_clusters[n].height, reverse=True)\n",
    "    leaf_spacing_factor = 1.2\n",
    "    real_pos[root_node] = (0.0, height_to_y_level_map[builder.all_clusters[root_node].height])\n",
    "    for parent_node in nodes_to_process:\n",
    "        if parent_node not in real_pos: continue\n",
    "        children = sorted(list(tree_graph.successors(parent_node)), key=lambda n: count_cities_under_weighted(n))\n",
    "        if not children: continue\n",
    "        parent_x, _ = real_pos[parent_node]\n",
    "        total_child_width = sum(count_cities_under_weighted(c) for c in children) * leaf_spacing_factor\n",
    "        current_x = parent_x - total_child_width / 2.0\n",
    "        for child in children:\n",
    "            child_width = count_cities_under_weighted(child) * leaf_spacing_factor\n",
    "            child_x = current_x + child_width / 2.0\n",
    "            child_y = height_to_y_level_map[builder.all_clusters[child].height]\n",
    "            real_pos[child] = (child_x, child_y)\n",
    "            current_x += child_width\n",
    "            \n",
    "    # d. Create \"enhanced\" nodes and edges for visualization (persistence nodes)\n",
    "    viz_pos, viz_edges = dict(real_pos), []\n",
    "    for parent, child in tree_graph.edges():\n",
    "        parent_y, child_y = real_pos[parent][1], real_pos[child][1]\n",
    "        if parent_y > child_y + 1:\n",
    "            last_node_id = child\n",
    "            for level in range(int(child_y) + 1, int(parent_y)):\n",
    "                persistence_node_id = f\"p_{child}_{level}\"\n",
    "                viz_pos[persistence_node_id] = (real_pos[child][0], level)\n",
    "                viz_edges.append((last_node_id, persistence_node_id))\n",
    "                last_node_id = persistence_node_id\n",
    "            viz_edges.append((last_node_id, parent))\n",
    "        else:\n",
    "            viz_edges.append((child, parent))\n",
    "    print(\"âœ… Advanced layout computation complete.\")\n",
    "\n",
    "    # --- Step 2: K-Means color grouping computation (adopted from your new script) ---\n",
    "    print(\"\\n[Coloring] Performing K-Means color grouping computation...\")\n",
    "    \n",
    "    # a. Associate tree nodes with H_S values\n",
    "    node_hs_map = {\n",
    "        n: hs_lookup_complete.get(frozenset(builder.all_clusters[n].cities))\n",
    "        for n in tree_graph.nodes()\n",
    "    }\n",
    "    \n",
    "    # b. Prepare input data for K-Means\n",
    "    valid_nodes = {n: hs for n, hs in node_hs_map.items() if hs is not None and np.isfinite(hs)}\n",
    "    if len(valid_nodes) < 5:\n",
    "        print(f\"âŒ Warning: Number of valid H_S values ({len(valid_nodes)}) is insufficient for 5 clusters. Plotting aborted.\")\n",
    "        return\n",
    "\n",
    "    hs_values_array = np.array(list(valid_nodes.values())).reshape(-1, 1)\n",
    "    \n",
    "    # c. Run K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto').fit(hs_values_array)\n",
    "    node_to_cluster_label = {node: kmeans.predict(np.array([[hs]]))[0] for node, hs in valid_nodes.items()}\n",
    "    \n",
    "    # d. Sort cluster results by H_S value and create color mapping\n",
    "    cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "    sorted_center_indices = np.argsort(cluster_centers)\n",
    "    rank_map = {original_label: rank for rank, original_label in enumerate(sorted_center_indices)}\n",
    "    colors = plt.cm.coolwarm_r(np.linspace(0, 1, 5)) # Blue (low Hs) to Red (high Hs)\n",
    "    print(\"âœ… K-Means coloring scheme computation complete.\")\n",
    "\n",
    "    # --- Step 3: Final plotting ---\n",
    "    print(\"\\n[Plotting] Starting final plot rendering...\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(40, 26))\n",
    "    \n",
    "    # a. Draw edges\n",
    "    for u, v in viz_edges:\n",
    "        if u in viz_pos and v in viz_pos:\n",
    "            ax.plot([viz_pos[u][0], viz_pos[v][0]], [viz_pos[u][1], viz_pos[v][1]], \n",
    "                   color='#cccccc', zorder=1, linewidth=0.8)\n",
    "    \n",
    "    # b. Draw nodes\n",
    "    unweighted_city_counts = {n: len(builder.all_clusters[n].cities) for n in tree_graph.nodes()}\n",
    "    for node, p in viz_pos.items():\n",
    "        is_persistence_node = str(node).startswith('p_')\n",
    "        original_node_id = str(node).split('_')[1] if is_persistence_node else node\n",
    "        \n",
    "        size = 8 + unweighted_city_counts.get(original_node_id, 1)**0.7 * 50\n",
    "        cities_count = unweighted_city_counts.get(original_node_id, 0)\n",
    "        \n",
    "        # Coloring logic\n",
    "        if cities_count <= 1:\n",
    "            color = 'gray'\n",
    "        elif original_node_id not in valid_nodes:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            cluster_label = node_to_cluster_label[original_node_id]\n",
    "            color_rank = rank_map[cluster_label]\n",
    "            color = colors[color_rank]\n",
    "        \n",
    "        ax.scatter(p[0], p[1], s=size, facecolor=color, edgecolor='black', zorder=2, alpha=0.9, linewidth=1.2)\n",
    "\n",
    "    # c. Add custom H_S legend (adopted from your new script)\n",
    "    ax_legend = fig.add_axes([0.8, 0.75, 0.015, 0.15])\n",
    "    custom_cmap = mcolors.ListedColormap(colors)\n",
    "    legend_data = np.arange(len(colors)).reshape(-1, 1)\n",
    "    ax_legend.imshow(legend_data, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "    ax_legend.set_title('$H_S$', fontsize=20, pad=10)\n",
    "    ax_legend.text(1.8, 0.95, '+', transform=ax_legend.transAxes, ha='left', va='center', fontsize=24) # High Hs\n",
    "    ax_legend.text(1.8, 0.05, 'âˆ’', transform=ax_legend.transAxes, ha='left', va='center', fontsize=28) # Low Hs\n",
    "    ax_legend.set_xticks([]); ax_legend.set_yticks([])\n",
    "    ax_legend.spines['top'].set_visible(False); ax_legend.spines['right'].set_visible(False)\n",
    "    ax_legend.spines['bottom'].set_visible(False); ax_legend.spines['left'].set_visible(False)\n",
    "    \n",
    "    # d. Final style\n",
    "    ax.set_title('Commuting Skill Diversity Dendrogram', fontsize=24, family='serif', pad=20)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "# --- Main function execution ---\n",
    "# Make sure the following variables are prepared in your environment:\n",
    "# 1. tree_graph: nx.DiGraph object generated by previous version\n",
    "# 2. percolation_tree_builder: builder object generated by previous version\n",
    "# 3. hs_lookup_complete: your dictionary containing H_S values\n",
    "if 'tree_graph' in globals() and 'percolation_tree_builder' in globals() and 'hs_lookup_complete' in globals():\n",
    "    plot_dendrogram_with_hs_coloring(tree_graph, percolation_tree_builder, hs_lookup_complete)\n",
    "else:\n",
    "    print(\"\\nâŒ Error: Please prepare 'tree_graph', 'percolation_tree_builder', and 'hs_lookup_complete' before calling this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Import K-Means clustering algorithm\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "except ImportError:\n",
    "    print(\"=\"*60)\n",
    "    print(\"âŒ Error: This script requires the 'scikit-learn' library.\")\n",
    "    print(\"Please run: pip install scikit-learn\")\n",
    "    print(\"=\"*60)\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Final Solution: Advanced layout + H_S (Blue-Yellow-Red) K-Means coloring\n",
    "# ============================================================================\n",
    "\n",
    "def plot_dendrogram_with_hs_coloring(tree_graph, builder, hs_lookup_complete):\n",
    "    \"\"\"\n",
    "    Plot a skill diversity dendrogram with advanced layout (no crossing)\n",
    "    and H_S (Blue-Yellow-Red) K-Means coloring scheme.\n",
    "\n",
    "    Args:\n",
    "        tree_graph (nx.DiGraph): Directed graph representation of the tree from percolation analysis.\n",
    "        builder (CorrectDendrogramBuilder): Builder object containing all clustering details.\n",
    "        hs_lookup_complete (dict): Dictionary of {frozenset(cities): H_S_value}.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting final plotting script (advanced layout + H_S blue-yellow-red coloring) ---\")\n",
    "    \n",
    "    # [Step 1] Data check\n",
    "    valid_hs_values = [v for v in hs_lookup_complete.values() if v is not None and np.isfinite(v)]\n",
    "    if len(valid_hs_values) < 5:\n",
    "        print(f\"âŒ Warning: Number of valid H_S values ({len(valid_hs_values)}) is insufficient for 5 clusters. Plotting aborted.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Compute advanced layout (adopted from previous final version) ---\n",
    "    print(\"\\n[Layout] Computing advanced layout...\")\n",
    "    \n",
    "    # a. Create \"ordinal height\" mapping\n",
    "    all_unique_heights = sorted(list(set(c.height for c in builder.all_clusters.values())))\n",
    "    height_to_y_level_map = {height: i for i, height in enumerate(all_unique_heights)}\n",
    "\n",
    "    # b. Compute node weights (for X-axis layout)\n",
    "    memo = {}\n",
    "    def count_cities_under_weighted(node):\n",
    "        if node in memo: return memo[node]\n",
    "        if tree_graph.out_degree(node) == 0: memo[node] = 1.0; return 1.0\n",
    "        count = sum(count_cities_under_weighted(child) for child in tree_graph.successors(node))\n",
    "        memo[node] = count\n",
    "        return count\n",
    "    root_node = [n for n, d in tree_graph.in_degree() if d == 0][0]\n",
    "    count_cities_under_weighted(root_node)\n",
    "\n",
    "    # c. Compute base layout for all \"real\" nodes\n",
    "    real_pos = {}\n",
    "    nodes_to_process = sorted(tree_graph.nodes(), key=lambda n: builder.all_clusters[n].height, reverse=True)\n",
    "    leaf_spacing_factor = 1.2\n",
    "    real_pos[root_node] = (0.0, height_to_y_level_map[builder.all_clusters[root_node].height])\n",
    "    for parent_node in nodes_to_process:\n",
    "        if parent_node not in real_pos: continue\n",
    "        children = sorted(list(tree_graph.successors(parent_node)), key=lambda n: count_cities_under_weighted(n))\n",
    "        if not children: continue\n",
    "        parent_x, _ = real_pos[parent_node]\n",
    "        total_child_width = sum(count_cities_under_weighted(c) for c in children) * leaf_spacing_factor\n",
    "        current_x = parent_x - total_child_width / 2.0\n",
    "        for child in children:\n",
    "            child_width = count_cities_under_weighted(child) * leaf_spacing_factor\n",
    "            child_x = current_x + child_width / 2.0\n",
    "            child_y = height_to_y_level_map[builder.all_clusters[child].height]\n",
    "            real_pos[child] = (child_x, child_y)\n",
    "            current_x += child_width\n",
    "            \n",
    "    # d. Create \"persistent\" nodes and edges for visualization\n",
    "    viz_pos, viz_edges = dict(real_pos), []\n",
    "    for parent, child in tree_graph.edges():\n",
    "        parent_y, child_y = real_pos[parent][1], real_pos[child][1]\n",
    "        if parent_y > child_y + 1:\n",
    "            last_node_id = child\n",
    "            for level in range(int(child_y) + 1, int(parent_y)):\n",
    "                persistence_node_id = f\"p_{child}_{level}\"\n",
    "                viz_pos[persistence_node_id] = (real_pos[child][0], level)\n",
    "                viz_edges.append((last_node_id, persistence_node_id))\n",
    "                last_node_id = persistence_node_id\n",
    "            viz_edges.append((last_node_id, parent))\n",
    "        else:\n",
    "            viz_edges.append((child, parent))\n",
    "    print(\"âœ… Advanced layout computed.\")\n",
    "\n",
    "    # --- Step 3: K-Means color grouping (adopted from your V4.1 script) ---\n",
    "    print(\"\\n[Coloring] Performing K-Means color grouping...\")\n",
    "    \n",
    "    # a. Associate tree nodes with H_S values\n",
    "    node_hs_map = {\n",
    "        n: hs_lookup_complete.get(frozenset(builder.all_clusters[n].cities))\n",
    "        for n in tree_graph.nodes()\n",
    "    }\n",
    "    \n",
    "    # b. Prepare input data for K-Means\n",
    "    valid_nodes = {n: hs for n, hs in node_hs_map.items() if hs is not None and np.isfinite(hs)}\n",
    "    hs_values_array = np.array(list(valid_nodes.values())).reshape(-1, 1)\n",
    "    \n",
    "    # c. Run K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto').fit(hs_values_array)\n",
    "    node_to_cluster_label = {node: kmeans.predict(np.array([[hs]]))[0] for node, hs in valid_nodes.items()}\n",
    "    \n",
    "    # d. Sort clusters by H_S value and create color mapping\n",
    "    cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "    sorted_center_indices = np.argsort(cluster_centers)\n",
    "    rank_map = {original_label: rank for rank, original_label in enumerate(sorted_center_indices)}\n",
    "    \n",
    "    # [Core change] Use \"Blue -> Yellow -> Red\" color palette\n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, 5))\n",
    "    print(\"âœ… K-Means coloring computed.\")\n",
    "\n",
    "    # --- Step 4: Final plotting ---\n",
    "    print(\"\\n[Plotting] Drawing final figure...\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(40, 26))\n",
    "    \n",
    "    # a. Draw edges\n",
    "    for u, v in viz_edges:\n",
    "        if u in viz_pos and v in viz_pos:\n",
    "            ax.plot([viz_pos[u][0], viz_pos[v][0]], [viz_pos[u][1], viz_pos[v][1]], \n",
    "                   color='#cccccc', zorder=1, linewidth=0.8)\n",
    "    \n",
    "    # b. Draw nodes\n",
    "    unweighted_city_counts = {n: len(builder.all_clusters[n].cities) for n in tree_graph.nodes()}\n",
    "    for node, p in viz_pos.items():\n",
    "        is_persistence_node = str(node).startswith('p_')\n",
    "        original_node_id = str(node).split('_')[1] if is_persistence_node else node\n",
    "        \n",
    "        size = 8 + unweighted_city_counts.get(original_node_id, 1)**0.7 * 50\n",
    "        cities_count = unweighted_city_counts.get(original_node_id, 0)\n",
    "        \n",
    "        edge_color = 'black'\n",
    "        \n",
    "        # Coloring logic\n",
    "        if cities_count <= 1:\n",
    "            color = 'lightgray' # Use light gray for leaves\n",
    "            edge_color = 'darkgray'\n",
    "        elif original_node_id not in valid_nodes:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            cluster_label = node_to_cluster_label[original_node_id]\n",
    "            color_rank = rank_map[cluster_label]\n",
    "            color = colors[color_rank]\n",
    "        \n",
    "        ax.scatter(p[0], p[1], s=size, facecolor=color, edgecolor=edge_color, zorder=2, alpha=0.9, linewidth=1.2)\n",
    "\n",
    "    # c. Add custom H_S legend (adopted from your V4.1 script)\n",
    "    ax_legend = fig.add_axes([0.8, 0.75, 0.015, 0.15])\n",
    "    custom_cmap = mcolors.ListedColormap(colors)\n",
    "    legend_data = np.arange(len(colors)).reshape(-1, 1)\n",
    "    ax_legend.imshow(legend_data, cmap=custom_cmap, aspect='auto', origin='lower')\n",
    "    ax_legend.set_title('$H_S$', fontsize=20, pad=10)\n",
    "    ax_legend.text(1.8, 0.95, '+', transform=ax_legend.transAxes, ha='left', va='center', fontsize=24) # High Hs\n",
    "    ax_legend.text(1.8, 0.05, 'âˆ’', transform=ax_legend.transAxes, ha='left', va='center', fontsize=28) # Low Hs\n",
    "    ax_legend.set_xticks([]); ax_legend.set_yticks([])\n",
    "    ax_legend.spines['top'].set_visible(False); ax_legend.spines['right'].set_visible(False)\n",
    "    ax_legend.spines['bottom'].set_visible(False); ax_legend.spines['left'].set_visible(False)\n",
    "    \n",
    "    # d. Final style\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n--- Script finished ---\")\n",
    "\n",
    "\n",
    "# --- Run main function ---\n",
    "if 'tree_graph' in globals() and 'percolation_tree_builder' in globals() and 'hs_lookup_complete' in globals():\n",
    "    plot_dendrogram_with_hs_coloring(tree_graph, percolation_tree_builder, hs_lookup_complete)\n",
    "else:\n",
    "    print(\"\\nâŒ Error: Please prepare 'tree_graph', 'percolation_tree_builder', and 'hs_lookup_complete' before calling this function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ba417",
   "metadata": {},
   "source": [
    "##### Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have already run previous code and obtained G and df_with_weights\n",
    "\n",
    "print(\"--- Starting calculation of network centrality metrics ---\")\n",
    "\n",
    "# --- Step 1: Calculate weighted in-degree ---\n",
    "# G.in_degree() returns a special object with (node, degree) pairs\n",
    "# Use weight='flow' to specify the edge weight, which corresponds to the 'flow' attribute you added to edges\n",
    "# If you want to use 'dependency_weight' as the weight, just change weight='flow' to weight='dependency_weight'\n",
    "print(\"   Step 1/4: Calculating weighted in-degree...\")\n",
    "in_degree_weighted = dict(G.in_degree(weight='flow'))\n",
    "\n",
    "# --- Step 2: Convert results to Pandas DataFrame ---\n",
    "print(\"   Step 2/4: Converting results to DataFrame...\")\n",
    "df_metrics = pd.DataFrame.from_dict(\n",
    "    in_degree_weighted, \n",
    "    orient='index', \n",
    "    columns=['in_degree_weighted']\n",
    ")\n",
    "df_metrics.index.name = 'TTWA_Code'\n",
    "df_metrics.reset_index(inplace=True)\n",
    "\n",
    "# --- Step 3: Add TTWA names for readability ---\n",
    "# Extract TTWA code and name mapping from your original df_with_weights data\n",
    "# Here, it assumes the destination columns are 'DEST_TTWA' and 'DEST_TTWA_NAME'; modify as needed for your data\n",
    "print(\"   Step 3/4: Adding TTWA names...\")\n",
    "try:\n",
    "    ttwa_names = df_with_weights[['DEST_TTWA', 'DEST_TTWA_NAME']].drop_duplicates().rename(\n",
    "        columns={'DEST_TTWA': 'TTWA_Code', 'DEST_TTWA_NAME': 'TTWA_Name'}\n",
    "    )\n",
    "    # Merge names into metrics DataFrame\n",
    "    final_df = pd.merge(ttwa_names, df_metrics, on='TTWA_Code', how='right')\n",
    "\n",
    "except KeyError:\n",
    "    print(\"   Warning: 'DEST_TTWA_NAME' column not found, output will not include TTWA names.\")\n",
    "    final_df = df_metrics\n",
    "\n",
    "# --- Step 4: Save to CSV file ---\n",
    "OUTPUT_FILE = 'inter_city_in_degree_metrics_filter.csv'\n",
    "print(f\"   Step 4/4: Saving results to '{OUTPUT_FILE}'...\")\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nâœ… Success! City centrality metrics have been calculated.\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(\"\\nPreview of file contents:\")\n",
    "print(final_df.sort_values('in_degree_weighted', ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_and_analyze_filtered.py (FINAL CORRECTED VERSION)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This script is the final step of the analysis.\n",
    "    Core logic: Merge data from groups A, B, and C. Before merging, fix column names in file B, then perform analysis.\n",
    "    \"\"\"\n",
    "    print(\"--- Final Analysis: Merging Data and Hypothesis Testing ---\")\n",
    "\n",
    "    # --- 1. File Path Configuration ---\n",
    "    FILE_A_INTRA_METRICS = 'ttwa_mobility_indicators_daily_aggregated_Feb.csv'\n",
    "    FILE_B_INTER_METRICS = 'inter_city_in_degree_metrics_filter.csv'\n",
    "    FILE_C_CONTROL_VARS = os.path.join('variables', 'jobdensity2023.xlsx')\n",
    "    OUTPUT_MERGED_FILE = 'master_analysis_data_final_filter.csv'\n",
    "\n",
    "    # --- 2. Read all data files ---\n",
    "    print(\"\\n Step 1/4: Reading all data files...\")\n",
    "    try:\n",
    "        df_a = pd.read_csv(FILE_A_INTRA_METRICS)\n",
    "        df_b = pd.read_csv(FILE_B_INTER_METRICS)\n",
    "        df_c = pd.read_excel(FILE_C_CONTROL_VARS)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error: File not found {e.filename}. Please check the file path and name.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Data Cleaning and Preparation ---\n",
    "    print(\" Step 2/4: Cleaning and preparing data...\")\n",
    "    \n",
    "    # Clean column names in file C\n",
    "    df_c.rename(columns={\n",
    "        'travel to work area 2011-based': 'TTWA_Name',\n",
    "        'Jobs density': 'jobs_density'\n",
    "    }, inplace=True)\n",
    "    df_c = df_c[['TTWA_Name', 'jobs_density']]\n",
    "\n",
    "    # â–¼â–¼â–¼ Key Fix: Correct column name error in df_b as discovered â–¼â–¼â–¼\n",
    "    if 'TTWA_Code' in df_b.columns and 'TTWA_Name' not in df_b.columns:\n",
    "        print(\"   Detected 'TTWA_Code' column in file B is actually names, correcting column name...\")\n",
    "        df_b.rename(columns={'TTWA_Code': 'TTWA_Name'}, inplace=True)\n",
    "    # â–²â–²â–² End of Fix â–²â–²â–²\n",
    "    \n",
    "    # --- 4. Merge and Filter ---\n",
    "    print(\" Step 3/4: Merging data and removing missing values...\")\n",
    "\n",
    "    # Remove TTWA with missing data in file B if needed\n",
    "    # Note: The in_degree_weighted column may not exist, so filtering is commented out for now\n",
    "    # df_b_filtered = df_b[df_b['in_degree_weighted'] > 0].copy()\n",
    "    # num_removed = len(df_b) - len(df_b_filtered)\n",
    "    # print(f\"   Removed {num_removed} TTWA with in_degree 0 from file B, {len(df_b_filtered)} left for analysis.\")\n",
    "    df_b_filtered = df_b # No filtering for now to ensure successful merge\n",
    "\n",
    "    # All merges are based on 'TTWA_Name'\n",
    "    df_merged_ab = pd.merge(df_a, df_b_filtered, on='TTWA_Name', how='inner')\n",
    "    print(f\"   After merging A and B, {len(df_merged_ab)} TTWA obtained.\")\n",
    "    \n",
    "    master_df = pd.merge(df_merged_ab, df_c, on='TTWA_Name', how='inner')\n",
    "    print(f\"   After merging with control variables, final TTWA count for analysis: {len(master_df)}.\")\n",
    "\n",
    "    # --- 5. Save processed master data table ---\n",
    "    master_df.to_csv(OUTPUT_MERGED_FILE, index=False)\n",
    "    print(f\"\\n Step 4/4: Final processed data saved to '{OUTPUT_MERGED_FILE}'.\")\n",
    "\n",
    "    # --- 6. Correlation Analysis ---\n",
    "    print(\"\\n--- Correlation Analysis on Final Dataset ---\")\n",
    "    analysis_cols = [\n",
    "        'Centripetality_Gamma_mean', \n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'in_degree_weighted',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    existing_analysis_cols = [col for col in analysis_cols if col in master_df.columns]\n",
    "    \n",
    "    if len(master_df) > 1 and len(existing_analysis_cols) > 1:\n",
    "        correlation_matrix = master_df[existing_analysis_cols].corr()\n",
    "        print(\"Correlation matrix:\")\n",
    "        print(correlation_matrix)\n",
    "        print(\"------------------------\\n\")\n",
    "\n",
    "        # --- 7. Visualize Key Relationships ---\n",
    "        print(\"Generating final scatter plot...\")\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        # Only plot if in_degree_weighted column exists\n",
    "        if 'in_degree_weighted' in master_df.columns:\n",
    "            sns.regplot(\n",
    "                data=master_df,\n",
    "                x='in_degree_weighted',\n",
    "                y='Centripetality_Gamma_mean',\n",
    "                scatter_kws={'alpha':0.6}\n",
    "            )\n",
    "            plt.title('Relationship between Inter-City Attraction and Intra-City Structure', fontsize=16)\n",
    "            plt.xlabel('Inter-City Attractiveness (Weighted In-Degree)', fontsize=12)\n",
    "            plt.ylabel('Intra-City Centripetality (Gamma Mean)', fontsize=12)\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"âš ï¸ Warning: 'in_degree_weighted' column not found, cannot generate scatter plot.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âš ï¸ Warning: Final merged data insufficient for correlation analysis or plotting. Please check input files.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422f521",
   "metadata": {},
   "source": [
    "Below are the filtered data entries where the values are not zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce287dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_analysis_with_full_cleaning.py\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This script is the final step of analysis, integrating two-step data cleaning logic:\n",
    "    1. Reliability: Filter by flow threshold.\n",
    "    2. Validity: Remove NaN values.\n",
    "    \"\"\"\n",
    "    print(\"--- Final Analysis: Merging, Cleaning, and Hypothesis Testing ---\")\n",
    "\n",
    "    # --- 1. Parameter Configuration ---\n",
    "    FILE_A_INTRA_METRICS = 'ttwa_mobility_indicators_daily_aggregated_Feb.csv'\n",
    "    FILE_B_INTER_METRICS = 'inter_city_in_degree_metrics_filter.csv'\n",
    "    FILE_C_CONTROL_VARS = os.path.join('variables', 'jobdensity2023.xlsx')\n",
    "    OUTPUT_MERGED_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "\n",
    "    # Parameters for cleaning step 1\n",
    "    FLOW_COUNT_COLUMN = 'total_internal_flow' \n",
    "    MIN_FLOW_THRESHOLD = 500\n",
    "    \n",
    "    # Parameters for cleaning step 2 (key columns for analysis)\n",
    "    ANALYSIS_COLS = [\n",
    "        'Centripetality_Gamma_mean', \n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'in_degree_weighted',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "\n",
    "    # --- 2. Read Data ---\n",
    "    print(\"\\n Step 1/6: Reading all data files...\")\n",
    "    try:\n",
    "        df_a = pd.read_csv(FILE_A_INTRA_METRICS)\n",
    "        df_b = pd.read_csv(FILE_B_INTER_METRICS)\n",
    "        df_c = pd.read_excel(FILE_C_CONTROL_VARS)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found {e.filename}.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Data Preparation ---\n",
    "    print(\" Step 2/6: Preparing data...\")\n",
    "    df_c.rename(columns={'travel to work area 2011-based': 'TTWA_Name', 'Jobs density': 'jobs_density'}, inplace=True)\n",
    "    df_c = df_c[['TTWA_Name', 'jobs_density']]\n",
    "    if 'TTWA_Code' in df_b.columns:\n",
    "        df_b.rename(columns={'TTWA_Code': 'TTWA_Name'}, inplace=True)\n",
    "\n",
    "    # --- 4. Cleaning Step 1: Reliability by Flow Threshold ---\n",
    "    print(\" Step 3/6: Filtering for data reliability...\")\n",
    "    df_a_cleaned = df_a.copy()\n",
    "    if FLOW_COUNT_COLUMN in df_a.columns:\n",
    "        initial_count = len(df_a_cleaned)\n",
    "        df_a_cleaned = df_a_cleaned[df_a_cleaned[FLOW_COUNT_COLUMN] >= MIN_FLOW_THRESHOLD]\n",
    "        removed_count = initial_count - len(df_a_cleaned)\n",
    "        print(f\"   Filtered by flow threshold (>= {MIN_FLOW_THRESHOLD}), removed {removed_count} TTWA with insufficient data.\")\n",
    "    else:\n",
    "        print(f\"Warning: Flow count column '{FLOW_COUNT_COLUMN}' not found, skipping this cleaning step.\")\n",
    "    \n",
    "    # --- 5. Merge Data ---\n",
    "    print(\" Step 4/6: Merging all data sources...\")\n",
    "    df_merged_ab = pd.merge(df_a_cleaned, df_b, on='TTWA_Name', how='inner')\n",
    "    master_df = pd.merge(df_merged_ab, df_c, on='TTWA_Name', how='inner')\n",
    "    print(f\"   After merging, the dataset contains {len(master_df)} TTWA.\")\n",
    "\n",
    "    # --- 6. â–¼â–¼â–¼ Cleaning Step 2: Remove NaN for Validity â–¼â–¼â–¼ ---\n",
    "    print(\" Step 5/6: Removing rows with NaN in key analysis columns...\")\n",
    "    # Check if ANALYSIS_COLS exist in DataFrame to avoid KeyError\n",
    "    existing_analysis_cols = [col for col in ANALYSIS_COLS if col in master_df.columns]\n",
    "    \n",
    "    initial_rows = len(master_df)\n",
    "    master_df.dropna(subset=existing_analysis_cols, inplace=True)\n",
    "    final_rows = len(master_df)\n",
    "    removed_rows = initial_rows - final_rows\n",
    "    if removed_rows > 0:\n",
    "        print(f\"   Removed {removed_rows} rows due to NaN in key analysis columns.\")\n",
    "    print(f\"   Final number of TTWA for analysis: {final_rows}.\")\n",
    "    # â–²â–²â–² End of cleaning logic â–²â–²â–²\n",
    "\n",
    "    # --- 7. Save and Final Analysis ---\n",
    "    print(f\"\\n Step 6/6: Saving final data and performing analysis...\")\n",
    "    master_df.to_csv(OUTPUT_MERGED_FILE, index=False)\n",
    "\n",
    "    # --- Further Analysis ---\n",
    "    if final_rows > 1 and len(existing_analysis_cols) > 1:\n",
    "        correlation_matrix = master_df[existing_analysis_cols].corr()\n",
    "        print(\"\\n--- Correlation Analysis on Final Cleaned Dataset ---\")\n",
    "        print(\"Correlation Matrix:\")\n",
    "        print(correlation_matrix)\n",
    "        print(\"------------------------\\n\")\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.regplot(data=master_df, x='in_degree_weighted', y='Centripetality_Gamma_mean', scatter_kws={'alpha':0.6})\n",
    "        plt.title('Relationship between Inter-City Attraction and Intra-City Structure', fontsize=16)\n",
    "        plt.xlabel('Inter-City Attractiveness (Weighted In-Degree)', fontsize=12)\n",
    "        plt.ylabel('Intra-City Centripetality (Gamma Mean)', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Warning: Not enough data for correlation analysis.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_diagnostics.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "\n",
    "# Ignore minor warnings during plotting\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_single_model(df, model_vars, model_name):\n",
    "    \"\"\"\n",
    "    Visualize correlation matrix and perform VIF analysis for a single model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n>>> Analyzing {model_name}...\")\n",
    "    \n",
    "    # Extract required data for the model and drop any rows with missing values in these columns\n",
    "    model_data = df[model_vars].dropna()\n",
    "    print(f\"   Number of valid observations for this model: {len(model_data)}\")\n",
    "    \n",
    "    # --- Step 1: Correlation Matrix ---\n",
    "    print(f\"\\n>>> Step 1/2: Correlation analysis for {model_name}...\")\n",
    "    \n",
    "    corr_matrix = model_data.corr()\n",
    "    print(f\"--- {model_name} Correlation Matrix ---\")\n",
    "    print(corr_matrix.round(4))\n",
    "\n",
    "    # Find highly correlated pairs among independent variables (exclude dependent variable)\n",
    "    independent_vars_only = [var for var in model_vars if var != 'log_in_degree_weighted']\n",
    "    high_corr_threshold = 0.7\n",
    "    high_corr_found = False\n",
    "    for i in range(len(independent_vars_only)):\n",
    "        for j in range(i + 1, len(independent_vars_only)):\n",
    "            var1 = independent_vars_only[i]\n",
    "            var2 = independent_vars_only[j]\n",
    "            corr_val = abs(corr_matrix.loc[var1, var2])\n",
    "            if corr_val > high_corr_threshold:\n",
    "                print(f\"âš ï¸  Warning: High correlation found between independent variables: {var1} and {var2} (r = {corr_matrix.loc[var1, var2]:.4f})\")\n",
    "                high_corr_found = True\n",
    "    \n",
    "    if not high_corr_found:\n",
    "        print(f\"âœ… No high correlation found among independent variables (all |r| <= {high_corr_threshold})\")\n",
    "\n",
    "    # --- Step 2: VIF Test (only for independent variables) ---\n",
    "    print(f\"\\n>>> Step 2/2: Calculating VIF values for {model_name} independent variables...\")\n",
    "    \n",
    "    vif_data = model_data[independent_vars_only]\n",
    "    \n",
    "    if len(independent_vars_only) < 2:\n",
    "        print(f\"   Info: Less than 2 independent variables in {model_name}, VIF calculation not needed.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate VIF\n",
    "    vif_result = pd.DataFrame()\n",
    "    vif_result[\"Variable\"] = vif_data.columns\n",
    "    vif_result[\"VIF\"] = [variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])]\n",
    "    \n",
    "    print(f\"\\n--- {model_name} VIF Results ---\")\n",
    "    print(vif_result.to_string(index=False))\n",
    "    \n",
    "    # Diagnostic conclusion based on VIF values\n",
    "    if (vif_result['VIF'] >= 5).any():\n",
    "        print(f\"ðŸŸ¡ Note: Some variables in {model_name} have VIF >= 5. Interpret regression coefficients with caution.\")\n",
    "    else:\n",
    "        print(\"âœ… No severe multicollinearity detected. Regression analysis can proceed safely.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function: reads final data and performs diagnostics for two models.\n",
    "    \"\"\"\n",
    "    # --- 1. Read the final data file generated by previous script ---\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "    \n",
    "    print(f\"\\n>>> Reading final dataset '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(f\"   Data read successfully, containing {len(df)} final observations.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{INPUT_DATA_FILE}' not found. Please run the data cleaning script first.\")\n",
    "        return\n",
    "\n",
    "    # Create log-transformed dependent variable\n",
    "    if 'in_degree_weighted' in df.columns:\n",
    "        df['log_in_degree_weighted'] = np.log1p(df['in_degree_weighted'])\n",
    "    else:\n",
    "        print(\"âŒ Error: 'in_degree_weighted' column not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Define variable sets for two models ---\n",
    "    # Model 1: Uses Gamma (Centripetality)\n",
    "    model1_vars = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Centripetality_Gamma_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Model 2: Uses Lambda (Anisotropy)\n",
    "    model2_vars = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Check if all variables exist\n",
    "    all_vars = list(set(model1_vars + model2_vars))\n",
    "    missing_vars = [var for var in all_vars if var not in df.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"âŒ Error: The following key variables are missing from the data: {missing_vars}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âœ… All variables found. Starting diagnostics for both models...\")\n",
    "\n",
    "    # --- 3. Analyze Model 1 ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model 1 Analysis: log(InDegree) ~ Centripetality_Gamma + jobs_density\")\n",
    "    print(\"=\"*60)\n",
    "    analyze_single_model(df, model1_vars, \"Model 1 (Gamma)\")\n",
    "    \n",
    "    # --- 4. Analyze Model 2 ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model 2 Analysis: log(InDegree) ~ Anisotropy_Lambda + jobs_density\")\n",
    "    print(\"=\"*60)\n",
    "    analyze_single_model(df, model2_vars, \"Model 2 (Lambda)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All diagnostics completed.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_beautiful_correlation_matrix():\n",
    "    \"\"\"\n",
    "    Create a visually appealing correlation matrix visualization with different color schemes.\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(f\"âœ… Data loaded successfully, containing {len(df)} observations.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Create log-transformed dependent variable\n",
    "    if 'log_in_degree_weighted' not in df.columns:\n",
    "        df['log_in_degree_weighted'] = np.log1p(df['in_degree_weighted'])\n",
    "\n",
    "    # Define analysis variables and labels\n",
    "    variables = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Centripetality_Gamma_mean', \n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Aesthetic variable labels\n",
    "    variable_labels = {\n",
    "        'log_in_degree_weighted': 'log(In-Degree)',\n",
    "        'Centripetality_Gamma_mean': 'Centripetality (Î“)',\n",
    "        'Anisotropy_Lambda_mean': 'Anisotropy (Î›)',\n",
    "        'jobs_density': 'Job Density'\n",
    "    }\n",
    "    \n",
    "    # Extract data and drop missing values\n",
    "    analysis_data = df[variables].dropna()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = analysis_data.corr()\n",
    "    \n",
    "    # Rename row and column labels\n",
    "    corr_matrix_labeled = corr_matrix.copy()\n",
    "    corr_matrix_labeled.index = [variable_labels[var] for var in corr_matrix.index]\n",
    "    corr_matrix_labeled.columns = [variable_labels[var] for var in corr_matrix.columns]\n",
    "    \n",
    "    # Set plot style\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # â–¼â–¼â–¼ Change color scheme â–¼â–¼â–¼\n",
    "    # ============================================================================\n",
    "    sns.heatmap(\n",
    "        corr_matrix_labeled, \n",
    "        annot=True,\n",
    "        cmap='coolwarm',     # Recommended: classic red-white-blue color scheme\n",
    "        # cmap='RdBu_r',     # Alternative: another classic red-white-blue\n",
    "        # cmap='PuOr',       # Alternative: soft purple-white-orange\n",
    "        # cmap='vlag',       # Alternative: modern red-gray-blue\n",
    "        center=0,\n",
    "        square=True,\n",
    "        fmt='.3f',\n",
    "        cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        annot_kws={'size': 12, 'weight': 'bold', 'color': 'black'},\n",
    "        vmin=-1, vmax=1\n",
    "    )\n",
    "    \n",
    "    # Aesthetic settings\n",
    "    ax.set_title('Correlation Matrix (Commuting Network)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(rotation=0, fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numeric matrix\n",
    "    print(\"\\nCorrelation Matrix Values:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(corr_matrix_labeled.round(3))\n",
    "    \n",
    "    # Significance test\n",
    "    print(\"\\nCorrelation Significance Test:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, var1 in enumerate(variables):\n",
    "        for j, var2 in enumerate(variables):\n",
    "            if i < j:  # Avoid duplicates\n",
    "                corr, p_value = pearsonr(analysis_data[var1], analysis_data[var2])\n",
    "                \n",
    "                # Significance marker\n",
    "                if p_value < 0.001:\n",
    "                    sig_level = \"***\"\n",
    "                elif p_value < 0.01:\n",
    "                    sig_level = \"**\"\n",
    "                elif p_value < 0.05:\n",
    "                    sig_level = \"*\"\n",
    "                else:\n",
    "                    sig_level = \"ns\"\n",
    "                \n",
    "                print(f\"{variable_labels[var1]} â†” {variable_labels[var2]}:\")\n",
    "                print(f\"   r = {corr:.3f}, p = {p_value:.4f} {sig_level}\")\n",
    "\n",
    "# Run function\n",
    "create_beautiful_correlation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8271be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_all_in_one_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def perform_diagnostics(df, model_name, independent_vars):\n",
    "    \"\"\"\n",
    "    Reliable collinearity diagnostics for a given set of independent variables.\n",
    "    1. Calculate pairwise correlations.\n",
    "    2. Manually compute VIF to avoid function bias.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing collinearity diagnostics for {model_name} ---\")\n",
    "    \n",
    "    # Extract independent variable data\n",
    "    X_data = df[independent_vars].dropna()\n",
    "    \n",
    "    # 1. Calculate and print correlation matrix\n",
    "    print(\"Correlation matrix of independent variables:\")\n",
    "    correlation_matrix = X_data.corr()\n",
    "    print(correlation_matrix.round(4))\n",
    "    \n",
    "    # 2. Manually calculate VIF\n",
    "    # For two independent variables, VIF is the same for both\n",
    "    var1, var2 = independent_vars[0], independent_vars[1]\n",
    "    y = X_data[var1]\n",
    "    X = X_data[[var2]] # Must be a DataFrame\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    rsquared = sm.OLS(y, X).fit().rsquared\n",
    "    vif = 1 / (1 - rsquared)\n",
    "    \n",
    "    print(f\"\\nManually calculated VIF: {vif:.4f}\")\n",
    "    \n",
    "    # 3. Final diagnostic conclusion\n",
    "    if vif >= 5:\n",
    "        print(f\"ðŸŸ¡ Diagnostic warning: VIF ({vif:.2f}) is greater than or equal to 5, potential collinearity issue.\")\n",
    "    else:\n",
    "        print(f\"âœ… Diagnostic conclusion: VIF ({vif:.2f}) is well below 5, no collinearity issue detected.\")\n",
    "\n",
    "def run_ols_regression(df, model_name, dependent_var, independent_vars):\n",
    "    \"\"\"\n",
    "    Run OLS regression and print summary results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running OLS regression for {model_name} ---\")\n",
    "    \n",
    "    # Prepare data (listwise deletion)\n",
    "    all_model_vars = [dependent_var] + independent_vars\n",
    "    model_data = df[all_model_vars].dropna()\n",
    "    \n",
    "    Y = model_data[dependent_var]\n",
    "    X = model_data[independent_vars]\n",
    "    X = sm.add_constant(X) # Intercept for OLS\n",
    "    \n",
    "    # Fit model\n",
    "    model_results = sm.OLS(Y, X).fit()\n",
    "    \n",
    "    # Print full regression results\n",
    "    print(f\"\\n{model_name}: Regression results for {dependent_var}\")\n",
    "    print(model_results.summary())\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function: integrates data cleaning, diagnostics, and final regression analysis.\n",
    "    \"\"\"\n",
    "    # --- 1. Parameter configuration ---\n",
    "    FILE_A_INTRA_METRICS = 'ttwa_mobility_indicators_daily_aggregated_Feb.csv'\n",
    "    FILE_B_INTER_METRICS = 'inter_city_in_degree_metrics_filter.csv'\n",
    "    FILE_C_CONTROL_VARS = 'variables/jobdensity2023.xlsx'\n",
    "    \n",
    "    FLOW_COUNT_COLUMN = 'total_internal_flow' \n",
    "    MIN_FLOW_THRESHOLD = 500\n",
    "    \n",
    "    # --- 2. Read and prepare data ---\n",
    "    try:\n",
    "        df_a = pd.read_csv(FILE_A_INTRA_METRICS)\n",
    "        df_b = pd.read_csv(FILE_B_INTER_METRICS)\n",
    "        df_c = pd.read_excel(FILE_C_CONTROL_VARS)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error: File {e.filename} not found. Please check the file path.\")\n",
    "        return\n",
    "\n",
    "    df_c.rename(columns={'travel to work area 2011-based': 'TTWA_Name', 'Jobs density': 'jobs_density'}, inplace=True)\n",
    "    df_c = df_c[['TTWA_Name', 'jobs_density']]\n",
    "    if 'TTWA_Code' in df_b.columns:\n",
    "        df_b.rename(columns={'TTWA_Code': 'TTWA_Name'}, inplace=True)\n",
    "\n",
    "    # --- 3. Data cleaning ---\n",
    "    # Cleaning step 1: based on flow threshold\n",
    "    if FLOW_COUNT_COLUMN in df_a.columns:\n",
    "        df_a = df_a[df_a[FLOW_COUNT_COLUMN] >= MIN_FLOW_THRESHOLD]\n",
    "    \n",
    "    # Merge data\n",
    "    df_merged = pd.merge(pd.merge(df_a, df_b, on='TTWA_Name', how='inner'), df_c, on='TTWA_Name', how='inner')\n",
    "    \n",
    "    # Cleaning step 2: remove missing values in key analysis columns\n",
    "    ANALYSIS_COLS = ['Centripetality_Gamma_mean', 'Anisotropy_Lambda_mean', 'in_degree_weighted', 'jobs_density']\n",
    "    master_df = df_merged.dropna(subset=ANALYSIS_COLS)\n",
    "    \n",
    "    # Prepare dependent variable\n",
    "    master_df['log_in_degree_weighted'] = np.log1p(master_df['in_degree_weighted'])\n",
    "    \n",
    "    print(f\"Data cleaning and preparation complete. Final number of TTWAs for analysis: {len(master_df)}.\")\n",
    "\n",
    "    # --- 4. Diagnostics and regression for two models ---\n",
    "    dependent_var = 'log_in_degree_weighted'\n",
    "    \n",
    "    # --- Model 1 (Gamma) ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Starting analysis for Model 1: log(InDegree) ~ Centripetality_Gamma + JobDensity\")\n",
    "    print(\"=\"*80)\n",
    "    model1_vars = ['Centripetality_Gamma_mean', 'jobs_density']\n",
    "    perform_diagnostics(master_df, \"Model 1 (Gamma)\", model1_vars)\n",
    "    run_ols_regression(master_df, \"Model 1 (Gamma)\", dependent_var, model1_vars)\n",
    "\n",
    "    # --- Model 2 (Lambda) ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Starting analysis for Model 2: log(InDegree) ~ Anisotropy_Lambda + JobDensity\")\n",
    "    print(\"=\"*80)\n",
    "    model2_vars = ['Anisotropy_Lambda_mean', 'jobs_density']\n",
    "    perform_diagnostics(master_df, \"Model 2 (Lambda)\", model2_vars)\n",
    "    run_ols_regression(master_df, \"Model 2 (Lambda)\", dependent_var, model2_vars)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All analyses completed.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scipy statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94427782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_regression_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def run_regression_analysis():\n",
    "    \"\"\"\n",
    "    This script loads the final cleaned dataset and runs two independent multiple linear regression models,\n",
    "    testing the relationship between Gamma and Lambda with in-degree, controlling for jobs_density.\n",
    "    \"\"\"\n",
    "    print(\"--- Running regression analysis ---\")\n",
    "\n",
    "    # --- 1. File path ---\n",
    "    # This file should be the output from your previous cleaning script\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv' \n",
    "\n",
    "    # --- 2. Read data ---\n",
    "    print(f\"\\n>>> Reading analysis data from '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(\"   Data loaded successfully.\")\n",
    "        print(f\"   Final dataset contains {len(df)} TTWA for analysis.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        print(\"   Please make sure you have run the data cleaning and merging script to generate this file.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Run first regression model (using Gamma) ---\n",
    "    print(\"\\n>>> Running Model 1: Using Centripetality_Gamma_mean...\")\n",
    "    \n",
    "    # Define formula for Model 1 (y ~ x1 + x2)\n",
    "    formula1 = 'in_degree_weighted ~ Centripetality_Gamma_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        # Create and fit OLS model\n",
    "        model1 = smf.ols(formula=formula1, data=df).fit()\n",
    "        \n",
    "        # Print full regression summary\n",
    "        print(\"\\n--- Model 1: Regression Results (Gamma) ---\")\n",
    "        print(model1.summary())\n",
    "        print(\"------------------------------------------\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred while running Model 1: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Run second regression model (using Lambda) ---\n",
    "    print(\"\\n>>> Running Model 2: Using Anisotropy_Lambda_mean...\")\n",
    "\n",
    "    # Define formula for Model 2\n",
    "    formula2 = 'in_degree_weighted ~ Anisotropy_Lambda_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        # Create and fit OLS model\n",
    "        model2 = smf.ols(formula=formula2, data=df).fit()\n",
    "        \n",
    "        # Print full regression summary\n",
    "        print(\"\\n--- Model 2: Regression Results (Lambda) ---\")\n",
    "        print(model2.summary())\n",
    "        print(\"------------------------------------------\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred while running Model 2: {e}\")\n",
    "\n",
    "# --- Run main function ---\n",
    "run_regression_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320d7b2",
   "metadata": {},
   "source": [
    "Run the regression again after log-transforming in-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_log_transformed_regression.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  # Import numpy for mathematical transformation\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def run_log_transformed_regression():\n",
    "    \"\"\"\n",
    "    This script loads the final cleaned dataset, applies log transformation to the dependent variable\n",
    "    'in_degree_weighted', and runs two multiple linear regression models for more robust results.\n",
    "    \"\"\"\n",
    "    print(\"--- Running log-transformed regression analysis (robust models) ---\")\n",
    "\n",
    "    # --- 1. File path ---\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "\n",
    "    # --- 2. Read data ---\n",
    "    print(f\"\\n>>> Reading analysis data from '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(\"   Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Core improvement: log-transform the dependent variable ---\n",
    "    print(\"\\n>>> Applying log transformation to dependent variable 'in_degree_weighted'...\")\n",
    "    \n",
    "    # Use np.log1p for transformation, which computes log(1 + x).\n",
    "    # This is a robust data processing technique to handle possible zeros in the original data.\n",
    "    # Using np.log(0) would cause errors.\n",
    "    log_target_col = 'log_in_degree_weighted'\n",
    "    original_target_col = 'in_degree_weighted'\n",
    "\n",
    "    if original_target_col in df.columns:\n",
    "        df[log_target_col] = np.log1p(df[original_target_col])\n",
    "        print(f\"   New column '{log_target_col}' created for analysis.\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: Original dependent variable column '{original_target_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Run first regression model (using Gamma) ---\n",
    "    print(\"\\n>>> Running Model 1 (using Gamma)...\")\n",
    "    \n",
    "    # Note: The dependent variable on the left side of the formula is now the log-transformed column\n",
    "    formula1 = f'{log_target_col} ~ Centripetality_Gamma_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        model1 = smf.ols(formula=formula1, data=df).fit()\n",
    "        print(\"\\n--- Model 1: Regression Results (log-transformed, using Gamma) ---\")\n",
    "        print(model1.summary())\n",
    "        print(\"--------------------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred while running Model 1: {e}\")\n",
    "\n",
    "    # --- 5. Run second regression model (using Lambda) ---\n",
    "    print(\"\\n>>> Running Model 2 (using Lambda)...\")\n",
    "\n",
    "    formula2 = f'{log_target_col} ~ Anisotropy_Lambda_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        model2 = smf.ols(formula=formula2, data=df).fit()\n",
    "        print(\"\\n--- Model 2: Regression Results (log-transformed, using Lambda) ---\")\n",
    "        print(model2.summary())\n",
    "        print(\"--------------------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred while running Model 2: {e}\")\n",
    "\n",
    "# --- Run main function ---\n",
    "run_log_transformed_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
