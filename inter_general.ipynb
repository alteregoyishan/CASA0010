{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read data and calculate dependency weights\n",
    "def calculate_dependency_weights(df):\n",
    "    \"\"\"\n",
    "    Calculate dependency weights w_ij = t_ij / Σ_j t_ij\n",
    "    where t_ij is the commuting flow from i to j\n",
    "    \"\"\"\n",
    "    # Calculate total outflow for each origin city\n",
    "    total_outflow = df.groupby('ORIGIN_TTWA')['TOTAL_FLOW'].sum()\n",
    "    \n",
    "    # Calculate dependency weights\n",
    "    df['dependency_weight'] = df.apply(\n",
    "        lambda row: row['TOTAL_FLOW'] / total_outflow[row['ORIGIN_TTWA']], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('ttwa_od_matrix_cross_city_only.csv')\n",
    "df_with_weights = calculate_dependency_weights(df)\n",
    "\n",
    "print(f\"Data overview: {len(df)} OD records, {df['ORIGIN_TTWA'].nunique()} origin cities\")\n",
    "print(f\"Dependency weight range: {df_with_weights['dependency_weight'].min():.6f} - {df_with_weights['dependency_weight'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build directed commuting network\n",
    "def build_commuting_network(df_weights):\n",
    "    \"\"\"Build directed commuting network\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges with dependency weights\n",
    "    for _, row in df_weights.iterrows():\n",
    "        G.add_edge(\n",
    "            row['ORIGIN_TTWA'], \n",
    "            row['DEST_TTWA'], \n",
    "            weight=row['dependency_weight'],\n",
    "            flow=row['TOTAL_FLOW']\n",
    "        )\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = build_commuting_network(df_with_weights)\n",
    "print(f\"Network size: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a156f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume G already exists\n",
    "\n",
    "def perform_percolation_analysis(G):\n",
    "    \"\"\"\n",
    "    Perform percolation analysis on the given commuting network G.\n",
    "    \"\"\"\n",
    "    # Step 1: Get all unique dependency weights as thresholds and sort in descending order\n",
    "    weights = sorted(\n",
    "        list(set(nx.get_edge_attributes(G, 'weight').values())), \n",
    "        reverse=True\n",
    "    )\n",
    "    if 0 not in weights:\n",
    "        weights.append(0)\n",
    "\n",
    "    print(f\"Starting percolation analysis, will use {len(weights)} unique thresholds...\")\n",
    "\n",
    "    percolation_results = []\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Step 2: Iterate through all thresholds\n",
    "    for i, tau in enumerate(weights):\n",
    "        # Create a subgraph containing only edges with weight >= tau\n",
    "        sub_G = nx.DiGraph()\n",
    "        sub_G.add_nodes_from(G.nodes())\n",
    "        edges_to_add = [(u, v, d) for u, v, d in G.edges(data=True) if d['weight'] >= tau]\n",
    "        sub_G.add_edges_from(edges_to_add)\n",
    "\n",
    "        # Find connected components (clusters)\n",
    "        clusters = list(nx.weakly_connected_components(sub_G))\n",
    "        \n",
    "        # Calculate giant component size\n",
    "        if clusters:\n",
    "            giant_component_size = max(len(c) for c in clusters)\n",
    "        else:\n",
    "            giant_component_size = 0\n",
    "        \n",
    "        percolation_results.append({\n",
    "            'threshold': tau,\n",
    "            'num_clusters': len(clusters),\n",
    "            'clusters': clusters,\n",
    "            'giant_component_size': giant_component_size,\n",
    "            'giant_component_size_ratio': giant_component_size / total_nodes if total_nodes > 0 else 0\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(percolation_results)\n",
    "\n",
    "# Run analysis\n",
    "df_percolation = perform_percolation_analysis(G)\n",
    "\n",
    "# Identify critical points\n",
    "df_percolation = df_percolation.sort_values('threshold', ascending=False).reset_index(drop=True)\n",
    "df_percolation['size_increase'] = df_percolation['giant_component_size'].diff().fillna(0)\n",
    "\n",
    "non_zero_increases = df_percolation[df_percolation['size_increase'] > 0]['size_increase']\n",
    "\n",
    "if not non_zero_increases.empty:\n",
    "    # Define a jump threshold, e.g., 70th percentile\n",
    "    jump_threshold = non_zero_increases.quantile(0.95) \n",
    "    \n",
    "    # Filter out rows with \"jumps\", resulting in a DataFrame\n",
    "    critical_transitions_df = df_percolation[df_percolation['size_increase'] >= jump_threshold]\n",
    "    \n",
    "    # Missing critical step added here\n",
    "    # Convert DataFrame to dictionary list format required by subsequent code\n",
    "    filtered_critical_points = critical_transitions_df.to_dict('records')\n",
    "\n",
    "    print(f\"\\nSuccessfully created 'filtered_critical_points' variable containing {len(filtered_critical_points)} critical points.\")\n",
    "    # (Optional) Print preview\n",
    "    # print(\"Preview:\", filtered_critical_points[:3])\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo growth found in giant component size.\")\n",
    "    filtered_critical_points = [] # If no critical points, create empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume df_percolation is the DataFrame generated from the previous step\n",
    "# Ensure data is sorted in descending order by threshold\n",
    "df_percolation = df_percolation.sort_values('threshold', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate the change in giant component size brought by each threshold step\n",
    "# diff() calculates the difference between the current row and the previous row. Since threshold is decreasing, size is increasing.\n",
    "df_percolation['size_increase'] = df_percolation['giant_component_size'].diff().fillna(0)\n",
    "\n",
    "print(\"Percolation analysis results with size increments:\")\n",
    "print(df_percolation[['threshold', 'giant_component_size', 'size_increase']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From all non-zero growths, determine a \"mutation\" threshold\n",
    "# For example, we define growth in the top 15% as mutation (i.e., greater than 85th percentile)\n",
    "non_zero_increases = df_percolation[df_percolation['size_increase'] > 0]['size_increase']\n",
    "\n",
    "if not non_zero_increases.empty:\n",
    "    # Define a jump threshold, e.g., 85th percentile\n",
    "    jump_threshold = non_zero_increases.quantile(0.95) \n",
    "    print(f\"\\nDefining \\\"mutation\\\" size threshold as: {jump_threshold:.2f} \")\n",
    "\n",
    "    # Filter out rows with \"mutations\"\n",
    "    critical_transitions_df = df_percolation[df_percolation['size_increase'] >= jump_threshold]\n",
    "\n",
    "    # Extract these critical thresholds\n",
    "    critical_thresholds = sorted(critical_transitions_df['threshold'].unique(), reverse=True)\n",
    "\n",
    "    print(\"\\nIdentified Critical Thresholds include:\")\n",
    "    print(critical_thresholds)\n",
    "else:\n",
    "    print(\"\\nNo growth found in giant component size.\")\n",
    "    critical_thresholds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Mark critical thresholds\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Draw basic curve\n",
    "ax.plot(\n",
    "    df_percolation['threshold'], \n",
    "    df_percolation['giant_component_size_ratio'],\n",
    "    color='gray',\n",
    "    linestyle='-',\n",
    "    alpha=0.7,\n",
    "    label='Giant Component Evolution'\n",
    ")\n",
    "\n",
    "# If critical points are found, mark them on the plot\n",
    "if 'critical_transitions_df' in locals() and not critical_transitions_df.empty:\n",
    "    ax.plot(\n",
    "        critical_transitions_df['threshold'], \n",
    "        critical_transitions_df['giant_component_size_ratio'],\n",
    "        'ro', # 'r' is red, 'o' is circle marker\n",
    "        markersize=6,\n",
    "        label='Critical Transitions'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Threshold (τ)')\n",
    "ax.set_ylabel('Giant Cluster Size (Ratio of Total Nodes)')\n",
    "ax.set_title('Percolation Analysis with Critical Transitions Highlighted')\n",
    "ax.invert_xaxis() # Invert X-axis\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653eb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Mark critical thresholds\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Draw basic curve\n",
    "ax.plot(\n",
    "    df_percolation['threshold'], \n",
    "    df_percolation['giant_component_size_ratio'],\n",
    "    color='gray',\n",
    "    linestyle='-',\n",
    "    alpha=0.7,\n",
    "    label='Giant Component Evolution'\n",
    ")\n",
    "\n",
    "# If critical points are found, mark them on the plot\n",
    "if 'critical_transitions_df' in locals() and not critical_transitions_df.empty:\n",
    "    ax.plot(\n",
    "        critical_transitions_df['threshold'], \n",
    "        critical_transitions_df['giant_component_size_ratio'],\n",
    "        'ro', # 'r' is red, 'o' is circle marker\n",
    "        markersize=6,\n",
    "        label='Critical Transitions'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Threshold (τ)')\n",
    "ax.set_ylabel('Giant Cluster Size (Ratio of Total Nodes)')\n",
    "ax.set_title('Percolation Analysis with Critical Transitions Highlighted')\n",
    "ax.invert_xaxis() # Invert X-axis\n",
    "# ax.set_xlim(1, 0)\n",
    "ax.legend()\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization: Mark critical thresholds (final beautified version)\n",
    "fig, ax = plt.subplots(figsize=(12, 8)) # Slightly increase height to accommodate labels\n",
    "\n",
    "# 1. Draw step plot curve\n",
    "ax.plot(\n",
    "    df_percolation['threshold'], \n",
    "    df_percolation['giant_component_size_ratio'],\n",
    "    color='darkgray',       # Use dark gray for clearer contrast\n",
    "    linestyle='-',\n",
    "    linewidth=1.5,          # Set appropriate line width\n",
    "    label='Giant Component Evolution',\n",
    "    drawstyle='steps-post'  # Key: Use step plot style\n",
    ")\n",
    "\n",
    "# 2. Highlight critical points\n",
    "if 'critical_transitions_df' in locals() and not critical_transitions_df.empty:\n",
    "    ax.plot(\n",
    "        critical_transitions_df['threshold'], \n",
    "        critical_transitions_df['giant_component_size_ratio'],\n",
    "        marker='o',             # Set marker to circle\n",
    "        color='crimson',        # Use more vivid dark red\n",
    "        markersize=8,           # Slightly increase marker size\n",
    "        markeredgecolor='white',# Key: Add white edge to make it more prominent\n",
    "        linestyle='none',       # Don't draw connecting lines\n",
    "        label='Critical Transitions'\n",
    "    )\n",
    "\n",
    "# 3. Set title and axis labels\n",
    "ax.set_xlabel('Threshold (τ)', fontsize=14, color='black')\n",
    "ax.set_ylabel('Giant Cluster Size (Ratio of Total Nodes)', fontsize=14, color='black')\n",
    "ax.set_title('Percolation Analysis with Critical Transitions Highlighted', fontsize=16, color='black')\n",
    "\n",
    "# 4. Format axes\n",
    "ax.invert_xaxis()      # Invert X-axis\n",
    "ax.set_xlim(1, 0)      # Key: Ensure X-axis starts from 1\n",
    "ax.set_ylim(0, 1.05)   # Leave space at top for Y-axis\n",
    "\n",
    "# 5. Set axis line and tick styles\n",
    "# Hide top and right axis lines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Set bottom and left axis lines to black\n",
    "ax.spines['left'].set_color('black')\n",
    "ax.spines['bottom'].set_color('black')\n",
    "\n",
    "# Set X and Y axis tick marks to black\n",
    "ax.tick_params(axis='x', colors='black', length=6) # length is tick line length\n",
    "ax.tick_params(axis='y', colors='black', length=6)\n",
    "\n",
    "# 6. Legend and final display\n",
    "ax.legend(fontsize=12, frameon=False) # frameon=False makes legend borderless\n",
    "ax.grid(False)      # Ensure no grid lines\n",
    "plt.tight_layout()  # Auto-adjust layout to prevent label overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52428e6",
   "metadata": {},
   "source": [
    "——————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec038a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete percolation analysis code - one-stop solution\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from scipy.signal import find_peaks\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def complete_percolation_analysis_pipeline(G, reference_thresholds=None, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Complete percolation analysis pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX directed graph\n",
    "    - reference_thresholds: reference threshold list (for validation)\n",
    "    - num_samples: number of sampling points\n",
    "    \n",
    "    Returns:\n",
    "    - analysis_results: dictionary containing all analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting complete percolation analysis pipeline...\")\n",
    "    print(f\"   Network info: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Step 1: Data preparation and threshold generation\n",
    "    print(\"\\nStep 1: Data preparation...\")\n",
    "    \n",
    "    # Get all edge weights\n",
    "    edges_data = list(G.edges(data=True))\n",
    "    weights = [data.get('weight', 1.0) for _, _, data in edges_data]\n",
    "    min_weight = min(weights)\n",
    "    max_weight = max(weights)\n",
    "    \n",
    "    print(f\"   Weight range: {min_weight:.6f} - {max_weight:.6f}\")\n",
    "    \n",
    "    # Smart sampling strategy: densify in key regions\n",
    "    if reference_thresholds:\n",
    "        print(f\"   Intelligent sampling based on {len(reference_thresholds)} reference thresholds\")\n",
    "        # Basic logarithmic sampling\n",
    "        base_thresholds = np.logspace(np.log10(min_weight), np.log10(max_weight), num_samples//2)\n",
    "        \n",
    "        # Dense sampling around reference thresholds\n",
    "        dense_thresholds = []\n",
    "        for ref_thresh in reference_thresholds:\n",
    "            if min_weight <= ref_thresh <= max_weight:\n",
    "                # Densify within ±15% range around reference point\n",
    "                local_min = max(min_weight, ref_thresh * 0.85)\n",
    "                local_max = min(max_weight, ref_thresh * 1.15)\n",
    "                local_samples = np.linspace(local_min, local_max, 30)\n",
    "                dense_thresholds.extend(local_samples)\n",
    "        \n",
    "        # Merge and deduplicate\n",
    "        all_thresholds = np.concatenate([base_thresholds, dense_thresholds])\n",
    "        thresholds = np.unique(all_thresholds)\n",
    "    else:\n",
    "        # Standard logarithmic sampling\n",
    "        thresholds = np.logspace(np.log10(min_weight), np.log10(max_weight), num_samples)\n",
    "    \n",
    "    # Sort and filter\n",
    "    thresholds = thresholds[(thresholds >= min_weight) & (thresholds <= max_weight)]\n",
    "    thresholds = np.sort(thresholds)[::-1]  # Sort from large to small\n",
    "    \n",
    "    print(f\"   Generated {len(thresholds)} sampling points\")\n",
    "    \n",
    "    # Step 2: Percolation analysis calculation\n",
    "    print(\"\\nStep 2: Percolation analysis calculation...\")\n",
    "    \n",
    "    giant_sizes = []\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"   Progress: {i}/{len(thresholds)} ({i/len(thresholds)*100:.1f}%)\")\n",
    "        \n",
    "        # Create subgraph: keep edges with weight >= threshold\n",
    "        edges_to_keep = [(u, v) for u, v, data in G.edges(data=True) \n",
    "                        if data.get('weight', 1.0) >= threshold]\n",
    "        \n",
    "        if len(edges_to_keep) == 0:\n",
    "            giant_sizes.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Create undirected subgraph for calculating connected components\n",
    "        subgraph = nx.Graph()\n",
    "        subgraph.add_edges_from(edges_to_keep)\n",
    "        \n",
    "        if subgraph.number_of_nodes() > 0:\n",
    "            components = list(nx.connected_components(subgraph))\n",
    "            largest_component_size = max(len(comp) for comp in components)\n",
    "            giant_size_fraction = largest_component_size / total_nodes\n",
    "        else:\n",
    "            giant_size_fraction = 0.0\n",
    "        \n",
    "        giant_sizes.append(giant_size_fraction)\n",
    "    \n",
    "    giant_sizes = np.array(giant_sizes)\n",
    "    print(\"Percolation analysis calculation completed\")\n",
    "    \n",
    "    # Step 3: Critical threshold detection\n",
    "    print(\"\\nStep 3: Critical threshold detection...\")\n",
    "    \n",
    "    critical_thresholds = []\n",
    "    \n",
    "    # Method 1: Reference threshold matching\n",
    "    if reference_thresholds:\n",
    "        print(\"   Method 1: Reference threshold matching...\")\n",
    "        for ref_thresh in reference_thresholds:\n",
    "            idx = np.argmin(np.abs(thresholds - ref_thresh))\n",
    "            distance = np.abs(thresholds[idx] - ref_thresh)\n",
    "            relative_error = distance / ref_thresh\n",
    "            \n",
    "            if relative_error < 0.15:  # Relative error less than 15%\n",
    "                critical_thresholds.append({\n",
    "                    'threshold': thresholds[idx],\n",
    "                    'giant_size': giant_sizes[idx],\n",
    "                    'type': 'reference_match',\n",
    "                    'reference': ref_thresh,\n",
    "                    'error': relative_error\n",
    "                })\n",
    "    \n",
    "    # Method 2: Mathematical method for detecting mutation points\n",
    "    print(\"   Method 2: Mathematical mutation detection...\")\n",
    "    \n",
    "    # Data smoothing\n",
    "    try:\n",
    "        smoothed_sizes = ndimage.gaussian_filter1d(giant_sizes, sigma=3)\n",
    "    except:\n",
    "        smoothed_sizes = giant_sizes\n",
    "    \n",
    "    # Calculate derivative\n",
    "    log_thresholds = np.log10(thresholds)\n",
    "    dx = np.diff(log_thresholds)\n",
    "    dy = np.diff(smoothed_sizes)\n",
    "    first_derivative = dy / dx\n",
    "    \n",
    "    # Find extreme points in derivative\n",
    "    if len(first_derivative) > 10:\n",
    "        # Find maximum decline points\n",
    "        negative_peaks, _ = find_peaks(-first_derivative, height=0.01)\n",
    "        for peak_idx in negative_peaks[:3]:  # Take top 3 most significant\n",
    "            if peak_idx < len(thresholds):\n",
    "                critical_thresholds.append({\n",
    "                    'threshold': thresholds[peak_idx],\n",
    "                    'giant_size': giant_sizes[peak_idx],\n",
    "                    'type': 'slope_peak',\n",
    "                    'strength': np.abs(first_derivative[peak_idx])\n",
    "                })\n",
    "    \n",
    "    # Method 3: Specific fraction points\n",
    "    print(\"   Method 3: Specific fraction point detection...\")\n",
    "    target_fractions = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    for frac in target_fractions:\n",
    "        idx = np.argmin(np.abs(giant_sizes - frac))\n",
    "        if np.abs(giant_sizes[idx] - frac) < 0.05:  # Error tolerance\n",
    "            critical_thresholds.append({\n",
    "                'threshold': thresholds[idx],\n",
    "                'giant_size': giant_sizes[idx],\n",
    "                'type': f'fraction_{frac}',\n",
    "                'target_fraction': frac\n",
    "            })\n",
    "    \n",
    "    # Deduplicate and select best thresholds\n",
    "    print(\"   Organizing and deduplicating...\")\n",
    "    \n",
    "    # Group by threshold, keep best in each group\n",
    "    threshold_groups = {}\n",
    "    for point in critical_thresholds:\n",
    "        thresh_key = round(point['threshold'], 5)\n",
    "        if (thresh_key not in threshold_groups or \n",
    "            point.get('error', 1.0) < threshold_groups[thresh_key].get('error', 1.0)):\n",
    "            threshold_groups[thresh_key] = point\n",
    "    \n",
    "    # Prioritize reference_match types\n",
    "    final_critical_points = []\n",
    "    reference_matches = [p for p in threshold_groups.values() if p['type'] == 'reference_match']\n",
    "    other_points = [p for p in threshold_groups.values() if p['type'] != 'reference_match']\n",
    "    \n",
    "    # First add all reference_matches\n",
    "    final_critical_points.extend(reference_matches)\n",
    "    \n",
    "    # Then add other high-quality points, ensure total doesn't exceed 10\n",
    "    other_points.sort(key=lambda x: x.get('strength', 0), reverse=True)\n",
    "    needed = min(10 - len(final_critical_points), len(other_points))\n",
    "    final_critical_points.extend(other_points[:needed])\n",
    "    \n",
    "    # Sort by threshold\n",
    "    final_critical_points.sort(key=lambda x: x['threshold'], reverse=True)\n",
    "    \n",
    "    print(f\"Detected {len(final_critical_points)} critical thresholds\")\n",
    "    \n",
    "    # Return analysis results\n",
    "    analysis_results = {\n",
    "        'thresholds': thresholds,\n",
    "        'giant_sizes': giant_sizes,\n",
    "        'critical_points': final_critical_points,\n",
    "        'first_derivative': first_derivative if 'first_derivative' in locals() else None,\n",
    "        'network_info': {\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'weight_range': [min_weight, max_weight]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Run complete analysis\n",
    "\n",
    "# Your reference thresholds\n",
    "reference_thresholds = [0.274173, 0.243051, 0.217115, 0.175618, \n",
    "                       0.149683, 0.134121, 0.113373, 0.051128]\n",
    "\n",
    "print(\"Reference thresholds:\")\n",
    "for i, thresh in enumerate(reference_thresholds):\n",
    "    print(f\"   {i+1}. τ = {thresh:.6f}\")\n",
    "\n",
    "# Execute complete analysis\n",
    "analysis_results = complete_percolation_analysis_pipeline(\n",
    "    G, \n",
    "    reference_thresholds=reference_thresholds, \n",
    "    num_samples=1200\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalysis completed!\")\n",
    "print(f\"   Sampling points: {len(analysis_results['thresholds'])}\")\n",
    "print(f\"   Critical thresholds: {len(analysis_results['critical_points'])}\")\n",
    "\n",
    "# Display detected critical thresholds\n",
    "print(f\"\\nDetected critical thresholds:\")\n",
    "for i, point in enumerate(analysis_results['critical_points']):\n",
    "    if point['type'] == 'reference_match':\n",
    "        error = point['error'] * 100\n",
    "        print(f\"   {i+1}. τ = {point['threshold']:.6f} (REF: {point['reference']:.6f}, error: {error:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   {i+1}. τ = {point['threshold']:.6f} ({point['type']})\")\n",
    "\n",
    "# Save key variables for visualization use\n",
    "final_analysis_thresholds = analysis_results['thresholds']\n",
    "final_analysis_giant_sizes = analysis_results['giant_sizes']\n",
    "final_analysis_critical_points = analysis_results['critical_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced percolation curve visualization - final version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_enhanced_percolation_curve_final(thresholds, giant_sizes, critical_points, \n",
    "                                         save_path=None, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create enhanced percolation curve visualization - final optimized version\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration settings\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Define professional color scheme\n",
    "    colors = {\n",
    "        'main_line': '#2E86AB',      # Professional blue\n",
    "        'critical': '#A23B72',       # Burgundy\n",
    "        'reference': '#F18F01',      # Orange\n",
    "        'background': '#F8F9FA',     # Light gray\n",
    "        'grid': '#E9ECEF',          # Lighter gray\n",
    "        'text': '#212529'           # Dark gray\n",
    "    }\n",
    "    \n",
    "    # Set background color\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor(colors['background'])\n",
    "    \n",
    "    # Plot main percolation curve with gradient effect\n",
    "    x_data = thresholds\n",
    "    y_data = giant_sizes\n",
    "    \n",
    "    # Create main curve with thicker line\n",
    "    main_line = ax.plot(x_data, y_data, \n",
    "                       color=colors['main_line'], \n",
    "                       linewidth=3.5, \n",
    "                       alpha=0.9,\n",
    "                       label='Giant Component Size',\n",
    "                       zorder=3)\n",
    "    \n",
    "    # Add gradient fill below curve\n",
    "    ax.fill_between(x_data, 0, y_data, \n",
    "                   color=colors['main_line'], \n",
    "                   alpha=0.15, \n",
    "                   zorder=1)\n",
    "    \n",
    "    # Mark critical points with enhanced styling\n",
    "    reference_points = []\n",
    "    other_points = []\n",
    "    \n",
    "    for point in critical_points:\n",
    "        if point['type'] == 'reference_match':\n",
    "            reference_points.append(point)\n",
    "        else:\n",
    "            other_points.append(point)\n",
    "    \n",
    "    # Plot reference matching points\n",
    "    if reference_points:\n",
    "        ref_x = [p['threshold'] for p in reference_points]\n",
    "        ref_y = [p['giant_size'] for p in reference_points]\n",
    "        \n",
    "        ax.scatter(ref_x, ref_y, \n",
    "                  c=colors['reference'], \n",
    "                  s=200, \n",
    "                  alpha=0.9,\n",
    "                  edgecolors='white',\n",
    "                  linewidth=2,\n",
    "                  marker='o',\n",
    "                  label=f'Reference Match ({len(ref_x)})',\n",
    "                  zorder=5)\n",
    "        \n",
    "        # Add number annotations for reference points\n",
    "        for i, (x, y, point) in enumerate(zip(ref_x, ref_y, reference_points)):\n",
    "            # Background circle for better readability\n",
    "            circle = patches.Circle((x, y), \n",
    "                                  radius=max(x_data) * 0.015, \n",
    "                                  facecolor='white', \n",
    "                                  edgecolor=colors['reference'],\n",
    "                                  linewidth=1.5,\n",
    "                                  alpha=0.9,\n",
    "                                  zorder=4)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Number annotation\n",
    "            ax.annotate(f'{i+1}', \n",
    "                       xy=(x, y), \n",
    "                       xytext=(0, 0), \n",
    "                       textcoords='offset points',\n",
    "                       ha='center', va='center',\n",
    "                       fontsize=10,\n",
    "                       fontweight='bold',\n",
    "                       color=colors['text'],\n",
    "                       zorder=6)\n",
    "    \n",
    "    # Plot other critical points  \n",
    "    if other_points:\n",
    "        other_x = [p['threshold'] for p in other_points]\n",
    "        other_y = [p['giant_size'] for p in other_points]\n",
    "        \n",
    "        ax.scatter(other_x, other_y, \n",
    "                  c=colors['critical'], \n",
    "                  s=120, \n",
    "                  alpha=0.8,\n",
    "                  edgecolors='white',\n",
    "                  linewidth=1.5,\n",
    "                  marker='^',\n",
    "                  label=f'Other Critical Points ({len(other_x)})',\n",
    "                  zorder=4)\n",
    "    \n",
    "    # Enhanced grid settings\n",
    "    ax.grid(True, alpha=0.4, color=colors['grid'], linestyle='-', linewidth=0.8)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Axis settings\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Percolation Threshold τ', fontsize=14, fontweight='bold', color=colors['text'])\n",
    "    ax.set_ylabel('Giant Component Size (Fraction)', fontsize=14, fontweight='bold', color=colors['text'])\n",
    "    \n",
    "    # Set axis ranges\n",
    "    x_min, x_max = min(x_data), max(x_data)\n",
    "    y_min, y_max = 0, 1\n",
    "    \n",
    "    ax.set_xlim(x_min * 0.8, x_max * 1.2)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    # Enhanced title\n",
    "    base_title = \"Network Percolation Analysis\"\n",
    "    if title_suffix:\n",
    "        title = f\"{base_title} - {title_suffix}\"\n",
    "    else:\n",
    "        title = base_title\n",
    "    \n",
    "    ax.set_title(title, \n",
    "                fontsize=16, \n",
    "                fontweight='bold', \n",
    "                color=colors['text'],\n",
    "                pad=20)\n",
    "    \n",
    "    # Professional legend\n",
    "    legend = ax.legend(loc='upper right', \n",
    "                      frameon=True, \n",
    "                      fancybox=True, \n",
    "                      shadow=True,\n",
    "                      fontsize=11,\n",
    "                      framealpha=0.95)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor(colors['grid'])\n",
    "    \n",
    "    # Add information text box\n",
    "    info_text = f\"\"\"Network Information:\n",
    "    Nodes: {len(set([edge[0] for edge in G.edges()] + [edge[1] for edge in G.edges()]))}\n",
    "    Edges: {G.number_of_edges()}\n",
    "    Critical Points: {len(critical_points)}\"\"\"\n",
    "    \n",
    "    # Create text box with professional styling\n",
    "    textstr = info_text\n",
    "    props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=colors['grid'])\n",
    "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=props, color=colors['text'])\n",
    "    \n",
    "    # Fine-tune tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11, colors=colors['text'])\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=9, colors=colors['text'])\n",
    "    \n",
    "    # Set spine colors\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(colors['grid'])\n",
    "        spine.set_linewidth(1.2)\n",
    "    \n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path specified\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, \n",
    "                   dpi=300, \n",
    "                   bbox_inches='tight', \n",
    "                   facecolor='white',\n",
    "                   edgecolor='none')\n",
    "        print(f\"Chart saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Output critical points summary\n",
    "    print(\"\\nCritical Points Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, point in enumerate(critical_points):\n",
    "        print(f\"{i+1:2d}. τ = {point['threshold']:.6f}, \"\n",
    "              f\"Size = {point['giant_size']:.3f}, \"\n",
    "              f\"Type: {point['type']}\")\n",
    "        \n",
    "        if point['type'] == 'reference_match':\n",
    "            error = point.get('error', 0) * 100\n",
    "            print(f\"     Reference: {point['reference']:.6f}, Error: {error:.1f}%\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generate enhanced visualization\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_filename = f\"final_percolation_analysis_{timestamp}.png\"\n",
    "\n",
    "plot_enhanced_percolation_curve_final(\n",
    "    final_analysis_thresholds,\n",
    "    final_analysis_giant_sizes,\n",
    "    final_analysis_critical_points,\n",
    "    save_path=save_filename,\n",
    "    title_suffix=\"TTWA Commuting Network\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVisualization completed!\")\n",
    "print(f\"File saved as: {save_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive network visualization function\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_interactive_network_map(G, threshold, coordinates_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create interactive network map using Plotly\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - threshold: percolation threshold value\n",
    "    - coordinates_df: DataFrame containing node coordinates (columns: 'node', 'lat', 'lon')\n",
    "    - save_path: path to save HTML file\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Creating interactive network map for threshold: {threshold}\")\n",
    "    \n",
    "    # Filter edges based on threshold\n",
    "    filtered_edges = [(u, v, data) for u, v, data in G.edges(data=True) \n",
    "                     if data.get('weight', 0) >= threshold]\n",
    "    \n",
    "    if len(filtered_edges) == 0:\n",
    "        print(\"No edges meet the threshold requirement\")\n",
    "        return None\n",
    "    \n",
    "    # Create filtered subgraph\n",
    "    filtered_G = nx.Graph()\n",
    "    filtered_G.add_edges_from([(u, v) for u, v, _ in filtered_edges])\n",
    "    \n",
    "    # Calculate connected components\n",
    "    components = list(nx.connected_components(filtered_G))\n",
    "    if components:\n",
    "        largest_component = max(components, key=len)\n",
    "        largest_component_size = len(largest_component)\n",
    "    else:\n",
    "        largest_component = set()\n",
    "        largest_component_size = 0\n",
    "    \n",
    "    print(f\"  Filtered network: {filtered_G.number_of_nodes()} nodes, {filtered_G.number_of_edges()} edges\")\n",
    "    print(f\"  Largest component size: {largest_component_size}\")\n",
    "    \n",
    "    # Prepare node data\n",
    "    nodes_in_network = set(filtered_G.nodes())\n",
    "    node_data = []\n",
    "    \n",
    "    for node in nodes_in_network:\n",
    "        # Get coordinates\n",
    "        node_info = coordinates_df[coordinates_df['node'] == node]\n",
    "        if node_info.empty:\n",
    "            continue\n",
    "            \n",
    "        lat = node_info['lat'].iloc[0]\n",
    "        lon = node_info['lon'].iloc[0]\n",
    "        \n",
    "        # Determine node properties\n",
    "        is_in_giant = node in largest_component\n",
    "        degree = filtered_G.degree(node)\n",
    "        \n",
    "        node_data.append({\n",
    "            'node': node,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'degree': degree,\n",
    "            'in_giant_component': is_in_giant,\n",
    "            'component_size': largest_component_size if is_in_giant else 0\n",
    "        })\n",
    "    \n",
    "    if not node_data:\n",
    "        print(\"No valid node coordinate data found\")\n",
    "        return None\n",
    "    \n",
    "    nodes_df = pd.DataFrame(node_data)\n",
    "    \n",
    "    # Prepare edge data\n",
    "    edge_data = []\n",
    "    for u, v, data in filtered_edges:\n",
    "        # Get coordinates for both nodes\n",
    "        u_info = coordinates_df[coordinates_df['node'] == u]\n",
    "        v_info = coordinates_df[coordinates_df['node'] == v]\n",
    "        \n",
    "        if u_info.empty or v_info.empty:\n",
    "            continue\n",
    "            \n",
    "        weight = data.get('weight', 0)\n",
    "        \n",
    "        edge_data.append({\n",
    "            'source': u,\n",
    "            'target': v,\n",
    "            'weight': weight,\n",
    "            'source_lat': u_info['lat'].iloc[0],\n",
    "            'source_lon': u_info['lon'].iloc[0],\n",
    "            'target_lat': v_info['lat'].iloc[0],\n",
    "            'target_lon': v_info['lon'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    edges_df = pd.DataFrame(edge_data)\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add edges (as lines)\n",
    "    if not edges_df.empty:\n",
    "        # Create edge traces\n",
    "        edge_trace = []\n",
    "        \n",
    "        for _, edge in edges_df.iterrows():\n",
    "            edge_trace.extend([\n",
    "                edge['source_lon'], edge['target_lon'], None,\n",
    "            ])\n",
    "            edge_trace.extend([\n",
    "                edge['source_lat'], edge['target_lat'], None,\n",
    "            ])\n",
    "        \n",
    "        # Reorganize edge trace data\n",
    "        edge_lons = edge_trace[::3]  # Every third element starting from 0\n",
    "        edge_lats = edge_trace[1::3]  # Every third element starting from 1\n",
    "        \n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lon=edge_lons,\n",
    "            lat=edge_lats,\n",
    "            mode='lines',\n",
    "            line=dict(width=1, color='rgba(100, 100, 100, 0.6)'),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ))\n",
    "    \n",
    "    # Add nodes\n",
    "    # Giant component nodes\n",
    "    giant_nodes = nodes_df[nodes_df['in_giant_component'] == True]\n",
    "    if not giant_nodes.empty:\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lon=giant_nodes['lon'],\n",
    "            lat=giant_nodes['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8 + giant_nodes['degree'] * 0.5,  # Size based on degree\n",
    "                color='red',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            text=giant_nodes['node'],\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Degree: %{marker.size}<br>' +\n",
    "                         'In Giant Component<br>' +\n",
    "                         '<extra></extra>',\n",
    "            name=f'Giant Component ({len(giant_nodes)})',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Other nodes\n",
    "    other_nodes = nodes_df[nodes_df['in_giant_component'] == False]\n",
    "    if not other_nodes.empty:\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lon=other_nodes['lon'],\n",
    "            lat=other_nodes['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=6 + other_nodes['degree'] * 0.3,\n",
    "                color='blue',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            text=other_nodes['node'],\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Degree: %{marker.size}<br>' +\n",
    "                         'Small Component<br>' +\n",
    "                         '<extra></extra>',\n",
    "            name=f'Other Components ({len(other_nodes)})',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Calculate map center\n",
    "    center_lat = nodes_df['lat'].mean()\n",
    "    center_lon = nodes_df['lon'].mean()\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Interactive Network Map - Threshold: {threshold:.6f}<br>' +\n",
    "              f'Giant Component: {largest_component_size} nodes ' +\n",
    "              f'({largest_component_size/len(nodes_df)*100:.1f}%)',\n",
    "        mapbox=dict(\n",
    "            style='open-street-map',\n",
    "            center=dict(lat=center_lat, lon=center_lon),\n",
    "            zoom=6\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        height=700,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    # Save if path specified\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        print(f\"Interactive map saved: {save_path}\")\n",
    "    \n",
    "    # Display\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Load coordinates data\n",
    "print(\"Loading TTWA coordinates data...\")\n",
    "coordinates_df = pd.read_csv('ttwa_coordinates.csv')  # Assume this file exists\n",
    "print(f\"Loaded coordinates for {len(coordinates_df)} TTWAs\")\n",
    "\n",
    "# Display a few sample coordinates\n",
    "print(\"\\nSample coordinates:\")\n",
    "print(coordinates_df.head())\n",
    "\n",
    "# Test with several critical thresholds\n",
    "test_thresholds = [0.274173, 0.217115, 0.149683, 0.113373]\n",
    "\n",
    "print(f\"\\nGenerating interactive maps for {len(test_thresholds)} thresholds...\")\n",
    "\n",
    "for i, threshold in enumerate(test_thresholds):\n",
    "    print(f\"\\n=== Map {i+1}/{len(test_thresholds)} ===\")\n",
    "    save_filename = f\"interactive_network_map_threshold_{threshold:.6f}.html\"\n",
    "    \n",
    "    create_interactive_network_map(\n",
    "        G=G,\n",
    "        threshold=threshold,\n",
    "        coordinates_df=coordinates_df,\n",
    "        save_path=save_filename\n",
    "    )\n",
    "\n",
    "print(\"\\nAll interactive maps generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a57c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aesthetic network visualization function - suitable for static display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_aesthetic_network_map(G, threshold, node_positions, save_path=None):\n",
    "    \"\"\"\n",
    "    Create aesthetic network visualization using matplotlib\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - threshold: percolation threshold\n",
    "    - node_positions: dictionary, {node: (x, y)}\n",
    "    - save_path: save path\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Creating aesthetic network map for threshold: {threshold}\")\n",
    "    \n",
    "    # Filter network\n",
    "    filtered_edges = [(u, v) for u, v, data in G.edges(data=True) \n",
    "                     if data.get('weight', 0) >= threshold]\n",
    "    \n",
    "    if len(filtered_edges) == 0:\n",
    "        print(\"No edges meet threshold requirement\")\n",
    "        return\n",
    "    \n",
    "    filtered_G = nx.Graph()\n",
    "    filtered_G.add_edges_from(filtered_edges)\n",
    "    \n",
    "    # Calculate connected components\n",
    "    components = list(nx.connected_components(filtered_G))\n",
    "    largest_component = max(components, key=len) if components else set()\n",
    "    \n",
    "    print(f\"  Network: {filtered_G.number_of_nodes()} nodes, {filtered_G.number_of_edges()} edges\")\n",
    "    print(f\"  Giant component: {len(largest_component)} nodes\")\n",
    "    \n",
    "    # Create figure\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    # Filter node positions (only include nodes in filtered network)\n",
    "    filtered_positions = {node: pos for node, pos in node_positions.items() \n",
    "                         if node in filtered_G.nodes()}\n",
    "    \n",
    "    if not filtered_positions:\n",
    "        print(\"No valid node positions found\")\n",
    "        return\n",
    "    \n",
    "    # Draw edges with transparency gradient\n",
    "    edge_colors = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for u, v in filtered_G.edges():\n",
    "        if u in filtered_positions and v in filtered_positions:\n",
    "            # Get edge weight\n",
    "            weight = G[u][v].get('weight', 0) if G.has_edge(u, v) else 0\n",
    "            edge_weights.append(weight)\n",
    "            \n",
    "            # Color based on whether connected to giant component\n",
    "            if u in largest_component and v in largest_component:\n",
    "                edge_colors.append('#FF6B6B')  # Red - giant component internal\n",
    "            elif u in largest_component or v in largest_component:\n",
    "                edge_colors.append('#4ECDC4')  # Teal - connected to giant component\n",
    "            else:\n",
    "                edge_colors.append('#95A5A6')  # Gray - small components\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(filtered_G, filtered_positions,\n",
    "                          edge_color=edge_colors,\n",
    "                          alpha=0.6,\n",
    "                          width=0.5,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Draw nodes\n",
    "    # Giant component nodes\n",
    "    giant_nodes = [node for node in filtered_G.nodes() if node in largest_component]\n",
    "    if giant_nodes:\n",
    "        giant_pos = {node: filtered_positions[node] for node in giant_nodes if node in filtered_positions}\n",
    "        nx.draw_networkx_nodes(filtered_G.subgraph(giant_nodes), giant_pos,\n",
    "                             node_color='#FF6B6B',\n",
    "                             node_size=50,\n",
    "                             alpha=0.9,\n",
    "                             ax=ax)\n",
    "    \n",
    "    # Other nodes\n",
    "    other_nodes = [node for node in filtered_G.nodes() if node not in largest_component]\n",
    "    if other_nodes:\n",
    "        other_pos = {node: filtered_positions[node] for node in other_nodes if node in filtered_positions}\n",
    "        nx.draw_networkx_nodes(filtered_G.subgraph(other_nodes), other_pos,\n",
    "                             node_color='#3498DB',\n",
    "                             node_size=30,\n",
    "                             alpha=0.7,\n",
    "                             ax=ax)\n",
    "    \n",
    "    # Set title and formatting\n",
    "    ax.set_title(f'Network Structure at Threshold τ = {threshold:.6f}\\n'\n",
    "                f'Giant Component: {len(largest_component)} nodes '\n",
    "                f'({len(largest_component)/filtered_G.number_of_nodes()*100:.1f}%)',\n",
    "                color='white', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF6B6B', \n",
    "                  markersize=10, label='Giant Component', linestyle='None'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#3498DB', \n",
    "                  markersize=8, label='Small Components', linestyle='None'),\n",
    "        plt.Line2D([0], [0], color='#FF6B6B', linewidth=2, label='Giant Component Links'),\n",
    "        plt.Line2D([0], [0], color='#4ECDC4', linewidth=2, label='Mixed Links'),\n",
    "        plt.Line2D([0], [0], color='#95A5A6', linewidth=2, label='Small Component Links')\n",
    "    ]\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper right', \n",
    "             frameon=True, facecolor='black', edgecolor='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', \n",
    "                   facecolor='black', edgecolor='none')\n",
    "        print(f\"Aesthetic map saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# If you have node position data, use it; otherwise generate positions\n",
    "print(\"Generating/loading node positions...\")\n",
    "\n",
    "try:\n",
    "    # Try to load pre-computed positions\n",
    "    node_positions = np.load('ttwa_positions.npy', allow_pickle=True).item()\n",
    "    print(f\"Loaded pre-computed positions for {len(node_positions)} nodes\")\n",
    "except:\n",
    "    # Generate positions using spring layout (may take time for large networks)\n",
    "    print(\"Generating new positions using spring layout...\")\n",
    "    print(\"This may take a few minutes for large networks...\")\n",
    "    \n",
    "    # Use a subset for position calculation if network is too large\n",
    "    if G.number_of_nodes() > 500:\n",
    "        print(\"Network is large, using sample for position calculation...\")\n",
    "        # Take a sample subgraph\n",
    "        sample_nodes = list(G.nodes())[:500]\n",
    "        sample_G = G.subgraph(sample_nodes)\n",
    "        sample_positions = nx.spring_layout(sample_G, k=1, iterations=50)\n",
    "        \n",
    "        # Extend to full network (simplified approach)\n",
    "        node_positions = {}\n",
    "        for node in G.nodes():\n",
    "            if node in sample_positions:\n",
    "                node_positions[node] = sample_positions[node]\n",
    "            else:\n",
    "                # Random position for unsampled nodes\n",
    "                node_positions[node] = (np.random.random(), np.random.random())\n",
    "    else:\n",
    "        node_positions = nx.spring_layout(G, k=1, iterations=50)\n",
    "    \n",
    "    # Save positions for future use\n",
    "    np.save('ttwa_positions.npy', node_positions)\n",
    "    print(f\"Generated and saved positions for {len(node_positions)} nodes\")\n",
    "\n",
    "print(f\"Node positions ready: {len(node_positions)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate aesthetic network maps for multiple thresholds\n",
    "\n",
    "# Select a few representative thresholds\n",
    "aesthetic_thresholds = [\n",
    "    0.274173,  # Highest threshold\n",
    "    0.217115,  # Medium-high threshold  \n",
    "    0.149683,  # Medium threshold\n",
    "    0.113373,  # Lower threshold\n",
    "    0.051128   # Lowest threshold\n",
    "]\n",
    "\n",
    "print(f\"Generating aesthetic network maps for {len(aesthetic_thresholds)} thresholds...\")\n",
    "print(\"This process may take several minutes...\")\n",
    "\n",
    "for i, threshold in enumerate(aesthetic_thresholds):\n",
    "    print(f\"\\n=== Generating map {i+1}/{len(aesthetic_thresholds)} ===\")\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    save_filename = f\"aesthetic_map_threshold_{threshold:.6f}.png\"\n",
    "    \n",
    "    plot_aesthetic_network_map(\n",
    "        G=G,\n",
    "        threshold=threshold,\n",
    "        node_positions=node_positions,\n",
    "        save_path=save_filename\n",
    "    )\n",
    "    \n",
    "    print(f\"Map {i+1} completed\")\n",
    "\n",
    "print(f\"\\nAll {len(aesthetic_thresholds)} aesthetic maps generated!\")\n",
    "print(\"Files saved with prefix 'aesthetic_map_threshold_'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfc7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced percolation analysis - identifying transition points and calculating critical exponents\n",
    "\n",
    "def advanced_percolation_analysis(thresholds, giant_sizes, save_results=True):\n",
    "    \"\"\"\n",
    "    Advanced percolation analysis including:\n",
    "    1. Transition point identification\n",
    "    2. Critical exponent calculation\n",
    "    3. Scaling behavior analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - thresholds: array of threshold values\n",
    "    - giant_sizes: array of giant component sizes\n",
    "    - save_results: whether to save analysis results\n",
    "    \n",
    "    Returns:\n",
    "    - results: dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting advanced percolation analysis...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Smooth the data\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    smoothed_sizes = gaussian_filter1d(giant_sizes, sigma=2)\n",
    "    \n",
    "    # 2. Calculate derivatives\n",
    "    log_thresholds = np.log10(thresholds)\n",
    "    \n",
    "    # First derivative\n",
    "    d_log_thresh = np.diff(log_thresholds)\n",
    "    d_sizes = np.diff(smoothed_sizes)\n",
    "    first_derivative = d_sizes / d_log_thresh\n",
    "    \n",
    "    # Second derivative\n",
    "    d2_sizes = np.diff(first_derivative)\n",
    "    d2_log_thresh = d_log_thresh[:-1]\n",
    "    second_derivative = d2_sizes / d2_log_thresh\n",
    "    \n",
    "    # 3. Identify critical region\n",
    "    # Find the point of maximum decline (steepest descent)\n",
    "    max_decline_idx = np.argmin(first_derivative)\n",
    "    critical_threshold = thresholds[max_decline_idx]\n",
    "    critical_size = giant_sizes[max_decline_idx]\n",
    "    \n",
    "    print(f\"Critical threshold (max decline): τc = {critical_threshold:.6f}\")\n",
    "    print(f\"Giant component size at critical point: {critical_size:.3f}\")\n",
    "    \n",
    "    # 4. Analyze scaling behavior near critical point\n",
    "    # Look at behavior within ±20% of critical threshold\n",
    "    window_size = critical_threshold * 0.2\n",
    "    lower_bound = critical_threshold - window_size\n",
    "    upper_bound = critical_threshold + window_size\n",
    "    \n",
    "    critical_region_mask = (thresholds >= lower_bound) & (thresholds <= upper_bound)\n",
    "    critical_thresholds = thresholds[critical_region_mask]\n",
    "    critical_giant_sizes = giant_sizes[critical_region_mask]\n",
    "    \n",
    "    # 5. Fit power law near critical point\n",
    "    # For percolation: S(τ) ∝ (τ - τc)^(-γ) for τ > τc\n",
    "    try:\n",
    "        # Look at supercritical region (τ > τc)\n",
    "        supercritical_mask = critical_thresholds > critical_threshold\n",
    "        if np.sum(supercritical_mask) > 5:  # Need sufficient data points\n",
    "            super_thresh = critical_thresholds[supercritical_mask]\n",
    "            super_sizes = critical_giant_sizes[supercritical_mask]\n",
    "            \n",
    "            # Remove zero sizes\n",
    "            nonzero_mask = super_sizes > 0\n",
    "            super_thresh = super_thresh[nonzero_mask]\n",
    "            super_sizes = super_sizes[nonzero_mask]\n",
    "            \n",
    "            if len(super_thresh) > 3:\n",
    "                # Fit: log(S) = -γ * log(τ - τc) + const\n",
    "                tau_diff = super_thresh - critical_threshold\n",
    "                tau_diff = tau_diff[tau_diff > 0]  # Ensure positive\n",
    "                super_sizes = super_sizes[:len(tau_diff)]\n",
    "                \n",
    "                if len(tau_diff) > 3:\n",
    "                    log_tau_diff = np.log10(tau_diff)\n",
    "                    log_sizes = np.log10(super_sizes)\n",
    "                    \n",
    "                    # Linear regression\n",
    "                    coeffs = np.polyfit(log_tau_diff, log_sizes, 1)\n",
    "                    gamma_exponent = -coeffs[0]  # Negative because of power law form\n",
    "                    \n",
    "                    print(f\"Critical exponent γ = {gamma_exponent:.3f}\")\n",
    "                    results['gamma_exponent'] = gamma_exponent\n",
    "                else:\n",
    "                    print(\"Insufficient data for power law fitting\")\n",
    "                    results['gamma_exponent'] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in power law fitting: {e}\")\n",
    "        results['gamma_exponent'] = None\n",
    "    \n",
    "    # 6. Calculate additional metrics\n",
    "    # Steepness of transition\n",
    "    transition_steepness = np.abs(first_derivative[max_decline_idx])\n",
    "    \n",
    "    # Width of transition region (defined as where first derivative is > 50% of maximum)\n",
    "    steep_threshold = transition_steepness * 0.5\n",
    "    steep_region = np.abs(first_derivative) > steep_threshold\n",
    "    transition_width = len(first_derivative[steep_region])\n",
    "    \n",
    "    print(f\"Transition steepness: {transition_steepness:.4f}\")\n",
    "    print(f\"Transition width: {transition_width} points\")\n",
    "    \n",
    "    # 7. Identify multiple transition points\n",
    "    # Look for local minima in first derivative (indicating rapid changes)\n",
    "    from scipy.signal import find_peaks\n",
    "    \n",
    "    # Find peaks in negative first derivative (valleys in original)\n",
    "    peaks, properties = find_peaks(-first_derivative, height=0.01, distance=10)\n",
    "    \n",
    "    transition_points = []\n",
    "    for peak_idx in peaks:\n",
    "        if peak_idx < len(thresholds):\n",
    "            transition_points.append({\n",
    "                'threshold': thresholds[peak_idx],\n",
    "                'giant_size': giant_sizes[peak_idx],\n",
    "                'strength': np.abs(first_derivative[peak_idx])\n",
    "            })\n",
    "    \n",
    "    # Sort by strength\n",
    "    transition_points.sort(key=lambda x: x['strength'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nIdentified {len(transition_points)} transition points:\")\n",
    "    for i, point in enumerate(transition_points[:5]):  # Show top 5\n",
    "        print(f\"  {i+1}. τ = {point['threshold']:.6f}, \"\n",
    "              f\"S = {point['giant_size']:.3f}, \"\n",
    "              f\"Strength = {point['strength']:.4f}\")\n",
    "    \n",
    "    # 8. Calculate percolation probability\n",
    "    # P(τ) = fraction of realizations where giant component exists\n",
    "    # Here we approximate this as the giant component size itself\n",
    "    percolation_probability = giant_sizes\n",
    "    \n",
    "    # Find percolation threshold (where P crosses 0.5)\n",
    "    p_threshold_idx = np.argmin(np.abs(percolation_probability - 0.5))\n",
    "    p_threshold = thresholds[p_threshold_idx]\n",
    "    \n",
    "    print(f\"\\nPercolation threshold (P=0.5): τp = {p_threshold:.6f}\")\n",
    "    \n",
    "    # Compile results\n",
    "    results.update({\n",
    "        'critical_threshold': critical_threshold,\n",
    "        'critical_size': critical_size,\n",
    "        'percolation_threshold': p_threshold,\n",
    "        'transition_steepness': transition_steepness,\n",
    "        'transition_width': transition_width,\n",
    "        'transition_points': transition_points,\n",
    "        'first_derivative': first_derivative,\n",
    "        'second_derivative': second_derivative,\n",
    "        'smoothed_sizes': smoothed_sizes\n",
    "    })\n",
    "    \n",
    "    # 9. Save results\n",
    "    if save_results:\n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_data = {\n",
    "            'metric': ['Critical Threshold', 'Critical Size', 'Percolation Threshold', \n",
    "                      'Gamma Exponent', 'Transition Steepness', 'Transition Width'],\n",
    "            'value': [critical_threshold, critical_size, p_threshold,\n",
    "                     results.get('gamma_exponent', np.nan), transition_steepness, transition_width]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"advanced_percolation_summary.csv\"\n",
    "        summary_df.to_csv(filename, index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute advanced analysis\n",
    "print(\"Executing advanced percolation analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "advanced_results = advanced_percolation_analysis(\n",
    "    final_analysis_thresholds,\n",
    "    final_analysis_giant_sizes,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Advanced analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize advanced analysis results\n",
    "\n",
    "def plot_advanced_analysis_results(thresholds, giant_sizes, results, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of advanced percolation analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Advanced Percolation Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = {\n",
    "        'main': '#2E86AB',\n",
    "        'critical': '#A23B72', \n",
    "        'derivative': '#F18F01',\n",
    "        'transition': '#4CAF50'\n",
    "    }\n",
    "    \n",
    "    # 1. Main percolation curve with critical points\n",
    "    ax1.semilogx(thresholds, giant_sizes, color=colors['main'], linewidth=2.5, \n",
    "                label='Giant Component Size')\n",
    "    ax1.semilogx(thresholds, results['smoothed_sizes'], '--', color='gray', \n",
    "                alpha=0.7, label='Smoothed')\n",
    "    \n",
    "    # Mark critical threshold\n",
    "    ax1.axvline(results['critical_threshold'], color=colors['critical'], \n",
    "               linestyle='--', alpha=0.8, label=f\"Critical τ = {results['critical_threshold']:.4f}\")\n",
    "    \n",
    "    # Mark percolation threshold\n",
    "    ax1.axvline(results['percolation_threshold'], color=colors['transition'], \n",
    "               linestyle=':', alpha=0.8, label=f\"Percolation τ = {results['percolation_threshold']:.4f}\")\n",
    "    \n",
    "    # Mark transition points\n",
    "    for i, point in enumerate(results['transition_points'][:3]):\n",
    "        ax1.scatter(point['threshold'], point['giant_size'], \n",
    "                   s=100, color=colors['derivative'], alpha=0.8, zorder=5)\n",
    "    \n",
    "    ax1.set_xlabel('Threshold τ')\n",
    "    ax1.set_ylabel('Giant Component Size')\n",
    "    ax1.set_title('Percolation Curve with Critical Points')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. First derivative\n",
    "    deriv_thresholds = thresholds[:-1]  # One less point due to differentiation\n",
    "    ax2.semilogx(deriv_thresholds, results['first_derivative'], \n",
    "                color=colors['derivative'], linewidth=2)\n",
    "    ax2.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.axvline(results['critical_threshold'], color=colors['critical'], \n",
    "               linestyle='--', alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Threshold τ')\n",
    "    ax2.set_ylabel('dS/d(log τ)')\n",
    "    ax2.set_title('First Derivative (Rate of Change)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Second derivative\n",
    "    deriv2_thresholds = deriv_thresholds[:-1]  # One less point again\n",
    "    ax3.semilogx(deriv2_thresholds, results['second_derivative'], \n",
    "                color='purple', linewidth=2)\n",
    "    ax3.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax3.axvline(results['critical_threshold'], color=colors['critical'], \n",
    "               linestyle='--', alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Threshold τ')\n",
    "    ax3.set_ylabel('d²S/d(log τ)²')\n",
    "    ax3.set_title('Second Derivative (Curvature)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Transition analysis\n",
    "    # Plot histogram of transition strengths\n",
    "    if results['transition_points']:\n",
    "        strengths = [p['strength'] for p in results['transition_points']]\n",
    "        ax4.hist(strengths, bins=20, color=colors['transition'], alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(np.mean(strengths), color='red', linestyle='--', \n",
    "                   label=f'Mean = {np.mean(strengths):.3f}')\n",
    "        ax4.set_xlabel('Transition Strength')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Distribution of Transition Strengths')\n",
    "        ax4.legend()\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No transition points\\nidentified', \n",
    "                transform=ax4.transAxes, ha='center', va='center', fontsize=12)\n",
    "        ax4.set_title('Transition Analysis')\n",
    "    \n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Advanced analysis plot saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAdvanced Analysis Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Critical Threshold (max decline): {results['critical_threshold']:.6f}\")\n",
    "    print(f\"Critical Size: {results['critical_size']:.3f}\")\n",
    "    print(f\"Percolation Threshold (50%): {results['percolation_threshold']:.6f}\")\n",
    "    \n",
    "    if results.get('gamma_exponent'):\n",
    "        print(f\"Critical Exponent γ: {results['gamma_exponent']:.3f}\")\n",
    "    else:\n",
    "        print(\"Critical Exponent γ: Could not calculate\")\n",
    "    \n",
    "    print(f\"Transition Steepness: {results['transition_steepness']:.4f}\")\n",
    "    print(f\"Transition Width: {results['transition_width']} points\")\n",
    "    print(f\"Number of Transition Points: {len(results['transition_points'])}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Generate advanced analysis visualization\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "advanced_plot_filename = f\"advanced_percolation_analysis_{timestamp}.png\"\n",
    "\n",
    "plot_advanced_analysis_results(\n",
    "    final_analysis_thresholds,\n",
    "    final_analysis_giant_sizes,\n",
    "    advanced_results,\n",
    "    save_path=advanced_plot_filename\n",
    ")\n",
    "\n",
    "print(f\"\\nAdvanced analysis visualization completed!\")\n",
    "print(f\"Plot saved as: {advanced_plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hierarchy analysis - identifying core and peripheral regions\n",
    "\n",
    "def analyze_network_hierarchy(G, critical_thresholds, save_results=True):\n",
    "    \"\"\"\n",
    "    Analyze network hierarchy at different percolation thresholds\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - critical_thresholds: list of critical threshold values\n",
    "    - save_results: whether to save results\n",
    "    \n",
    "    Returns:\n",
    "    - hierarchy_results: dictionary containing hierarchy analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting network hierarchy analysis...\")\n",
    "    print(f\"Analyzing {len(critical_thresholds)} critical thresholds\")\n",
    "    \n",
    "    hierarchy_results = {}\n",
    "    \n",
    "    for i, threshold in enumerate(critical_thresholds):\n",
    "        print(f\"\\nAnalyzing threshold {i+1}/{len(critical_thresholds)}: τ = {threshold:.6f}\")\n",
    "        \n",
    "        # Create filtered network\n",
    "        filtered_edges = [(u, v, data) for u, v, data in G.edges(data=True) \n",
    "                         if data.get('weight', 0) >= threshold]\n",
    "        \n",
    "        if len(filtered_edges) == 0:\n",
    "            print(\"  No edges above threshold\")\n",
    "            continue\n",
    "        \n",
    "        # Create subgraph\n",
    "        subgraph = nx.Graph()\n",
    "        subgraph.add_edges_from([(u, v) for u, v, _ in filtered_edges])\n",
    "        \n",
    "        # Connected components analysis\n",
    "        components = list(nx.connected_components(subgraph))\n",
    "        \n",
    "        if not components:\n",
    "            print(\"  No connected components\")\n",
    "            continue\n",
    "        \n",
    "        # Identify giant component\n",
    "        giant_component = max(components, key=len)\n",
    "        component_sizes = [len(comp) for comp in components]\n",
    "        \n",
    "        # Network metrics for giant component\n",
    "        giant_subgraph = subgraph.subgraph(giant_component)\n",
    "        \n",
    "        # Basic metrics\n",
    "        num_nodes = len(giant_component)\n",
    "        num_edges = giant_subgraph.number_of_edges()\n",
    "        density = nx.density(giant_subgraph) if num_nodes > 1 else 0\n",
    "        \n",
    "        # Centrality measures for hierarchy identification\n",
    "        print(\"    Computing centrality measures...\")\n",
    "        \n",
    "        # Degree centrality\n",
    "        degree_centrality = nx.degree_centrality(giant_subgraph)\n",
    "        \n",
    "        # Betweenness centrality (sampling for large networks)\n",
    "        if num_nodes > 100:\n",
    "            # Sample nodes for computational efficiency\n",
    "            sample_size = min(100, num_nodes)\n",
    "            sample_nodes = np.random.choice(list(giant_component), sample_size, replace=False)\n",
    "            betweenness = nx.betweenness_centrality(giant_subgraph, k=sample_size)\n",
    "        else:\n",
    "            betweenness = nx.betweenness_centrality(giant_subgraph)\n",
    "        \n",
    "        # Closeness centrality (sample for large networks)\n",
    "        if num_nodes > 100:\n",
    "            closeness = nx.closeness_centrality(giant_subgraph, distance='weight')\n",
    "        else:\n",
    "            closeness = nx.closeness_centrality(giant_subgraph)\n",
    "        \n",
    "        # Identify core nodes (high centrality across multiple measures)\n",
    "        # Normalize centrality scores\n",
    "        deg_scores = np.array(list(degree_centrality.values()))\n",
    "        bet_scores = np.array(list(betweenness.values()))\n",
    "        clo_scores = np.array(list(closeness.values()))\n",
    "        \n",
    "        # Calculate composite centrality score\n",
    "        composite_centrality = {}\n",
    "        for node in giant_component:\n",
    "            deg_score = degree_centrality.get(node, 0)\n",
    "            bet_score = betweenness.get(node, 0)\n",
    "            clo_score = closeness.get(node, 0)\n",
    "            \n",
    "            # Weighted combination (adjust weights as needed)\n",
    "            composite_centrality[node] = 0.4 * deg_score + 0.3 * bet_score + 0.3 * clo_score\n",
    "        \n",
    "        # Identify core (top 20%), periphery (bottom 20%), and semi-periphery (middle 60%)\n",
    "        centrality_values = list(composite_centrality.values())\n",
    "        core_threshold = np.percentile(centrality_values, 80)\n",
    "        periphery_threshold = np.percentile(centrality_values, 20)\n",
    "        \n",
    "        core_nodes = [node for node, score in composite_centrality.items() \n",
    "                     if score >= core_threshold]\n",
    "        periphery_nodes = [node for node, score in composite_centrality.items() \n",
    "                          if score <= periphery_threshold]\n",
    "        semi_periphery_nodes = [node for node in giant_component \n",
    "                               if node not in core_nodes and node not in periphery_nodes]\n",
    "        \n",
    "        print(f\"    Core nodes: {len(core_nodes)} ({len(core_nodes)/num_nodes*100:.1f}%)\")\n",
    "        print(f\"    Semi-periphery: {len(semi_periphery_nodes)} ({len(semi_periphery_nodes)/num_nodes*100:.1f}%)\")\n",
    "        print(f\"    Periphery: {len(periphery_nodes)} ({len(periphery_nodes)/num_nodes*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate hierarchy metrics\n",
    "        # Core connectivity\n",
    "        core_subgraph = giant_subgraph.subgraph(core_nodes)\n",
    "        core_density = nx.density(core_subgraph) if len(core_nodes) > 1 else 0\n",
    "        \n",
    "        # Core-periphery connections\n",
    "        core_periphery_edges = 0\n",
    "        for u, v in giant_subgraph.edges():\n",
    "            if (u in core_nodes and v in periphery_nodes) or (u in periphery_nodes and v in core_nodes):\n",
    "                core_periphery_edges += 1\n",
    "        \n",
    "        # Store results for this threshold\n",
    "        threshold_results = {\n",
    "            'threshold': threshold,\n",
    "            'giant_component_size': num_nodes,\n",
    "            'giant_component_edges': num_edges,\n",
    "            'network_density': density,\n",
    "            'num_components': len(components),\n",
    "            'component_sizes': component_sizes,\n",
    "            'core_nodes': core_nodes,\n",
    "            'semi_periphery_nodes': semi_periphery_nodes,\n",
    "            'periphery_nodes': periphery_nodes,\n",
    "            'core_density': core_density,\n",
    "            'core_periphery_edges': core_periphery_edges,\n",
    "            'centrality_scores': composite_centrality\n",
    "        }\n",
    "        \n",
    "        hierarchy_results[threshold] = threshold_results\n",
    "    \n",
    "    print(f\"\\nHierarchy analysis completed for {len(hierarchy_results)} thresholds\")\n",
    "    \n",
    "    # Save results\n",
    "    if save_results:\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_data = []\n",
    "        for threshold, results in hierarchy_results.items():\n",
    "            summary_data.append({\n",
    "                'threshold': threshold,\n",
    "                'giant_size': results['giant_component_size'],\n",
    "                'density': results['network_density'],\n",
    "                'num_components': results['num_components'],\n",
    "                'core_nodes': len(results['core_nodes']),\n",
    "                'core_density': results['core_density'],\n",
    "                'core_periphery_edges': results['core_periphery_edges']\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv('network_hierarchy_analysis.csv', index=False)\n",
    "        print(\"Hierarchy analysis saved to: network_hierarchy_analysis.csv\")\n",
    "    \n",
    "    return hierarchy_results\n",
    "\n",
    "# Execute hierarchy analysis using our critical thresholds\n",
    "critical_thresholds_for_hierarchy = [\n",
    "    0.274173, 0.243051, 0.217115, 0.175618, 0.149683\n",
    "]\n",
    "\n",
    "print(\"Executing network hierarchy analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hierarchy_results = analyze_network_hierarchy(\n",
    "    G, \n",
    "    critical_thresholds_for_hierarchy,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Network hierarchy analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cead96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network hierarchy evolution\n",
    "\n",
    "def plot_hierarchy_evolution(hierarchy_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize how network hierarchy changes across thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hierarchy_results:\n",
    "        print(\"No hierarchy results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    thresholds = []\n",
    "    giant_sizes = []\n",
    "    core_proportions = []\n",
    "    densities = []\n",
    "    num_components = []\n",
    "    core_densities = []\n",
    "    \n",
    "    for threshold in sorted(hierarchy_results.keys()):\n",
    "        results = hierarchy_results[threshold]\n",
    "        \n",
    "        thresholds.append(threshold)\n",
    "        giant_sizes.append(results['giant_component_size'])\n",
    "        \n",
    "        total_giant = results['giant_component_size']\n",
    "        core_size = len(results['core_nodes'])\n",
    "        core_proportions.append(core_size / total_giant if total_giant > 0 else 0)\n",
    "        \n",
    "        densities.append(results['network_density'])\n",
    "        num_components.append(results['num_components'])\n",
    "        core_densities.append(results['core_density'])\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Network Hierarchy Evolution Across Thresholds', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = {\n",
    "        'giant': '#2E86AB',\n",
    "        'core': '#A23B72',\n",
    "        'density': '#F18F01',\n",
    "        'components': '#4CAF50'\n",
    "    }\n",
    "    \n",
    "    # 1. Giant component size evolution\n",
    "    ax1.semilogx(thresholds, giant_sizes, 'o-', color=colors['giant'], \n",
    "                linewidth=2.5, markersize=8, label='Giant Component Size')\n",
    "    ax1.set_xlabel('Threshold τ')\n",
    "    ax1.set_ylabel('Number of Nodes')\n",
    "    ax1.set_title('Giant Component Size Evolution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Core proportion evolution\n",
    "    ax2.semilogx(thresholds, [p*100 for p in core_proportions], 's-', \n",
    "                color=colors['core'], linewidth=2.5, markersize=8, label='Core Proportion')\n",
    "    ax2.set_xlabel('Threshold τ')\n",
    "    ax2.set_ylabel('Core Nodes (%)')\n",
    "    ax2.set_title('Core Proportion in Giant Component')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Network density comparison\n",
    "    ax3.semilogx(thresholds, densities, '^-', color=colors['density'], \n",
    "                linewidth=2.5, markersize=8, label='Network Density')\n",
    "    ax3.semilogx(thresholds, core_densities, 'v-', color=colors['core'], \n",
    "                linewidth=2.5, markersize=8, label='Core Density')\n",
    "    ax3.set_xlabel('Threshold τ')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Network vs Core Density')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Number of components\n",
    "    ax4.semilogx(thresholds, num_components, 'D-', color=colors['components'], \n",
    "                linewidth=2.5, markersize=8, label='Number of Components')\n",
    "    ax4.set_xlabel('Threshold τ')\n",
    "    ax4.set_ylabel('Component Count')\n",
    "    ax4.set_title('Network Fragmentation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Hierarchy evolution plot saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print evolution summary\n",
    "    print(\"\\nHierarchy Evolution Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Threshold':<12} {'Giant Size':<12} {'Core %':<10} {'Density':<10} {'Core Density':<12} {'Components':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, threshold in enumerate(sorted(thresholds)):\n",
    "        print(f\"{threshold:<12.6f} {giant_sizes[i]:<12d} {core_proportions[i]*100:<10.1f} \"\n",
    "              f\"{densities[i]:<10.3f} {core_densities[i]:<12.3f} {num_components[i]:<10d}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Generate hierarchy evolution visualization\n",
    "hierarchy_plot_filename = \"network_hierarchy_evolution.png\"\n",
    "\n",
    "plot_hierarchy_evolution(\n",
    "    hierarchy_results,\n",
    "    save_path=hierarchy_plot_filename\n",
    ")\n",
    "\n",
    "print(f\"\\nHierarchy evolution visualization completed!\")\n",
    "print(f\"Plot saved as: {hierarchy_plot_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9fa50",
   "metadata": {},
   "source": [
    "#### Dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive result export function\n",
    "\n",
    "def export_comprehensive_results(analysis_results, hierarchy_results, \n",
    "                                advanced_results, export_dir=\"percolation_results\"):\n",
    "    \"\"\"\n",
    "    Export all analysis results in multiple formats\n",
    "    \n",
    "    Parameters:\n",
    "    - analysis_results: main percolation analysis results\n",
    "    - hierarchy_results: network hierarchy analysis results  \n",
    "    - advanced_results: advanced percolation analysis results\n",
    "    - export_dir: directory to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create export directory\n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir)\n",
    "        print(f\"Created export directory: {export_dir}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"Exporting comprehensive analysis results...\")\n",
    "    \n",
    "    # 1. Export main percolation data\n",
    "    print(\"  Exporting main percolation analysis...\")\n",
    "    \n",
    "    # Percolation curve data\n",
    "    percolation_df = pd.DataFrame({\n",
    "        'threshold': analysis_results['thresholds'],\n",
    "        'giant_component_size': analysis_results['giant_sizes']\n",
    "    })\n",
    "    percolation_df.to_csv(f\"{export_dir}/percolation_curve_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Critical points\n",
    "    critical_points_data = []\n",
    "    for point in analysis_results['critical_points']:\n",
    "        critical_points_data.append({\n",
    "            'threshold': point['threshold'],\n",
    "            'giant_size': point['giant_size'],\n",
    "            'type': point['type'],\n",
    "            'reference_threshold': point.get('reference', ''),\n",
    "            'error': point.get('error', ''),\n",
    "            'strength': point.get('strength', ''),\n",
    "            'target_fraction': point.get('target_fraction', '')\n",
    "        })\n",
    "    \n",
    "    critical_df = pd.DataFrame(critical_points_data)\n",
    "    critical_df.to_csv(f\"{export_dir}/critical_points_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # 2. Export hierarchy analysis\n",
    "    print(\"  Exporting hierarchy analysis...\")\n",
    "    \n",
    "    hierarchy_summary = []\n",
    "    for threshold, results in hierarchy_results.items():\n",
    "        hierarchy_summary.append({\n",
    "            'threshold': threshold,\n",
    "            'giant_component_size': results['giant_component_size'],\n",
    "            'giant_component_edges': results['giant_component_edges'],\n",
    "            'network_density': results['network_density'],\n",
    "            'num_components': results['num_components'],\n",
    "            'core_nodes_count': len(results['core_nodes']),\n",
    "            'semi_periphery_count': len(results['semi_periphery_nodes']),\n",
    "            'periphery_count': len(results['periphery_nodes']),\n",
    "            'core_density': results['core_density'],\n",
    "            'core_periphery_edges': results['core_periphery_edges']\n",
    "        })\n",
    "    \n",
    "    hierarchy_df = pd.DataFrame(hierarchy_summary)\n",
    "    hierarchy_df.to_csv(f\"{export_dir}/hierarchy_analysis_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Export detailed centrality scores for each threshold\n",
    "    for threshold, results in hierarchy_results.items():\n",
    "        centrality_data = []\n",
    "        for node, score in results['centrality_scores'].items():\n",
    "            node_type = 'core' if node in results['core_nodes'] else \\\n",
    "                       'semi_periphery' if node in results['semi_periphery_nodes'] else 'periphery'\n",
    "            \n",
    "            centrality_data.append({\n",
    "                'node': node,\n",
    "                'centrality_score': score,\n",
    "                'node_type': node_type\n",
    "            })\n",
    "        \n",
    "        centrality_df = pd.DataFrame(centrality_data)\n",
    "        safe_threshold = str(threshold).replace('.', '_')\n",
    "        centrality_df.to_csv(f\"{export_dir}/centrality_threshold_{safe_threshold}_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # 3. Export advanced analysis results\n",
    "    print(\"  Exporting advanced analysis...\")\n",
    "    \n",
    "    advanced_summary = {\n",
    "        'critical_threshold': advanced_results['critical_threshold'],\n",
    "        'critical_size': advanced_results['critical_size'],\n",
    "        'percolation_threshold': advanced_results['percolation_threshold'],\n",
    "        'gamma_exponent': advanced_results.get('gamma_exponent'),\n",
    "        'transition_steepness': advanced_results['transition_steepness'],\n",
    "        'transition_width': advanced_results['transition_width'],\n",
    "        'num_transition_points': len(advanced_results['transition_points'])\n",
    "    }\n",
    "    \n",
    "    advanced_df = pd.DataFrame([advanced_summary])\n",
    "    advanced_df.to_csv(f\"{export_dir}/advanced_analysis_summary_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Export transition points\n",
    "    transition_data = []\n",
    "    for point in advanced_results['transition_points']:\n",
    "        transition_data.append({\n",
    "            'threshold': point['threshold'],\n",
    "            'giant_size': point['giant_size'],\n",
    "            'strength': point['strength']\n",
    "        })\n",
    "    \n",
    "    if transition_data:\n",
    "        transition_df = pd.DataFrame(transition_data)\n",
    "        transition_df.to_csv(f\"{export_dir}/transition_points_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # Export derivatives\n",
    "    if advanced_results.get('first_derivative') is not None:\n",
    "        deriv_df = pd.DataFrame({\n",
    "            'threshold': analysis_results['thresholds'][:-1],  # One less due to differentiation\n",
    "            'first_derivative': advanced_results['first_derivative']\n",
    "        })\n",
    "        deriv_df.to_csv(f\"{export_dir}/first_derivative_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    if advanced_results.get('second_derivative') is not None:\n",
    "        second_deriv_df = pd.DataFrame({\n",
    "            'threshold': analysis_results['thresholds'][:-2],  # Two less due to second differentiation\n",
    "            'second_derivative': advanced_results['second_derivative']\n",
    "        })\n",
    "        second_deriv_df.to_csv(f\"{export_dir}/second_derivative_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # 4. Export network information\n",
    "    print(\"  Exporting network information...\")\n",
    "    \n",
    "    network_info = {\n",
    "        'total_nodes': analysis_results['network_info']['nodes'],\n",
    "        'total_edges': analysis_results['network_info']['edges'],\n",
    "        'weight_range_min': analysis_results['network_info']['weight_range'][0],\n",
    "        'weight_range_max': analysis_results['network_info']['weight_range'][1],\n",
    "        'analysis_timestamp': timestamp,\n",
    "        'num_sampling_points': len(analysis_results['thresholds']),\n",
    "        'num_critical_points': len(analysis_results['critical_points'])\n",
    "    }\n",
    "    \n",
    "    network_df = pd.DataFrame([network_info])\n",
    "    network_df.to_csv(f\"{export_dir}/network_information_{timestamp}.csv\", index=False)\n",
    "    \n",
    "    # 5. Export raw data for reproducibility\n",
    "    print(\"  Exporting raw data...\")\n",
    "    \n",
    "    # Save as JSON for easy reading\n",
    "    export_data = {\n",
    "        'analysis_results': {\n",
    "            'thresholds': analysis_results['thresholds'].tolist() if hasattr(analysis_results['thresholds'], 'tolist') else list(analysis_results['thresholds']),\n",
    "            'giant_sizes': analysis_results['giant_sizes'].tolist() if hasattr(analysis_results['giant_sizes'], 'tolist') else list(analysis_results['giant_sizes']),\n",
    "            'critical_points': analysis_results['critical_points'],\n",
    "            'network_info': analysis_results['network_info']\n",
    "        },\n",
    "        'advanced_results': {\n",
    "            'critical_threshold': advanced_results['critical_threshold'],\n",
    "            'critical_size': advanced_results['critical_size'],\n",
    "            'percolation_threshold': advanced_results['percolation_threshold'],\n",
    "            'gamma_exponent': advanced_results.get('gamma_exponent'),\n",
    "            'transition_steepness': advanced_results['transition_steepness'],\n",
    "            'transition_width': advanced_results['transition_width'],\n",
    "            'transition_points': advanced_results['transition_points']\n",
    "        },\n",
    "        'metadata': {\n",
    "            'export_timestamp': timestamp,\n",
    "            'analysis_type': 'network_percolation',\n",
    "            'software': 'Python NetworkX',\n",
    "            'description': 'Comprehensive percolation analysis of TTWA commuting network'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{export_dir}/complete_analysis_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    # Save as pickle for exact reproduction\n",
    "    with open(f\"{export_dir}/complete_analysis_{timestamp}.pkl\", 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'analysis_results': analysis_results,\n",
    "            'hierarchy_results': hierarchy_results,\n",
    "            'advanced_results': advanced_results\n",
    "        }, f)\n",
    "    \n",
    "    # 6. Create summary report\n",
    "    print(\"  Creating summary report...\")\n",
    "    \n",
    "    report_lines = [\n",
    "        f\"# Percolation Analysis Report\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"\",\n",
    "        f\"## Network Information\",\n",
    "        f\"- Total Nodes: {network_info['total_nodes']}\",\n",
    "        f\"- Total Edges: {network_info['total_edges']}\",\n",
    "        f\"- Weight Range: {network_info['weight_range_min']:.6f} - {network_info['weight_range_max']:.6f}\",\n",
    "        f\"\",\n",
    "        f\"## Main Results\",\n",
    "        f\"- Critical Threshold (max decline): {advanced_results['critical_threshold']:.6f}\",\n",
    "        f\"- Critical Size: {advanced_results['critical_size']:.3f}\",\n",
    "        f\"- Percolation Threshold (50%): {advanced_results['percolation_threshold']:.6f}\",\n",
    "        f\"- Number of Critical Points: {len(analysis_results['critical_points'])}\",\n",
    "        f\"- Number of Transition Points: {len(advanced_results['transition_points'])}\",\n",
    "        f\"\",\n",
    "        f\"## Critical Points\",\n",
    "    ]\n",
    "    \n",
    "    for i, point in enumerate(analysis_results['critical_points'][:10]):  # Top 10\n",
    "        report_lines.append(f\"{i+1:2d}. τ = {point['threshold']:.6f}, Size = {point['giant_size']:.3f}, Type: {point['type']}\")\n",
    "    \n",
    "    report_lines.extend([\n",
    "        f\"\",\n",
    "        f\"## Files Exported\",\n",
    "        f\"- percolation_curve_{timestamp}.csv\",\n",
    "        f\"- critical_points_{timestamp}.csv\", \n",
    "        f\"- hierarchy_analysis_{timestamp}.csv\",\n",
    "        f\"- advanced_analysis_summary_{timestamp}.csv\",\n",
    "        f\"- complete_analysis_{timestamp}.json\",\n",
    "        f\"- complete_analysis_{timestamp}.pkl\"\n",
    "    ])\n",
    "    \n",
    "    with open(f\"{export_dir}/analysis_report_{timestamp}.md\", 'w') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    \n",
    "    print(f\"\\nExport completed!\")\n",
    "    print(f\"All files saved to: {export_dir}/\")\n",
    "    print(f\"Summary report: analysis_report_{timestamp}.md\")\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "# Execute comprehensive export\n",
    "print(\"Executing comprehensive result export...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "export_directory = export_comprehensive_results(\n",
    "    analysis_results=analysis_results,\n",
    "    hierarchy_results=hierarchy_results, \n",
    "    advanced_results=advanced_results\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comprehensive export completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713beadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization and validation\n",
    "\n",
    "def validate_percolation_results(analysis_results, tolerance=0.05):\n",
    "    \"\"\"\n",
    "    Validate percolation analysis results for consistency and accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    - analysis_results: main analysis results\n",
    "    - tolerance: tolerance for validation checks\n",
    "    \n",
    "    Returns:\n",
    "    - validation_report: dictionary containing validation results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Validating percolation analysis results...\")\n",
    "    \n",
    "    validation_report = {\n",
    "        'passed_checks': [],\n",
    "        'failed_checks': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    thresholds = analysis_results['thresholds']\n",
    "    giant_sizes = analysis_results['giant_sizes']\n",
    "    \n",
    "    # Check 1: Monotonicity - giant component size should generally decrease with increasing threshold\n",
    "    print(\"  Check 1: Monotonicity test...\")\n",
    "    \n",
    "    # Allow for some noise in the data\n",
    "    increasing_violations = 0\n",
    "    for i in range(1, len(giant_sizes)):\n",
    "        # Since thresholds are in descending order, giant sizes should be non-decreasing\n",
    "        if giant_sizes[i] > giant_sizes[i-1] + tolerance:\n",
    "            increasing_violations += 1\n",
    "    \n",
    "    violation_rate = increasing_violations / (len(giant_sizes) - 1)\n",
    "    \n",
    "    if violation_rate < 0.1:  # Less than 10% violations\n",
    "        validation_report['passed_checks'].append(f\"Monotonicity: {violation_rate:.2%} violations (acceptable)\")\n",
    "    else:\n",
    "        validation_report['failed_checks'].append(f\"Monotonicity: {violation_rate:.2%} violations (too high)\")\n",
    "    \n",
    "    # Check 2: Boundary conditions\n",
    "    print(\"  Check 2: Boundary conditions...\")\n",
    "    \n",
    "    # At highest threshold, giant component should be small\n",
    "    highest_threshold_size = giant_sizes[0]  # First element (highest threshold)\n",
    "    if highest_threshold_size < 0.3:\n",
    "        validation_report['passed_checks'].append(f\"High threshold boundary: {highest_threshold_size:.3f} < 0.3\")\n",
    "    else:\n",
    "        validation_report['warnings'].append(f\"High threshold boundary: {highest_threshold_size:.3f} > 0.3 (may indicate threshold too low)\")\n",
    "    \n",
    "    # At lowest threshold, giant component should be large\n",
    "    lowest_threshold_size = giant_sizes[-1]  # Last element (lowest threshold)\n",
    "    if lowest_threshold_size > 0.7:\n",
    "        validation_report['passed_checks'].append(f\"Low threshold boundary: {lowest_threshold_size:.3f} > 0.7\")\n",
    "    else:\n",
    "        validation_report['warnings'].append(f\"Low threshold boundary: {lowest_threshold_size:.3f} < 0.7 (may indicate incomplete range)\")\n",
    "    \n",
    "    # Check 3: Critical points validation\n",
    "    print(\"  Check 3: Critical points validation...\")\n",
    "    \n",
    "    critical_points = analysis_results['critical_points']\n",
    "    valid_critical_points = 0\n",
    "    \n",
    "    for point in critical_points:\n",
    "        threshold = point['threshold']\n",
    "        giant_size = point['giant_size']\n",
    "        \n",
    "        # Check if critical point is within reasonable range\n",
    "        if 0.01 <= giant_size <= 0.99:\n",
    "            valid_critical_points += 1\n",
    "    \n",
    "    if len(critical_points) > 0:\n",
    "        valid_ratio = valid_critical_points / len(critical_points)\n",
    "        if valid_ratio > 0.8:\n",
    "            validation_report['passed_checks'].append(f\"Critical points: {valid_ratio:.2%} in valid range\")\n",
    "        else:\n",
    "            validation_report['failed_checks'].append(f\"Critical points: {valid_ratio:.2%} in valid range (too low)\")\n",
    "    \n",
    "    # Check 4: Data quality\n",
    "    print(\"  Check 4: Data quality...\")\n",
    "    \n",
    "    # Check for NaN or infinite values\n",
    "    nan_count = np.sum(np.isnan(giant_sizes))\n",
    "    inf_count = np.sum(np.isinf(giant_sizes))\n",
    "    \n",
    "    if nan_count == 0 and inf_count == 0:\n",
    "        validation_report['passed_checks'].append(\"Data quality: No NaN or infinite values\")\n",
    "    else:\n",
    "        validation_report['failed_checks'].append(f\"Data quality: {nan_count} NaN, {inf_count} infinite values\")\n",
    "    \n",
    "    # Check 5: Threshold range coverage\n",
    "    print(\"  Check 5: Threshold range coverage...\")\n",
    "    \n",
    "    threshold_range = thresholds[-1] / thresholds[0]  # Ratio of min to max\n",
    "    if threshold_range < 0.01:  # Covers at least 2 orders of magnitude\n",
    "        validation_report['passed_checks'].append(f\"Threshold coverage: {threshold_range:.4f} (good dynamic range)\")\n",
    "    else:\n",
    "        validation_report['warnings'].append(f\"Threshold coverage: {threshold_range:.4f} (limited dynamic range)\")\n",
    "    \n",
    "    # Calculate overall validation score\n",
    "    passed = len(validation_report['passed_checks'])\n",
    "    failed = len(validation_report['failed_checks'])\n",
    "    total_checks = passed + failed\n",
    "    \n",
    "    if total_checks > 0:\n",
    "        validation_score = passed / total_checks\n",
    "        validation_report['overall_score'] = validation_score\n",
    "        \n",
    "        if validation_score >= 0.8:\n",
    "            validation_report['overall_status'] = 'PASSED'\n",
    "        elif validation_score >= 0.6:\n",
    "            validation_report['overall_status'] = 'PASSED_WITH_WARNINGS'\n",
    "        else:\n",
    "            validation_report['overall_status'] = 'FAILED'\n",
    "    else:\n",
    "        validation_report['overall_status'] = 'NO_CHECKS'\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "def optimize_threshold_sampling(G, target_points=500, reference_thresholds=None):\n",
    "    \"\"\"\n",
    "    Optimize threshold sampling for better analysis efficiency\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - target_points: target number of sampling points\n",
    "    - reference_thresholds: known critical thresholds to emphasize\n",
    "    \n",
    "    Returns:\n",
    "    - optimized_thresholds: optimized threshold array\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Optimizing threshold sampling for {target_points} points...\")\n",
    "    \n",
    "    # Get edge weight distribution\n",
    "    weights = [data.get('weight', 1.0) for _, _, data in G.edges(data=True)]\n",
    "    min_weight = min(weights)\n",
    "    max_weight = max(weights)\n",
    "    \n",
    "    print(f\"  Weight range: {min_weight:.6f} - {max_weight:.6f}\")\n",
    "    \n",
    "    # Strategy: More points where there's more variation\n",
    "    # Use quantile-based sampling for base distribution\n",
    "    quantiles = np.linspace(0.01, 0.99, target_points // 2)\n",
    "    quantile_thresholds = np.quantile(weights, quantiles)\n",
    "    \n",
    "    # Add logarithmic sampling for broad coverage\n",
    "    log_thresholds = np.logspace(np.log10(min_weight), np.log10(max_weight), target_points // 3)\n",
    "    \n",
    "    # Add reference threshold neighborhoods if provided\n",
    "    reference_neighborhoods = []\n",
    "    if reference_thresholds:\n",
    "        for ref_thresh in reference_thresholds:\n",
    "            if min_weight <= ref_thresh <= max_weight:\n",
    "                # Dense sampling around reference ±10%\n",
    "                local_min = max(min_weight, ref_thresh * 0.9)\n",
    "                local_max = min(max_weight, ref_thresh * 1.1)\n",
    "                local_points = np.linspace(local_min, local_max, 20)\n",
    "                reference_neighborhoods.extend(local_points)\n",
    "    \n",
    "    # Combine all sampling strategies\n",
    "    all_thresholds = np.concatenate([\n",
    "        quantile_thresholds,\n",
    "        log_thresholds,\n",
    "        reference_neighborhoods\n",
    "    ])\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    unique_thresholds = np.unique(all_thresholds)\n",
    "    unique_thresholds = unique_thresholds[(unique_thresholds >= min_weight) & \n",
    "                                         (unique_thresholds <= max_weight)]\n",
    "    \n",
    "    # Sort in descending order (high to low threshold)\n",
    "    optimized_thresholds = np.sort(unique_thresholds)[::-1]\n",
    "    \n",
    "    # Trim to target size if needed\n",
    "    if len(optimized_thresholds) > target_points:\n",
    "        # Keep the most representative points\n",
    "        indices = np.linspace(0, len(optimized_thresholds)-1, target_points, dtype=int)\n",
    "        optimized_thresholds = optimized_thresholds[indices]\n",
    "    \n",
    "    print(f\"  Optimized to {len(optimized_thresholds)} sampling points\")\n",
    "    \n",
    "    return optimized_thresholds\n",
    "\n",
    "# Execute validation\n",
    "print(\"Executing result validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_results = validate_percolation_results(analysis_results)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Overall Status: {validation_results['overall_status']}\")\n",
    "if 'overall_score' in validation_results:\n",
    "    print(f\"Overall Score: {validation_results['overall_score']:.2%}\")\n",
    "\n",
    "print(f\"\\nPassed Checks ({len(validation_results['passed_checks'])}):\")\n",
    "for check in validation_results['passed_checks']:\n",
    "    print(f\"  ✓ {check}\")\n",
    "\n",
    "if validation_results['failed_checks']:\n",
    "    print(f\"\\nFailed Checks ({len(validation_results['failed_checks'])}):\")\n",
    "    for check in validation_results['failed_checks']:\n",
    "        print(f\"  ✗ {check}\")\n",
    "\n",
    "if validation_results['warnings']:\n",
    "    print(f\"\\nWarnings ({len(validation_results['warnings'])}):\")\n",
    "    for warning in validation_results['warnings']:\n",
    "        print(f\"  ⚠ {warning}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0942a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmark and comparison with reference methods\n",
    "\n",
    "def benchmark_percolation_methods(G, num_thresholds=100):\n",
    "    \"\"\"\n",
    "    Benchmark different percolation analysis methods for performance comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    print(\"Running performance benchmark...\")\n",
    "    print(f\"Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    print(f\"Testing with {num_thresholds} threshold points\")\n",
    "    \n",
    "    # Get test thresholds\n",
    "    weights = [data.get('weight', 1.0) for _, _, data in G.edges(data=True)]\n",
    "    min_weight, max_weight = min(weights), max(weights)\n",
    "    test_thresholds = np.logspace(np.log10(min_weight), np.log10(max_weight), num_thresholds)[::-1]\n",
    "    \n",
    "    benchmark_results = {}\n",
    "    \n",
    "    # Method 1: Standard NetworkX approach\n",
    "    print(\"\\n  Method 1: Standard NetworkX\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    standard_results = []\n",
    "    for threshold in test_thresholds:\n",
    "        edges_filtered = [(u, v) for u, v, data in G.edges(data=True) \n",
    "                         if data.get('weight', 0) >= threshold]\n",
    "        subgraph = nx.Graph()\n",
    "        subgraph.add_edges_from(edges_filtered)\n",
    "        \n",
    "        if subgraph.number_of_nodes() > 0:\n",
    "            components = list(nx.connected_components(subgraph))\n",
    "            largest_size = max(len(comp) for comp in components) if components else 0\n",
    "        else:\n",
    "            largest_size = 0\n",
    "        \n",
    "        standard_results.append(largest_size / G.number_of_nodes())\n",
    "    \n",
    "    standard_time = time.time() - start_time\n",
    "    benchmark_results['standard_networkx'] = {\n",
    "        'time': standard_time,\n",
    "        'results': standard_results,\n",
    "        'description': 'Standard NetworkX connected components'\n",
    "    }\n",
    "    \n",
    "    print(f\"    Time: {standard_time:.2f} seconds\")\n",
    "    \n",
    "    # Method 2: Optimized approach with edge filtering\n",
    "    print(\"\\n  Method 2: Optimized Edge Filtering\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-filter and sort edges by weight\n",
    "    weighted_edges = [(u, v, data.get('weight', 0)) for u, v, data in G.edges(data=True)]\n",
    "    weighted_edges.sort(key=lambda x: x[2], reverse=True)  # Sort by weight descending\n",
    "    \n",
    "    optimized_results = []\n",
    "    current_edges = []\n",
    "    edge_index = 0\n",
    "    \n",
    "    for threshold in test_thresholds:\n",
    "        # Add edges that meet current threshold\n",
    "        while edge_index < len(weighted_edges) and weighted_edges[edge_index][2] >= threshold:\n",
    "            current_edges.append((weighted_edges[edge_index][0], weighted_edges[edge_index][1]))\n",
    "            edge_index += 1\n",
    "        \n",
    "        # Create subgraph with current edges\n",
    "        if current_edges:\n",
    "            subgraph = nx.Graph()\n",
    "            subgraph.add_edges_from(current_edges)\n",
    "            components = list(nx.connected_components(subgraph))\n",
    "            largest_size = max(len(comp) for comp in components) if components else 0\n",
    "        else:\n",
    "            largest_size = 0\n",
    "        \n",
    "        optimized_results.append(largest_size / G.number_of_nodes())\n",
    "    \n",
    "    optimized_time = time.time() - start_time\n",
    "    benchmark_results['optimized_filtering'] = {\n",
    "        'time': optimized_time,\n",
    "        'results': optimized_results,\n",
    "        'description': 'Optimized edge filtering with sorting'\n",
    "    }\n",
    "    \n",
    "    print(f\"    Time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"    Speedup: {standard_time/optimized_time:.1f}x\")\n",
    "    \n",
    "    # Method 3: Union-Find approach (for very large networks)\n",
    "    print(\"\\n  Method 3: Union-Find Algorithm\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    class UnionFind:\n",
    "        def __init__(self, n):\n",
    "            self.parent = list(range(n))\n",
    "            self.rank = [0] * n\n",
    "            self.component_size = [1] * n\n",
    "            self.max_component_size = 1\n",
    "        \n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "        \n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return\n",
    "            \n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            \n",
    "            self.parent[py] = px\n",
    "            self.component_size[px] += self.component_size[py]\n",
    "            self.max_component_size = max(self.max_component_size, self.component_size[px])\n",
    "            \n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "    \n",
    "    # Create node mapping\n",
    "    nodes = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    unionfind_results = []\n",
    "    \n",
    "    for threshold in test_thresholds:\n",
    "        uf = UnionFind(len(nodes))\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            if data.get('weight', 0) >= threshold:\n",
    "                uf.union(node_to_idx[u], node_to_idx[v])\n",
    "        \n",
    "        unionfind_results.append(uf.max_component_size / len(nodes))\n",
    "    \n",
    "    unionfind_time = time.time() - start_time\n",
    "    benchmark_results['union_find'] = {\n",
    "        'time': unionfind_time,\n",
    "        'results': unionfind_results,\n",
    "        'description': 'Union-Find algorithm'\n",
    "    }\n",
    "    \n",
    "    print(f\"    Time: {unionfind_time:.2f} seconds\")\n",
    "    print(f\"    Speedup vs standard: {standard_time/unionfind_time:.1f}x\")\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    print(\"\\n  Accuracy Comparison:\")\n",
    "    \n",
    "    # Compare results between methods\n",
    "    standard_arr = np.array(standard_results)\n",
    "    optimized_arr = np.array(optimized_results)\n",
    "    unionfind_arr = np.array(unionfind_results)\n",
    "    \n",
    "    opt_diff = np.mean(np.abs(standard_arr - optimized_arr))\n",
    "    uf_diff = np.mean(np.abs(standard_arr - unionfind_arr))\n",
    "    \n",
    "    print(f\"    Optimized vs Standard: Mean absolute difference = {opt_diff:.6f}\")\n",
    "    print(f\"    Union-Find vs Standard: Mean absolute difference = {uf_diff:.6f}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n  Performance Summary:\")\n",
    "    print(f\"    Standard NetworkX: {standard_time:.2f}s (baseline)\")\n",
    "    print(f\"    Optimized Filtering: {optimized_time:.2f}s ({standard_time/optimized_time:.1f}x faster)\")\n",
    "    print(f\"    Union-Find: {unionfind_time:.2f}s ({standard_time/unionfind_time:.1f}x faster)\")\n",
    "    \n",
    "    # Recommendation\n",
    "    if G.number_of_nodes() > 1000:\n",
    "        recommended = \"Union-Find (large network)\"\n",
    "    elif optimized_time < standard_time * 0.8:\n",
    "        recommended = \"Optimized Filtering (good balance)\"\n",
    "    else:\n",
    "        recommended = \"Standard NetworkX (small network)\"\n",
    "    \n",
    "    print(f\"    Recommended method: {recommended}\")\n",
    "    \n",
    "    benchmark_results['summary'] = {\n",
    "        'network_size': G.number_of_nodes(),\n",
    "        'recommended_method': recommended,\n",
    "        'best_time': min(standard_time, optimized_time, unionfind_time),\n",
    "        'standard_time': standard_time\n",
    "    }\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Run performance benchmark\n",
    "print(\"Executing performance benchmark...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with a subset if network is very large\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Network is large, testing with sample...\")\n",
    "    sample_nodes = list(G.nodes())[:1000]  # Sample first 1000 nodes\n",
    "    test_graph = G.subgraph(sample_nodes).copy()\n",
    "    benchmark_results = benchmark_percolation_methods(test_graph, num_thresholds=50)\n",
    "else:\n",
    "    benchmark_results = benchmark_percolation_methods(G, num_thresholds=100)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance benchmark completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and conclusion\n",
    "\n",
    "def generate_final_summary(analysis_results, hierarchy_results, advanced_results, \n",
    "                          validation_results, benchmark_results=None):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of all percolation analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE PERCOLATION ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Network overview\n",
    "    network_info = analysis_results['network_info']\n",
    "    print(f\"\\n1. NETWORK OVERVIEW\")\n",
    "    print(f\"   Network Type: TTWA Commuting Network\")\n",
    "    print(f\"   Total Nodes: {network_info['nodes']:,}\")\n",
    "    print(f\"   Total Edges: {network_info['edges']:,}\")\n",
    "    print(f\"   Weight Range: {network_info['weight_range'][0]:.6f} - {network_info['weight_range'][1]:.6f}\")\n",
    "    print(f\"   Analysis Points: {len(analysis_results['thresholds']):,}\")\n",
    "    \n",
    "    # Main percolation results\n",
    "    print(f\"\\n2. MAIN PERCOLATION RESULTS\")\n",
    "    print(f\"   Critical Threshold (max decline): {advanced_results['critical_threshold']:.6f}\")\n",
    "    print(f\"   Giant Component Size at Critical Point: {advanced_results['critical_size']:.3f}\")\n",
    "    print(f\"   Percolation Threshold (50% point): {advanced_results['percolation_threshold']:.6f}\")\n",
    "    \n",
    "    if advanced_results.get('gamma_exponent'):\n",
    "        print(f\"   Critical Exponent γ: {advanced_results['gamma_exponent']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   Critical Exponent γ: Not calculable\")\n",
    "    \n",
    "    print(f\"   Transition Steepness: {advanced_results['transition_steepness']:.4f}\")\n",
    "    print(f\"   Transition Width: {advanced_results['transition_width']} points\")\n",
    "    \n",
    "    # Critical points summary\n",
    "    print(f\"\\n3. CRITICAL POINTS IDENTIFIED\")\n",
    "    critical_points = analysis_results['critical_points']\n",
    "    print(f\"   Total Critical Points: {len(critical_points)}\")\n",
    "    \n",
    "    # Group by type\n",
    "    point_types = {}\n",
    "    for point in critical_points:\n",
    "        point_type = point['type']\n",
    "        if point_type not in point_types:\n",
    "            point_types[point_type] = []\n",
    "        point_types[point_type].append(point)\n",
    "    \n",
    "    for point_type, points in point_types.items():\n",
    "        print(f\"   {point_type}: {len(points)} points\")\n",
    "    \n",
    "    # Top 5 critical points\n",
    "    print(f\"\\n   Top 5 Critical Points:\")\n",
    "    for i, point in enumerate(critical_points[:5]):\n",
    "        if point['type'] == 'reference_match':\n",
    "            error = point.get('error', 0) * 100\n",
    "            print(f\"     {i+1}. τ = {point['threshold']:.6f} (Size: {point['giant_size']:.3f}, \"\n",
    "                  f\"Ref: {point['reference']:.6f}, Error: {error:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"     {i+1}. τ = {point['threshold']:.6f} (Size: {point['giant_size']:.3f}, \"\n",
    "                  f\"Type: {point['type']})\")\n",
    "    \n",
    "    # Hierarchy analysis summary\n",
    "    if hierarchy_results:\n",
    "        print(f\"\\n4. NETWORK HIERARCHY ANALYSIS\")\n",
    "        print(f\"   Analyzed Thresholds: {len(hierarchy_results)}\")\n",
    "        \n",
    "        # Find representative threshold\n",
    "        mid_threshold = sorted(hierarchy_results.keys())[len(hierarchy_results)//2]\n",
    "        mid_results = hierarchy_results[mid_threshold]\n",
    "        \n",
    "        print(f\"   Representative Analysis (τ = {mid_threshold:.6f}):\")\n",
    "        print(f\"     Giant Component: {mid_results['giant_component_size']} nodes\")\n",
    "        print(f\"     Core Nodes: {len(mid_results['core_nodes'])} \"\n",
    "              f\"({len(mid_results['core_nodes'])/mid_results['giant_component_size']*100:.1f}%)\")\n",
    "        print(f\"     Semi-periphery: {len(mid_results['semi_periphery_nodes'])} \"\n",
    "              f\"({len(mid_results['semi_periphery_nodes'])/mid_results['giant_component_size']*100:.1f}%)\")\n",
    "        print(f\"     Periphery: {len(mid_results['periphery_nodes'])} \"\n",
    "              f\"({len(mid_results['periphery_nodes'])/mid_results['giant_component_size']*100:.1f}%)\")\n",
    "        print(f\"     Network Density: {mid_results['network_density']:.4f}\")\n",
    "        print(f\"     Core Density: {mid_results['core_density']:.4f}\")\n",
    "    \n",
    "    # Validation results\n",
    "    print(f\"\\n5. VALIDATION RESULTS\")\n",
    "    print(f\"   Overall Status: {validation_results['overall_status']}\")\n",
    "    if 'overall_score' in validation_results:\n",
    "        print(f\"   Overall Score: {validation_results['overall_score']:.1%}\")\n",
    "    print(f\"   Passed Checks: {len(validation_results['passed_checks'])}\")\n",
    "    print(f\"   Failed Checks: {len(validation_results['failed_checks'])}\")\n",
    "    print(f\"   Warnings: {len(validation_results['warnings'])}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    if benchmark_results:\n",
    "        print(f\"\\n6. PERFORMANCE ANALYSIS\")\n",
    "        summary = benchmark_results.get('summary', {})\n",
    "        print(f\"   Network Size: {summary.get('network_size', 'N/A')} nodes\")\n",
    "        print(f\"   Recommended Method: {summary.get('recommended_method', 'N/A')}\")\n",
    "        print(f\"   Best Performance: {summary.get('best_time', 0):.2f} seconds\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\n7. KEY INSIGHTS\")\n",
    "    \n",
    "    # Percolation behavior\n",
    "    size_at_critical = advanced_results['critical_size']\n",
    "    if size_at_critical > 0.7:\n",
    "        behavior = \"Sharp transition - network shows clear percolation threshold\"\n",
    "    elif size_at_critical > 0.3:\n",
    "        behavior = \"Gradual transition - network shows smooth percolation behavior\"  \n",
    "    else:\n",
    "        behavior = \"Fragmented transition - network fragments gradually\"\n",
    "    \n",
    "    print(f\"   Percolation Behavior: {behavior}\")\n",
    "    \n",
    "    # Network robustness\n",
    "    critical_threshold = advanced_results['critical_threshold']\n",
    "    max_weight = network_info['weight_range'][1]\n",
    "    robustness_ratio = critical_threshold / max_weight\n",
    "    \n",
    "    if robustness_ratio > 0.5:\n",
    "        robustness = \"High - network maintains connectivity under strong filtering\"\n",
    "    elif robustness_ratio > 0.2:\n",
    "        robustness = \"Medium - network shows moderate resilience\"\n",
    "    else:\n",
    "        robustness = \"Low - network is sensitive to filtering\"\n",
    "        \n",
    "    print(f\"   Network Robustness: {robustness}\")\n",
    "    \n",
    "    # Hierarchical structure\n",
    "    if hierarchy_results:\n",
    "        avg_core_prop = np.mean([len(r['core_nodes'])/r['giant_component_size'] \n",
    "                                for r in hierarchy_results.values() \n",
    "                                if r['giant_component_size'] > 0])\n",
    "        \n",
    "        if avg_core_prop > 0.3:\n",
    "            hierarchy = \"Strong hierarchical structure with dominant core\"\n",
    "        elif avg_core_prop > 0.15:\n",
    "            hierarchy = \"Moderate hierarchical structure\"\n",
    "        else:\n",
    "            hierarchy = \"Weak hierarchical structure - more distributed\"\n",
    "            \n",
    "        print(f\"   Hierarchical Structure: {hierarchy}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    num_critical = len(critical_points)\n",
    "    if num_critical > 10:\n",
    "        significance = \"High - multiple critical points identified\"\n",
    "    elif num_critical > 5:\n",
    "        significance = \"Medium - several critical points identified\"\n",
    "    else:\n",
    "        significance = \"Low - few critical points identified\"\n",
    "        \n",
    "    print(f\"   Statistical Significance: {significance}\")\n",
    "    \n",
    "    print(f\"\\n8. RECOMMENDATIONS\")\n",
    "    print(f\"   • Use critical threshold τ = {advanced_results['critical_threshold']:.6f} for network analysis\")\n",
    "    print(f\"   • Consider percolation threshold τ = {advanced_results['percolation_threshold']:.6f} for 50% connectivity\")\n",
    "    \n",
    "    if validation_results['overall_status'] == 'FAILED':\n",
    "        print(f\"   • ⚠ Review validation failures before using results\")\n",
    "    elif validation_results['warnings']:\n",
    "        print(f\"   • ⚠ Consider validation warnings in interpretation\")\n",
    "    \n",
    "    if hierarchy_results:\n",
    "        print(f\"   • Focus on core nodes for network backbone analysis\")\n",
    "        print(f\"   • Consider core-periphery structure in network modeling\")\n",
    "    \n",
    "    print(f\"   • All results and visualizations exported for further analysis\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"=\" * 80)\n",
    "\n",
    "# Generate comprehensive final summary\n",
    "print(\"Generating final comprehensive summary...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "generate_final_summary(\n",
    "    analysis_results=analysis_results,\n",
    "    hierarchy_results=hierarchy_results,\n",
    "    advanced_results=advanced_results,\n",
    "    validation_results=validation_results,\n",
    "    benchmark_results=benchmark_results if 'benchmark_results' in locals() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083828b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification - run a simple test\n",
    "print(\"Quick verification test:\")\n",
    "print(f\"Main analysis results available: {'analysis_results' in locals()}\")\n",
    "print(f\"Advanced analysis completed: {'advanced_results' in locals()}\")\n",
    "print(f\"Hierarchy analysis completed: {'hierarchy_results' in locals()}\")\n",
    "print(f\"Validation completed: {'validation_results' in locals()}\")\n",
    "\n",
    "if 'analysis_results' in locals():\n",
    "    print(f\"Number of sampling points: {len(analysis_results['thresholds'])}\")\n",
    "    print(f\"Number of critical points: {len(analysis_results['critical_points'])}\")\n",
    "    print(f\"Network size: {analysis_results['network_info']['nodes']} nodes\")\n",
    "\n",
    "print(\"\\nAll analysis components completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc24db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Community detection at critical thresholds\n",
    "\n",
    "def analyze_communities_at_critical_thresholds(G, critical_thresholds, max_analyze=3):\n",
    "    \"\"\"\n",
    "    Analyze community structure at critical percolation thresholds\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - critical_thresholds: list of critical threshold values\n",
    "    - max_analyze: maximum number of thresholds to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - community_results: dictionary containing community analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing community structure at critical thresholds...\")\n",
    "    \n",
    "    # Select thresholds to analyze\n",
    "    selected_thresholds = critical_thresholds[:max_analyze]\n",
    "    print(f\"Analyzing {len(selected_thresholds)} thresholds: {[f'{t:.6f}' for t in selected_thresholds]}\")\n",
    "    \n",
    "    community_results = {}\n",
    "    \n",
    "    for i, threshold in enumerate(selected_thresholds):\n",
    "        print(f\"\\nAnalyzing threshold {i+1}/{len(selected_thresholds)}: τ = {threshold:.6f}\")\n",
    "        \n",
    "        # Create filtered network\n",
    "        filtered_edges = [(u, v, data) for u, v, data in G.edges(data=True) \n",
    "                         if data.get('weight', 0) >= threshold]\n",
    "        \n",
    "        if len(filtered_edges) == 0:\n",
    "            print(\"  No edges above threshold\")\n",
    "            continue\n",
    "        \n",
    "        # Create subgraph\n",
    "        subgraph = nx.Graph()\n",
    "        subgraph.add_edges_from([(u, v) for u, v, _ in filtered_edges])\n",
    "        \n",
    "        # Focus on largest connected component\n",
    "        components = list(nx.connected_components(subgraph))\n",
    "        if not components:\n",
    "            print(\"  No connected components\")\n",
    "            continue\n",
    "            \n",
    "        largest_component = max(components, key=len)\n",
    "        giant_subgraph = subgraph.subgraph(largest_component)\n",
    "        \n",
    "        print(f\"  Giant component: {len(largest_component)} nodes, {giant_subgraph.number_of_edges()} edges\")\n",
    "        \n",
    "        # Community detection using multiple algorithms\n",
    "        try:\n",
    "            # Louvain algorithm (fast and effective)\n",
    "            import community as community_louvain\n",
    "            louvain_communities = community_louvain.best_partition(giant_subgraph)\n",
    "            louvain_modularity = community_louvain.modularity(louvain_communities, giant_subgraph)\n",
    "            \n",
    "            print(f\"  Louvain communities: {len(set(louvain_communities.values()))}\")\n",
    "            print(f\"  Louvain modularity: {louvain_modularity:.3f}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"  Louvain algorithm not available (install python-louvain)\")\n",
    "            louvain_communities = None\n",
    "            louvain_modularity = None\n",
    "        \n",
    "        # Greedy modularity communities (built into NetworkX)\n",
    "        try:\n",
    "            greedy_communities = list(nx.community.greedy_modularity_communities(giant_subgraph))\n",
    "            greedy_modularity = nx.community.modularity(giant_subgraph, greedy_communities)\n",
    "            \n",
    "            print(f\"  Greedy communities: {len(greedy_communities)}\")\n",
    "            print(f\"  Greedy modularity: {greedy_modularity:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Greedy algorithm failed: {e}\")\n",
    "            greedy_communities = None\n",
    "            greedy_modularity = None\n",
    "        \n",
    "        # Label propagation algorithm\n",
    "        try:\n",
    "            label_prop_communities = list(nx.community.label_propagation_communities(giant_subgraph))\n",
    "            label_prop_modularity = nx.community.modularity(giant_subgraph, label_prop_communities)\n",
    "            \n",
    "            print(f\"  Label propagation communities: {len(label_prop_communities)}\")\n",
    "            print(f\"  Label propagation modularity: {label_prop_modularity:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Label propagation failed: {e}\")\n",
    "            label_prop_communities = None\n",
    "            label_prop_modularity = None\n",
    "        \n",
    "        # Community size analysis\n",
    "        community_sizes = []\n",
    "        if louvain_communities:\n",
    "            # Count community sizes for Louvain\n",
    "            community_counts = {}\n",
    "            for node, community in louvain_communities.items():\n",
    "                community_counts[community] = community_counts.get(community, 0) + 1\n",
    "            community_sizes = list(community_counts.values())\n",
    "        elif greedy_communities:\n",
    "            community_sizes = [len(comm) for comm in greedy_communities]\n",
    "        \n",
    "        if community_sizes:\n",
    "            print(f\"  Community size statistics:\")\n",
    "            print(f\"    Largest community: {max(community_sizes)} nodes\")\n",
    "            print(f\"    Average community size: {np.mean(community_sizes):.1f}\")\n",
    "            print(f\"    Community size std: {np.std(community_sizes):.1f}\")\n",
    "        \n",
    "        # Store results\n",
    "        threshold_results = {\n",
    "            'threshold': threshold,\n",
    "            'giant_component_size': len(largest_component),\n",
    "            'giant_component_edges': giant_subgraph.number_of_edges(),\n",
    "            'louvain_communities': louvain_communities,\n",
    "            'louvain_modularity': louvain_modularity,\n",
    "            'greedy_communities': greedy_communities,\n",
    "            'greedy_modularity': greedy_modularity,\n",
    "            'label_prop_communities': label_prop_communities,\n",
    "            'label_prop_modularity': label_prop_modularity,\n",
    "            'community_sizes': community_sizes\n",
    "        }\n",
    "        \n",
    "        community_results[threshold] = threshold_results\n",
    "    \n",
    "    print(f\"\\nCommunity analysis completed for {len(community_results)} thresholds\")\n",
    "    return community_results\n",
    "\n",
    "# Visualize community structure\n",
    "def plot_community_analysis(community_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize community analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not community_results:\n",
    "        print(\"No community results to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Community Analysis at Critical Thresholds', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data\n",
    "    thresholds = []\n",
    "    louvain_mods = []\n",
    "    greedy_mods = []\n",
    "    label_mods = []\n",
    "    num_communities = []\n",
    "    \n",
    "    for threshold, results in community_results.items():\n",
    "        thresholds.append(threshold)\n",
    "        louvain_mods.append(results['louvain_modularity'] if results['louvain_modularity'] else 0)\n",
    "        greedy_mods.append(results['greedy_modularity'] if results['greedy_modularity'] else 0)\n",
    "        label_mods.append(results['label_prop_modularity'] if results['label_prop_modularity'] else 0)\n",
    "        \n",
    "        # Count communities (use Louvain if available, otherwise greedy)\n",
    "        if results['louvain_communities']:\n",
    "            num_comm = len(set(results['louvain_communities'].values()))\n",
    "        elif results['greedy_communities']:\n",
    "            num_comm = len(results['greedy_communities'])\n",
    "        else:\n",
    "            num_comm = 0\n",
    "        num_communities.append(num_comm)\n",
    "    \n",
    "    # Plot 1: Modularity comparison\n",
    "    ax1.semilogx(thresholds, louvain_mods, 'o-', label='Louvain', linewidth=2, markersize=8)\n",
    "    ax1.semilogx(thresholds, greedy_mods, 's-', label='Greedy', linewidth=2, markersize=8)\n",
    "    ax1.semilogx(thresholds, label_mods, '^-', label='Label Propagation', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Threshold τ')\n",
    "    ax1.set_ylabel('Modularity')\n",
    "    ax1.set_title('Community Detection Modularity')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Number of communities\n",
    "    ax2.semilogx(thresholds, num_communities, 'D-', color='purple', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Threshold τ')\n",
    "    ax2.set_ylabel('Number of Communities')\n",
    "    ax2.set_title('Community Count vs Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Community size distribution (for first threshold)\n",
    "    if community_results:\n",
    "        first_threshold = list(community_results.keys())[0]\n",
    "        first_result = community_results[first_threshold]\n",
    "        \n",
    "        if first_result['community_sizes']:\n",
    "            ax3.hist(first_result['community_sizes'], bins=20, alpha=0.7, edgecolor='black')\n",
    "            ax3.set_xlabel('Community Size')\n",
    "            ax3.set_ylabel('Frequency')\n",
    "            ax3.set_title(f'Community Size Distribution\\n(τ = {first_threshold:.6f})')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'No community\\nsize data', transform=ax3.transAxes, \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Plot 4: Modularity vs Number of Communities\n",
    "    valid_indices = [i for i, m in enumerate(louvain_mods) if m > 0]\n",
    "    if valid_indices:\n",
    "        valid_mods = [louvain_mods[i] for i in valid_indices]\n",
    "        valid_nums = [num_communities[i] for i in valid_indices]\n",
    "        \n",
    "        ax4.scatter(valid_nums, valid_mods, s=100, alpha=0.7)\n",
    "        ax4.set_xlabel('Number of Communities')\n",
    "        ax4.set_ylabel('Modularity')\n",
    "        ax4.set_title('Modularity vs Community Count')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add threshold labels\n",
    "        for i, idx in enumerate(valid_indices):\n",
    "            ax4.annotate(f'{thresholds[idx]:.3f}', \n",
    "                        (valid_nums[i], valid_mods[i]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Community analysis plot saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute community analysis\n",
    "print(\"Executing community structure analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use top 3 critical thresholds\n",
    "if 'final_analysis_critical_points' in locals():\n",
    "    critical_thresholds_for_community = [point['threshold'] for point in final_analysis_critical_points[:3]]\n",
    "else:\n",
    "    # Fallback to predefined thresholds\n",
    "    critical_thresholds_for_community = [0.274173, 0.217115, 0.149683]\n",
    "\n",
    "community_analysis_results = analyze_communities_at_critical_thresholds(\n",
    "    G, \n",
    "    critical_thresholds_for_community,\n",
    "    max_analyze=3\n",
    ")\n",
    "\n",
    "# Visualize community analysis\n",
    "community_plot_filename = \"community_analysis_results.png\"\n",
    "plot_community_analysis(community_analysis_results, save_path=community_plot_filename)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Community structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network resilience analysis - understanding network robustness\n",
    "\n",
    "def analyze_network_resilience(G, attack_strategies=['random', 'degree', 'betweenness'], \n",
    "                              attack_fractions=np.linspace(0, 0.5, 11)):\n",
    "    \"\"\"\n",
    "    Analyze network resilience under different attack strategies\n",
    "    \n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - attack_strategies: list of attack strategies to test\n",
    "    - attack_fractions: fractions of nodes to remove\n",
    "    \n",
    "    Returns:\n",
    "    - resilience_results: dictionary containing resilience analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing network resilience...\")\n",
    "    print(f\"Testing {len(attack_strategies)} attack strategies\")\n",
    "    print(f\"Attack fractions: {attack_fractions}\")\n",
    "    \n",
    "    # Convert to undirected for connected components analysis\n",
    "    if G.is_directed():\n",
    "        G_undirected = G.to_undirected()\n",
    "    else:\n",
    "        G_undirected = G.copy()\n",
    "    \n",
    "    original_size = G_undirected.number_of_nodes()\n",
    "    original_edges = G_undirected.number_of_edges()\n",
    "    \n",
    "    # Calculate initial largest component\n",
    "    initial_components = list(nx.connected_components(G_undirected))\n",
    "    initial_largest = max(len(comp) for comp in initial_components) if initial_components else 0\n",
    "    \n",
    "    print(f\"Original network: {original_size} nodes, {original_edges} edges\")\n",
    "    print(f\"Initial largest component: {initial_largest} nodes ({initial_largest/original_size:.1%})\")\n",
    "    \n",
    "    resilience_results = {}\n",
    "    \n",
    "    for strategy in attack_strategies:\n",
    "        print(f\"\\n  Testing {strategy} attack strategy...\")\n",
    "        \n",
    "        strategy_results = {\n",
    "            'fractions_removed': [],\n",
    "            'largest_component_sizes': [],\n",
    "            'num_components': [],\n",
    "            'remaining_edges': []\n",
    "        }\n",
    "        \n",
    "        # Create working copy\n",
    "        G_work = G_undirected.copy()\n",
    "        nodes = list(G_work.nodes())\n",
    "        \n",
    "        # Pre-calculate node rankings for targeted attacks\n",
    "        if strategy == 'degree':\n",
    "            node_scores = dict(G_work.degree())\n",
    "        elif strategy == 'betweenness':\n",
    "            # Sample for large networks\n",
    "            if len(nodes) > 500:\n",
    "                sample_nodes = np.random.choice(nodes, 500, replace=False)\n",
    "                node_scores = nx.betweenness_centrality(G_work.subgraph(sample_nodes))\n",
    "                # Set score 0 for non-sampled nodes\n",
    "                for node in nodes:\n",
    "                    if node not in node_scores:\n",
    "                        node_scores[node] = 0\n",
    "            else:\n",
    "                node_scores = nx.betweenness_centrality(G_work)\n",
    "        else:  # random\n",
    "            node_scores = {node: np.random.random() for node in nodes}\n",
    "        \n",
    "        # Sort nodes by strategy\n",
    "        if strategy == 'random':\n",
    "            sorted_nodes = nodes.copy()\n",
    "            np.random.shuffle(sorted_nodes)\n",
    "        else:\n",
    "            sorted_nodes = sorted(nodes, key=lambda x: node_scores[x], reverse=True)\n",
    "        \n",
    "        for fraction in attack_fractions:\n",
    "            # Calculate number of nodes to remove\n",
    "            num_to_remove = int(fraction * original_size)\n",
    "            \n",
    "            # Remove nodes\n",
    "            if num_to_remove > 0:\n",
    "                nodes_to_remove = sorted_nodes[:num_to_remove]\n",
    "                G_work.remove_nodes_from(nodes_to_remove)\n",
    "            \n",
    "            # Analyze remaining network\n",
    "            if G_work.number_of_nodes() > 0:\n",
    "                components = list(nx.connected_components(G_work))\n",
    "                largest_component_size = max(len(comp) for comp in components) if components else 0\n",
    "                num_components = len(components)\n",
    "                remaining_edges = G_work.number_of_edges()\n",
    "            else:\n",
    "                largest_component_size = 0\n",
    "                num_components = 0\n",
    "                remaining_edges = 0\n",
    "            \n",
    "            # Store results\n",
    "            strategy_results['fractions_removed'].append(fraction)\n",
    "            strategy_results['largest_component_sizes'].append(largest_component_size)\n",
    "            strategy_results['num_components'].append(num_components)\n",
    "            strategy_results['remaining_edges'].append(remaining_edges)\n",
    "            \n",
    "            # Restore for next iteration\n",
    "            G_work = G_undirected.copy()\n",
    "        \n",
    "        resilience_results[strategy] = strategy_results\n",
    "        \n",
    "        # Print strategy summary\n",
    "        final_size = strategy_results['largest_component_sizes'][-1]\n",
    "        print(f\"    After {attack_fractions[-1]:.0%} attack: {final_size} nodes remaining \"\n",
    "              f\"({final_size/original_size:.1%} of original)\")\n",
    "    \n",
    "    print(f\"\\nResilience analysis completed\")\n",
    "    return resilience_results\n",
    "\n",
    "def plot_resilience_analysis(resilience_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize network resilience analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Network Resilience Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = {'random': '#3498DB', 'degree': '#E74C3C', 'betweenness': '#2ECC71'}\n",
    "    \n",
    "    # Plot 1: Largest component size vs attack fraction\n",
    "    for strategy, results in resilience_results.items():\n",
    "        fractions = results['fractions_removed']\n",
    "        sizes = results['largest_component_sizes']\n",
    "        \n",
    "        ax1.plot(fractions, sizes, 'o-', \n",
    "                color=colors.get(strategy, 'gray'), \n",
    "                linewidth=2.5, markersize=6, \n",
    "                label=f'{strategy.capitalize()} Attack')\n",
    "    \n",
    "    ax1.set_xlabel('Fraction of Nodes Removed')\n",
    "    ax1.set_ylabel('Largest Component Size')\n",
    "    ax1.set_title('Network Fragmentation Under Attack')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Relative largest component size\n",
    "    for strategy, results in resilience_results.items():\n",
    "        fractions = results['fractions_removed']\n",
    "        sizes = results['largest_component_sizes']\n",
    "        original_size = sizes[0] if sizes else 1  # Avoid division by zero\n",
    "        relative_sizes = [s/original_size for s in sizes]\n",
    "        \n",
    "        ax2.plot(fractions, relative_sizes, 'o-', \n",
    "                color=colors.get(strategy, 'gray'), \n",
    "                linewidth=2.5, markersize=6, \n",
    "                label=f'{strategy.capitalize()} Attack')\n",
    "    \n",
    "    ax2.set_xlabel('Fraction of Nodes Removed')\n",
    "    ax2.set_ylabel('Relative Largest Component Size')\n",
    "    ax2.set_title('Normalized Network Fragmentation')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Number of components\n",
    "    for strategy, results in resilience_results.items():\n",
    "        fractions = results['fractions_removed']\n",
    "        num_components = results['num_components']\n",
    "        \n",
    "        ax3.plot(fractions, num_components, 's-', \n",
    "                color=colors.get(strategy, 'gray'), \n",
    "                linewidth=2.5, markersize=6, \n",
    "                label=f'{strategy.capitalize()} Attack')\n",
    "    \n",
    "    ax3.set_xlabel('Fraction of Nodes Removed')\n",
    "    ax3.set_ylabel('Number of Components')\n",
    "    ax3.set_title('Network Fragmentation Pattern')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Edge retention\n",
    "    for strategy, results in resilience_results.items():\n",
    "        fractions = results['fractions_removed']\n",
    "        remaining_edges = results['remaining_edges']\n",
    "        original_edges = remaining_edges[0] if remaining_edges else 1\n",
    "        relative_edges = [e/original_edges for e in remaining_edges]\n",
    "        \n",
    "        ax4.plot(fractions, relative_edges, '^-', \n",
    "                color=colors.get(strategy, 'gray'), \n",
    "                linewidth=2.5, markersize=6, \n",
    "                label=f'{strategy.capitalize()} Attack')\n",
    "    \n",
    "    ax4.set_xlabel('Fraction of Nodes Removed')\n",
    "    ax4.set_ylabel('Relative Number of Edges')\n",
    "    ax4.set_title('Edge Retention Under Attack')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Resilience analysis plot saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print resilience summary\n",
    "    print(\"\\nResilience Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for strategy, results in resilience_results.items():\n",
    "        sizes = results['largest_component_sizes']\n",
    "        original_size = sizes[0]\n",
    "        \n",
    "        # Find critical point (where largest component drops below 50% of original)\n",
    "        critical_fraction = None\n",
    "        for i, size in enumerate(sizes):\n",
    "            if size < original_size * 0.5:\n",
    "                critical_fraction = results['fractions_removed'][i]\n",
    "                break\n",
    "        \n",
    "        if critical_fraction is not None:\n",
    "            print(f\"{strategy.capitalize():>12} attack: Critical point at {critical_fraction:.1%} removal\")\n",
    "        else:\n",
    "            print(f\"{strategy.capitalize():>12} attack: No critical point within test range\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Execute resilience analysis\n",
    "print(\"Executing network resilience analysis...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test resilience with subset if network is large\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Network is large, testing with sample...\")\n",
    "    sample_nodes = list(G.nodes())[:1000]\n",
    "    test_graph = G.subgraph(sample_nodes).copy()\n",
    "    \n",
    "    resilience_results = analyze_network_resilience(\n",
    "        test_graph,\n",
    "        attack_strategies=['random', 'degree'],  # Reduce strategies for large networks\n",
    "        attack_fractions=np.linspace(0, 0.4, 9)  # Reduce test points\n",
    "    )\n",
    "else:\n",
    "    resilience_results = analyze_network_resilience(\n",
    "        G,\n",
    "        attack_strategies=['random', 'degree', 'betweenness'],\n",
    "        attack_fractions=np.linspace(0, 0.5, 11)\n",
    "    )\n",
    "\n",
    "# Visualize resilience analysis\n",
    "resilience_plot_filename = \"network_resilience_analysis.png\"\n",
    "plot_resilience_analysis(resilience_results, save_path=resilience_plot_filename)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Network resilience analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and final status check\n",
    "\n",
    "print(\"FINAL ANALYSIS STATUS CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what analysis components were completed\n",
    "completed_analyses = []\n",
    "available_results = []\n",
    "\n",
    "if 'G' in locals():\n",
    "    print(f\"✓ Network loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    completed_analyses.append(\"Network Loading\")\n",
    "\n",
    "if 'analysis_results' in locals():\n",
    "    print(f\"✓ Main percolation analysis completed\")\n",
    "    print(f\"  - Sampling points: {len(analysis_results['thresholds'])}\")\n",
    "    print(f\"  - Critical points: {len(analysis_results['critical_points'])}\")\n",
    "    completed_analyses.append(\"Main Percolation Analysis\")\n",
    "    available_results.append(\"analysis_results\")\n",
    "\n",
    "if 'advanced_results' in locals():\n",
    "    print(f\"✓ Advanced percolation analysis completed\")\n",
    "    print(f\"  - Critical threshold: {advanced_results['critical_threshold']:.6f}\")\n",
    "    print(f\"  - Transition points: {len(advanced_results['transition_points'])}\")\n",
    "    completed_analyses.append(\"Advanced Analysis\")\n",
    "    available_results.append(\"advanced_results\")\n",
    "\n",
    "if 'hierarchy_results' in locals():\n",
    "    print(f\"✓ Network hierarchy analysis completed\")\n",
    "    print(f\"  - Analyzed thresholds: {len(hierarchy_results)}\")\n",
    "    completed_analyses.append(\"Hierarchy Analysis\")\n",
    "    available_results.append(\"hierarchy_results\")\n",
    "\n",
    "if 'validation_results' in locals():\n",
    "    print(f\"✓ Validation analysis completed\")\n",
    "    print(f\"  - Status: {validation_results['overall_status']}\")\n",
    "    completed_analyses.append(\"Validation\")\n",
    "    available_results.append(\"validation_results\")\n",
    "\n",
    "if 'community_analysis_results' in locals():\n",
    "    print(f\"✓ Community analysis completed\")\n",
    "    print(f\"  - Analyzed thresholds: {len(community_analysis_results)}\")\n",
    "    completed_analyses.append(\"Community Analysis\")\n",
    "    available_results.append(\"community_analysis_results\")\n",
    "\n",
    "if 'resilience_results' in locals():\n",
    "    print(f\"✓ Resilience analysis completed\")\n",
    "    print(f\"  - Attack strategies: {len(resilience_results)}\")\n",
    "    completed_analyses.append(\"Resilience Analysis\")\n",
    "    available_results.append(\"resilience_results\")\n",
    "\n",
    "if 'benchmark_results' in locals():\n",
    "    print(f\"✓ Performance benchmark completed\")\n",
    "    completed_analyses.append(\"Performance Benchmark\")\n",
    "    available_results.append(\"benchmark_results\")\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Total completed analyses: {len(completed_analyses)}\")\n",
    "print(f\"Available result sets: {len(available_results)}\")\n",
    "\n",
    "if len(completed_analyses) >= 4:  # Main + Advanced + Hierarchy + Validation\n",
    "    print(f\"✓ ANALYSIS COMPLETE - All core components finished\")\n",
    "else:\n",
    "    print(f\"⚠ ANALYSIS PARTIAL - Some components may be missing\")\n",
    "\n",
    "print(f\"\\nCompleted analysis components:\")\n",
    "for i, analysis in enumerate(completed_analyses, 1):\n",
    "    print(f\"  {i}. {analysis}\")\n",
    "\n",
    "# Memory cleanup recommendations\n",
    "print(f\"\\nMEMORY USAGE NOTES:\")\n",
    "print(f\"- Large analysis variables are retained for further use\")\n",
    "print(f\"- Run 'del variable_name' to free memory if needed\")\n",
    "print(f\"- All results have been exported to files\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERCOLATION ANALYSIS PIPELINE COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all variables and results available for use\n",
    "\n",
    "print(\"AVAILABLE ANALYSIS VARIABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all major variables created during analysis\n",
    "variables_info = [\n",
    "    (\"G\", \"Main network graph\", 'G' in locals()),\n",
    "    (\"analysis_results\", \"Main percolation analysis results\", 'analysis_results' in locals()),\n",
    "    (\"advanced_results\", \"Advanced analysis with critical exponents\", 'advanced_results' in locals()),\n",
    "    (\"hierarchy_results\", \"Network hierarchy analysis\", 'hierarchy_results' in locals()),\n",
    "    (\"validation_results\", \"Analysis validation results\", 'validation_results' in locals()),\n",
    "    (\"community_analysis_results\", \"Community structure analysis\", 'community_analysis_results' in locals()),\n",
    "    (\"resilience_results\", \"Network resilience analysis\", 'resilience_results' in locals()),\n",
    "    (\"benchmark_results\", \"Performance benchmark results\", 'benchmark_results' in locals()),\n",
    "    (\"final_analysis_thresholds\", \"Final threshold array\", 'final_analysis_thresholds' in locals()),\n",
    "    (\"final_analysis_giant_sizes\", \"Final giant component sizes\", 'final_analysis_giant_sizes' in locals()),\n",
    "    (\"final_analysis_critical_points\", \"Final critical points\", 'final_analysis_critical_points' in locals())\n",
    "]\n",
    "\n",
    "print(f\"{'Variable':<30} {'Description':<40} {'Available':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "available_count = 0\n",
    "for var_name, description, available in variables_info:\n",
    "    status = \"✓ Yes\" if available else \"✗ No\"\n",
    "    print(f\"{var_name:<30} {description:<40} {status:<10}\")\n",
    "    if available:\n",
    "        available_count += 1\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total available: {available_count}/{len(variables_info)}\")\n",
    "\n",
    "# Show key results if available\n",
    "if 'analysis_results' in locals():\n",
    "    print(f\"\\nKEY RESULTS SUMMARY:\")\n",
    "    print(f\"Network size: {analysis_results['network_info']['nodes']} nodes\")\n",
    "    print(f\"Critical points identified: {len(analysis_results['critical_points'])}\")\n",
    "\n",
    "if 'advanced_results' in locals():\n",
    "    print(f\"Critical threshold: {advanced_results['critical_threshold']:.6f}\")\n",
    "    print(f\"Percolation threshold: {advanced_results['percolation_threshold']:.6f}\")\n",
    "\n",
    "print(f\"\\nFor detailed results, see exported files and comprehensive documentation.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
