{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read data and calculate dependency weights\n",
    "def calculate_dependency_weights(df):\n",
    "    \"\"\"\n",
    "    Calculate dependency weights w_ij = t_ij / Σ_j t_ij\n",
    "    where t_ij is the commuting flow from i to j\n",
    "    \"\"\"\n",
    "    # Calculate total outflow for each origin city\n",
    "    total_outflow = df.groupby('ORIGIN_TTWA')['TOTAL_FLOW'].sum()\n",
    "    \n",
    "    # Calculate dependency weights\n",
    "    df['dependency_weight'] = df.apply(\n",
    "        lambda row: row['TOTAL_FLOW'] / total_outflow[row['ORIGIN_TTWA']], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('ttwa_od_matrix_cross_city_only.csv')\n",
    "df_with_weights = calculate_dependency_weights(df)\n",
    "\n",
    "print(f\"Data overview: {len(df)} OD records, {df['ORIGIN_TTWA'].nunique()} origin cities\")\n",
    "print(f\"Dependency weight range: {df_with_weights['dependency_weight'].min():.6f} - {df_with_weights['dependency_weight'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build directed commuting network\n",
    "def build_commuting_network(df_weights):\n",
    "    \"\"\"Build directed commuting network\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges with dependency weights\n",
    "    for _, row in df_weights.iterrows():\n",
    "        G.add_edge(\n",
    "            row['ORIGIN_TTWA'], \n",
    "            row['DEST_TTWA'], \n",
    "            weight=row['dependency_weight'],\n",
    "            flow=row['TOTAL_FLOW']\n",
    "        )\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = build_commuting_network(df_with_weights)\n",
    "print(f\"Network scale: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have already run the previous code and obtained G and df_with_weights\n",
    "\n",
    "print(\"--- Starting calculation of network centrality metrics ---\")\n",
    "\n",
    "# --- Step 3: Calculate weighted in-degree (Weighted In-Degree) ---\n",
    "# G.in_degree() returns a special object containing (node, degree) pairs\n",
    "# We use weight='flow' to specify weights, which corresponds to the 'flow' attribute you added to edges\n",
    "# If you want to use 'dependency_weight' as weight, just change weight='flow' to weight='dependency_weight'\n",
    "print(\"   Step 1/4: Calculating weighted in-degree...\")\n",
    "in_degree_weighted = dict(G.in_degree(weight='flow'))\n",
    "\n",
    "# --- Step 4: Convert results to Pandas DataFrame ---\n",
    "print(\"   Step 2/4: Organizing results into table format...\")\n",
    "df_metrics = pd.DataFrame.from_dict(\n",
    "    in_degree_weighted, \n",
    "    orient='index', \n",
    "    columns=['in_degree_weighted']\n",
    ")\n",
    "df_metrics.index.name = 'TTWA_Code'\n",
    "df_metrics.reset_index(inplace=True)\n",
    "\n",
    "# --- Step 5: Add TTWA names for easier reading ---\n",
    "# We extract the correspondence between TTWA codes and names from your original df_with_weights data\n",
    "# Here we assume the destination columns are named 'DEST_TTWA' and 'DEST_TTWA_NAME', please modify according to your actual situation\n",
    "print(\"   Step 3/4: Adding TTWA names...\")\n",
    "try:\n",
    "    ttwa_names = df_with_weights[['DEST_TTWA', 'DEST_TTWA_NAME']].drop_duplicates().rename(\n",
    "        columns={'DEST_TTWA': 'TTWA_Code', 'DEST_TTWA_NAME': 'TTWA_Name'}\n",
    "    )\n",
    "    # Merge names into metrics table\n",
    "    final_df = pd.merge(ttwa_names, df_metrics, on='TTWA_Code', how='right')\n",
    "\n",
    "except KeyError:\n",
    "    print(\"   Warning: 'DEST_TTWA_NAME' column not found, output results will not include TTWA names.\")\n",
    "    final_df = df_metrics\n",
    "\n",
    "\n",
    "# --- Step 6: Save to CSV file ---\n",
    "OUTPUT_FILE = 'inter_city_in_degree_metrics.csv'\n",
    "print(f\"   Step 4/4: Saving results to '{OUTPUT_FILE}'...\")\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nSuccess! City status metrics have been calculated.\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(\"\\nFile content preview:\")\n",
    "print(final_df.sort_values('in_degree_weighted', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb3c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_diagnostics.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "\n",
    "# Ignore some minor warnings that may appear during plotting\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_single_model(df, model_vars, model_name):\n",
    "    \"\"\"\n",
    "    Perform correlation matrix visualization and VIF analysis for a single model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n>>> Analyzing {model_name}...\")\n",
    "    \n",
    "    # Extract data required for the model and remove rows containing missing values based on these columns\n",
    "    model_data = df[model_vars].dropna()\n",
    "    print(f\"   Number of valid observations for this model: {len(model_data)}\")\n",
    "    \n",
    "    # --- Step 1: Correlation matrix ---\n",
    "    print(f\"\\n>>> Step 1/2: Correlation analysis for {model_name}...\")\n",
    "    \n",
    "    corr_matrix = model_data.corr()\n",
    "    print(f\"--- {model_name} Correlation matrix values ---\")\n",
    "    print(corr_matrix.round(4))\n",
    "\n",
    "    # Find highly correlated independent variable pairs (check only among independent variables)\n",
    "    independent_vars_only = [var for var in model_vars if var != 'log_in_degree_weighted']\n",
    "    high_corr_threshold = 0.7\n",
    "    high_corr_found = False\n",
    "    for i in range(len(independent_vars_only)):\n",
    "        for j in range(i + 1, len(independent_vars_only)):\n",
    "            var1 = independent_vars_only[i]\n",
    "            var2 = independent_vars_only[j]\n",
    "            corr_val = abs(corr_matrix.loc[var1, var2])\n",
    "            if corr_val > high_corr_threshold:\n",
    "                print(f\"Warning: High correlation found among independent variables: {var1} and {var2} (r = {corr_matrix.loc[var1, var2]:.4f})\")\n",
    "                high_corr_found = True\n",
    "    \n",
    "    if not high_corr_found:\n",
    "        print(f\"No high correlation found among independent variables (all |r| <= {high_corr_threshold})\")\n",
    "\n",
    "    # --- Step 2: VIF test (for independent variables only) ---\n",
    "    print(f\"\\n>>> Step 2/2: Calculating VIF values for {model_name} independent variables...\")\n",
    "    \n",
    "    vif_data = model_data[independent_vars_only]\n",
    "    \n",
    "    if len(independent_vars_only) < 2:\n",
    "        print(f\"   Info: {model_name} has fewer than 2 independent variables, no need to calculate VIF.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate VIF\n",
    "    vif_result = pd.DataFrame()\n",
    "    vif_result[\"Variable\"] = vif_data.columns\n",
    "    vif_result[\"VIF\"] = [variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])]\n",
    "    \n",
    "    print(f\"\\n--- {model_name} VIF test results ---\")\n",
    "    print(vif_result.to_string(index=False))\n",
    "    \n",
    "    # Provide diagnostic conclusions based on VIF values\n",
    "    if (vif_result['VIF'] >= 5).any():\n",
    "        print(f\"Reminder: Variables in {model_name} have VIF values greater than or equal to 5, please interpret regression coefficients carefully.\")\n",
    "    else:\n",
    "        print(\"No serious multicollinearity issues, regression analysis can be performed safely.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to read final data and test both models.\n",
    "    \"\"\"\n",
    "    # --- 1. Read final data file generated by previous script ---\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "    \n",
    "    print(f\"\\n>>> Reading final dataset '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(f\"   Data read successfully, contains {len(df)} final observations.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{INPUT_DATA_FILE}' not found. Please run the data cleaning script first.\")\n",
    "        return\n",
    "\n",
    "    # Create log-transformed dependent variable\n",
    "    if 'in_degree_weighted' in df.columns:\n",
    "        df['log_in_degree_weighted'] = np.log1p(df['in_degree_weighted'])\n",
    "    else:\n",
    "        print(\"Error: 'in_degree_weighted' column not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Define variable sets for both models ---\n",
    "    # Model 1: Using Gamma (Centripetality)\n",
    "    model1_vars = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Centripetality_Gamma_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Model 2: Using Lambda (Anisotropy)\n",
    "    model2_vars = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Check if all variables exist\n",
    "    all_vars = list(set(model1_vars + model2_vars))\n",
    "    missing_vars = [var for var in all_vars if var not in df.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"Error: The following key variables do not exist in the data: {missing_vars}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"All variables exist, starting to test both models separately...\")\n",
    "\n",
    "    # --- 3. Analyze Model 1 ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model 1 analysis: log(InDegree) ~ Centripetality_Gamma + jobs_density\")\n",
    "    print(\"=\"*60)\n",
    "    analyze_single_model(df, model1_vars, \"Model 1 (Gamma)\")\n",
    "    \n",
    "    # --- 4. Analyze Model 2 ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model 2 analysis: log(InDegree) ~ Anisotropy_Lambda + jobs_density\")\n",
    "    print(\"=\"*60)\n",
    "    analyze_single_model(df, model2_vars, \"Model 2 (Lambda)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All tests completed.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_beautiful_correlation_matrix():\n",
    "    \"\"\"\n",
    "    Create beautiful correlation matrix visualization with different color schemes\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(f\"Data read successfully, contains {len(df)} observations.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Create log-transformed dependent variable\n",
    "    if 'log_in_degree_weighted' not in df.columns:\n",
    "        df['log_in_degree_weighted'] = np.log1p(df['in_degree_weighted'])\n",
    "\n",
    "    # Define analysis variables and labels\n",
    "    variables = [\n",
    "        'log_in_degree_weighted',\n",
    "        'Centripetality_Gamma_mean', \n",
    "        'Anisotropy_Lambda_mean',\n",
    "        'jobs_density'\n",
    "    ]\n",
    "    \n",
    "    # Beautiful variable labels\n",
    "    variable_labels = {\n",
    "        'log_in_degree_weighted': 'log(In-Degree)',\n",
    "        'Centripetality_Gamma_mean': 'Centripetality (Γ)',\n",
    "        'Anisotropy_Lambda_mean': 'Anisotropy (Λ)',\n",
    "        'jobs_density': 'Job Density'\n",
    "    }\n",
    "    \n",
    "    # Extract data and remove missing values\n",
    "    analysis_data = df[variables].dropna()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = analysis_data.corr()\n",
    "    \n",
    "    # Rename row and column labels\n",
    "    corr_matrix_labeled = corr_matrix.copy()\n",
    "    corr_matrix_labeled.index = [variable_labels[var] for var in corr_matrix.index]\n",
    "    corr_matrix_labeled.columns = [variable_labels[var] for var in corr_matrix.columns]\n",
    "    \n",
    "    # Set figure style\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Modified color scheme\n",
    "    # ============================================================================\n",
    "    sns.heatmap(\n",
    "        corr_matrix_labeled, \n",
    "        annot=True,\n",
    "        cmap='coolwarm',     # Recommended scheme 1: classic red-white-blue color scheme\n",
    "        # cmap='RdBu_r',     # Alternative scheme: another classic red-white-blue\n",
    "        # cmap='PuOr',       # Alternative scheme: soft purple-white-orange\n",
    "        # cmap='vlag',       # Alternative scheme: modern red-gray-blue\n",
    "        center=0,\n",
    "        square=True,\n",
    "        fmt='.3f',\n",
    "        cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        annot_kws={'size': 12, 'weight': 'bold', 'color': 'black'},\n",
    "        vmin=-1, vmax=1\n",
    "    )\n",
    "    \n",
    "    # Beautification settings\n",
    "    ax.set_title('Correlation Matrix (General Mobility Network)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "    plt.yticks(rotation=0, fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display figure\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical matrix\n",
    "    print(\"\\nCorrelation matrix values:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(corr_matrix_labeled.round(3))\n",
    "    \n",
    "    # Significance test\n",
    "    print(\"\\nCorrelation significance test:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, var1 in enumerate(variables):\n",
    "        for j, var2 in enumerate(variables):\n",
    "            if i < j:  # Avoid duplication\n",
    "                corr, p_value = pearsonr(analysis_data[var1], analysis_data[var2])\n",
    "                \n",
    "                # Significance markers\n",
    "                if p_value < 0.001:\n",
    "                    sig_level = \"***\"\n",
    "                elif p_value < 0.01:\n",
    "                    sig_level = \"**\"\n",
    "                elif p_value < 0.05:\n",
    "                    sig_level = \"*\"\n",
    "                else:\n",
    "                    sig_level = \"ns\"\n",
    "                \n",
    "                print(f\"{variable_labels[var1]} ↔ {variable_labels[var2]}:\")\n",
    "                print(f\"   r = {corr:.3f}, p = {p_value:.4f} {sig_level}\")\n",
    "\n",
    "# Run function\n",
    "create_beautiful_correlation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- 1. Load final data file ---\n",
    "try:\n",
    "    df = pd.read_csv('master_analysis_data_final_cleaned.csv')\n",
    "    print(\"--- Final diagnosis: Manual VIF calculation and visualization ---\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Cannot find 'master_analysis_data_final_cleaned.csv'\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Prepare Model 2 independent variables data ---\n",
    "model2_vars = ['Anisotropy_Lambda_mean', 'jobs_density']\n",
    "data = df[model2_vars].dropna()\n",
    "\n",
    "# --- 3. Manual VIF calculation ---\n",
    "# The principle of VIF calculation is: use one independent variable as dependent variable, others as independent variables, perform regression\n",
    "# Then use the R² value from regression to calculate VIF\n",
    "\n",
    "# Model A: Anisotropy ~ Job Density\n",
    "y = data['Anisotropy_Lambda_mean']\n",
    "X = data['jobs_density']\n",
    "X = sm.add_constant(X) # Regression needs to add constant term\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "rsquared = model.rsquared\n",
    "\n",
    "# VIF = 1 / (1 - R²)\n",
    "manual_vif = 1 / (1 - rsquared)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Decisive test results:\")\n",
    "print(f\"  1. R-squared from regression of 'Anisotropy' on 'Job Density': {rsquared:.6f}\")\n",
    "print(f\"  2. Manual VIF calculated from this R-squared: {manual_vif:.6f}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Visualize relationship between two variables ---\n",
    "print(\">>> Generating relationship plot between Anisotropy_Lambda_mean and jobs_density...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Use seaborn's regplot, which plots both scatter plot and linear regression line\n",
    "sns.regplot(\n",
    "    x='jobs_density', \n",
    "    y='Anisotropy_Lambda_mean', \n",
    "    data=data,\n",
    "    scatter_kws={'alpha': 0.6},\n",
    "    line_kws={'color': 'red', 'linewidth': 2}\n",
    ")\n",
    "\n",
    "plt.title('Scatter Plot with Regression Line', fontsize=16)\n",
    "plt.xlabel('Job Density', fontsize=12)\n",
    "plt.ylabel('Anisotropy (Lambda)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Conclusion ---\n",
    "print(\"\\n--- Diagnostic conclusion ---\")\n",
    "print(f\"The correlation coefficient r ≈ 0.06 you saw earlier should theoretically give VIF = 1.004.\")\n",
    "print(f\"Our manually calculated VIF is {manual_vif:.4f}.\")\n",
    "if abs(manual_vif - 1.004) < 0.01:\n",
    "    print(\"Manual calculation results match theoretical values. This indicates that the statsmodels VIF function may behave abnormally in your environment.\")\n",
    "    print(\"   You can adopt the manually calculated VIF value as the final basis. Conclusion: Model 2 has no multicollinearity issues.\")\n",
    "else:\n",
    "    print(\"Manual calculation results do not match theoretical values but are close to VIF function results.\")\n",
    "    print(\"   This indicates that there may be extreme outliers or special distributions in the data, seriously affecting the stability of linear regression. Please carefully examine the scatter plot above for clues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b573a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step can be added to your merge_and_analyze.py or advanced_analysis.py script\n",
    "# Or as a new preparation script\n",
    "\n",
    "import geopandas as gpd\n",
    "# 1. Load your main analysis data\n",
    "master_df = pd.read_csv('master_analysis_data_final.csv')\n",
    "\n",
    "# 2. Load TTWA geographic boundary file\n",
    "TTWA_BOUNDARY_FILE = 'boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson'\n",
    "ttwa_gdf = gpd.read_file(TTWA_BOUNDARY_FILE)\n",
    "\n",
    "# 3. Calculate centroid coordinates for each TTWA\n",
    "ttwa_gdf['centroid'] = ttwa_gdf.geometry.centroid\n",
    "ttwa_gdf['centroid_x'] = ttwa_gdf.centroid.x\n",
    "ttwa_gdf['centroid_y'] = ttwa_gdf.centroid.y\n",
    "\n",
    "# 4. Merge coordinates into main analysis data\n",
    "# We need a common key, here assuming it's TTWA name\n",
    "master_df_geo = pd.merge(master_df, ttwa_gdf[['TTWA11NM', 'centroid_x', 'centroid_y']], left_on='TTWA_Name', right_on='TTWA11NM', how='left')\n",
    "\n",
    "# Check merged data\n",
    "print(master_df_geo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8973a",
   "metadata": {},
   "source": [
    "#### New analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e11081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_regression_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def run_regression_analysis():\n",
    "    \"\"\"\n",
    "    This script loads the final cleaned data and runs two independent multiple linear regression models,\n",
    "    examining the relationship between Gamma and Lambda with in-degree respectively, while controlling for jobs_density.\n",
    "    \"\"\"\n",
    "    print(\"--- Running regression analysis ---\")\n",
    "\n",
    "    # --- 1. File path ---\n",
    "    # This file should be the output from your previous cleaning script\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv' \n",
    "\n",
    "    # --- 2. Read data ---\n",
    "    print(f\"\\n>>> Reading analysis data '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(\"   Data read successfully.\")\n",
    "        print(f\"   Final dataset for analysis contains {len(df)} TTWAs.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        print(\"   Please ensure you have successfully run the data cleaning and merging script and generated this file.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Run first regression model (using Gamma) ---\n",
    "    print(\"\\n>>> Running Model 1: using Centripetality_Gamma_mean...\")\n",
    "    \n",
    "    # Define Model 1 formula (y ~ x1 + x2)\n",
    "    formula1 = 'in_degree_weighted ~ Centripetality_Gamma_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        # Create and fit OLS model\n",
    "        model1 = smf.ols(formula=formula1, data=df).fit()\n",
    "        \n",
    "        # Print complete regression results summary\n",
    "        print(\"\\n--- Model 1: Regression analysis results (using Gamma) ---\")\n",
    "        print(model1.summary())\n",
    "        print(\"------------------------------------------\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running Model 1: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Run second regression model (using Lambda) ---\n",
    "    print(\"\\n>>> Running Model 2: using Anisotropy_Lambda_mean...\")\n",
    "\n",
    "    # Define Model 2 formula\n",
    "    formula2 = 'in_degree_weighted ~ Anisotropy_Lambda_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        # Create and fit OLS model\n",
    "        model2 = smf.ols(formula=formula2, data=df).fit()\n",
    "        \n",
    "        # Print complete regression results summary\n",
    "        print(\"\\n--- Model 2: Regression analysis results (using Lambda) ---\")\n",
    "        print(model2.summary())\n",
    "        print(\"------------------------------------------\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error running Model 2: {e}\")\n",
    "\n",
    "# --- Run main function ---\n",
    "run_regression_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_log_transformed_regression.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  # Import numpy library for mathematical transformations\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def run_log_transformed_regression():\n",
    "    \"\"\"\n",
    "    This script loads the final cleaned data, performs log transformation on the dependent variable 'in_degree_weighted',\n",
    "    then re-runs two multiple linear regression models to obtain more robust results.\n",
    "    \"\"\"\n",
    "    print(\"--- Running log-transformed regression analysis (more robust models) ---\")\n",
    "\n",
    "    # --- 1. File path ---\n",
    "    INPUT_DATA_FILE = 'master_analysis_data_final_cleaned.csv'\n",
    "\n",
    "    # --- 2. Read data ---\n",
    "    print(f\"\\n>>> Reading analysis data '{INPUT_DATA_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_DATA_FILE)\n",
    "        print(\"   Data read successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{INPUT_DATA_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Core improvement: log transform the dependent variable ---\n",
    "    print(\"\\n>>> Log transforming dependent variable 'in_degree_weighted'...\")\n",
    "    \n",
    "    # We use np.log1p for transformation, which calculates log(1 + x).\n",
    "    # This is a professional data processing technique that can robustly handle potential 0 values in original data.\n",
    "    # Using np.log(0) directly would cause errors.\n",
    "    log_target_col = 'log_in_degree_weighted'\n",
    "    original_target_col = 'in_degree_weighted'\n",
    "\n",
    "    if original_target_col in df.columns:\n",
    "        df[log_target_col] = np.log1p(df[original_target_col])\n",
    "        print(f\"   Created new column '{log_target_col}' for analysis.\")\n",
    "    else:\n",
    "        print(f\"Error: Original dependent variable column '{original_target_col}' not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Run first regression model (using Gamma) ---\n",
    "    print(\"\\n>>> Running Model 1 (using Gamma)...\")\n",
    "    \n",
    "    # Note: The dependent variable on the left side of the formula has been changed to the new log-transformed column\n",
    "    formula1 = f'{log_target_col} ~ Centripetality_Gamma_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        model1 = smf.ols(formula=formula1, data=df).fit()\n",
    "        print(\"\\n--- Model 1: Regression analysis results (log-transformed, using Gamma) ---\")\n",
    "        print(model1.summary())\n",
    "        print(\"--------------------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Model 1: {e}\")\n",
    "\n",
    "    # --- 5. Run second regression model (using Lambda) ---\n",
    "    print(\"\\n>>> Running Model 2 (using Lambda)...\")\n",
    "\n",
    "    formula2 = f'{log_target_col} ~ Anisotropy_Lambda_mean + jobs_density'\n",
    "    \n",
    "    try:\n",
    "        model2 = smf.ols(formula=formula2, data=df).fit()\n",
    "        print(\"\\n--- Model 2: Regression analysis results (log-transformed, using Lambda) ---\")\n",
    "        print(model2.summary())\n",
    "        print(\"--------------------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Model 2: {e}\")\n",
    "\n",
    "# --- Run main function ---\n",
    "run_log_transformed_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a477c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran_i_test_final.py\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "\n",
    "def run_final_spatial_diagnostics():\n",
    "    \"\"\"\n",
    "    This script is the final diagnostic step for comprehensive network analysis.\n",
    "    Core logic:\n",
    "    1. Load data and ensure geographic coordinate system is EPSG:2770.\n",
    "    2. Run the final confirmed log-transformed OLS regression model.\n",
    "    3. Extract model residuals.\n",
    "    4. Perform Moran's I spatial autocorrelation test on residuals.\n",
    "    5. Based on test results, clarify next action recommendations (whether to proceed with MGWR).\n",
    "    \"\"\"\n",
    "    print(\"--- Final spatial diagnostics for comprehensive network regression model ---\")\n",
    "\n",
    "    # --- 1. File paths and model configuration ---\n",
    "    ANALYSIS_DATA_FILE = 'master_analysis_data_final_cleaned.csv' \n",
    "    TTWA_BOUNDARY_FILE = \"boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson\"\n",
    "    \n",
    "    # Final confirmed model formula\n",
    "    FINAL_MODEL_FORMULA = 'log_in_degree_weighted ~ Anisotropy_Lambda_mean + jobs_density'\n",
    "\n",
    "    # --- 2. Load, transform coordinate system and merge data ---\n",
    "    print(\"\\n>>> Loading data and setting correct coordinate system (EPSG:2770)...\")\n",
    "    try:\n",
    "        df_analysis = pd.read_csv(ANALYSIS_DATA_FILE)\n",
    "        gdf_ttwa = gpd.read_file(TTWA_BOUNDARY_FILE)\n",
    "        \n",
    "        # Core step: Transform geographic data to EPSG:2770\n",
    "        gdf_ttwa_projected = gdf_ttwa.to_crs('EPSG:2770')\n",
    "        \n",
    "        # Merge analysis data to geographic data\n",
    "        gdf_analysis = gdf_ttwa_projected.merge(df_analysis, left_on='TTWA11NM', right_on='TTWA_Name', how='inner')\n",
    "        \n",
    "        print(f\"   Data loading, coordinate system transformation and merging successful, {len(gdf_analysis)} TTWAs for spatial analysis.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File {e.filename} not found. Please check path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading or merging: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Run final OLS regression model and extract residuals ---\n",
    "    print(\"\\n>>> Running final log-transformed OLS model to obtain residuals...\")\n",
    "    \n",
    "    # Core step: Log transform dependent variable\n",
    "    gdf_analysis['log_in_degree_weighted'] = np.log1p(gdf_analysis['in_degree_weighted'])\n",
    "    \n",
    "    try:\n",
    "        model = smf.ols(formula=FINAL_MODEL_FORMULA, data=gdf_analysis).fit()\n",
    "        gdf_analysis['residuals'] = model.resid\n",
    "        print(\"   OLS model run completed, residuals successfully extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running OLS model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Build spatial weight matrix (W) ---\n",
    "    print(\"\\n>>> Building spatial weight matrix (W) based on adjacency relationships...\")\n",
    "    try:\n",
    "        # Use Queen Contiguity: neighbors if they share even one vertex\n",
    "        w = Queen.from_dataframe(gdf_analysis)\n",
    "        w.transform = 'r' # Row standardization\n",
    "        print(\"   Spatial weight matrix (W) built successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error building spatial weight matrix: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 5. Calculate Moran's I ---\n",
    "    print(\"\\n>>> Performing Moran's I test on model residuals...\")\n",
    "    try:\n",
    "        moran = Moran(gdf_analysis['residuals'], w)\n",
    "        print(\"   Moran's I calculation completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Moran's I: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 6. Report results and final recommendations ---\n",
    "    print(\"\\n--- Spatial autocorrelation test results ---\")\n",
    "    print(f\"Moran's I statistic: {moran.I:.4f}\")\n",
    "    print(f\"p-value (simulation): {moran.p_sim:.4f}\")\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "    print(\"\\n--- Final action recommendations ---\")\n",
    "    if moran.p_sim < 0.05:\n",
    "        print(\"**Conclusion: p-value less than 0.05, OLS model residuals show significant spatial autocorrelation (spatial clustering).**\")\n",
    "        print(\"   This means your global model failed to capture all spatial patterns, and the 'internal form-external status' relationship varies across different regions in the UK.\")\n",
    "        print(\"   **Strongly recommend proceeding to next step: Use MGWR to explore and explain this interesting spatial heterogeneity.**\")\n",
    "    else:\n",
    "        print(\"**Conclusion: p-value greater than or equal to 0.05, OLS model residuals are randomly distributed in space.**\")\n",
    "        print(\"   This means your global OLS model has well explained the variable relationships, which are relatively stable spatially.\")\n",
    "        print(\"   **You do not need to proceed with MGWR analysis, current OLS results are sufficient as final, robust conclusions.**\")\n",
    "\n",
    "# --- Run main function ---\n",
    "run_final_spatial_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran_i_test_for_gamma_model.py\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "\n",
    "def run_spatial_diagnostics_for_gamma():\n",
    "    \"\"\"\n",
    "    This script specifically performs spatial diagnostics for the Gamma model.\n",
    "    Core logic:\n",
    "    1. Load data and ensure geographic coordinate system is EPSG:2770.\n",
    "    2. Run log-transformed OLS regression model using Gamma.\n",
    "    3. Extract model residuals.\n",
    "    4. Perform Moran's I spatial autocorrelation test on residuals.\n",
    "    5. Report test results.\n",
    "    \"\"\"\n",
    "    print(\"--- Spatial diagnostics for Gamma model regression residuals ---\")\n",
    "\n",
    "    # --- 1. File paths and model configuration ---\n",
    "    ANALYSIS_DATA_FILE = 'master_analysis_data_final_cleaned.csv' \n",
    "    TTWA_BOUNDARY_FILE = \"boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson\"\n",
    "    \n",
    "    # Core difference: model formula using Gamma\n",
    "    MODEL_FORMULA_GAMMA = 'log_in_degree_weighted ~ Centripetality_Gamma_mean + jobs_density'\n",
    "\n",
    "    # --- 2. Load, transform coordinate system and merge data ---\n",
    "    print(\"\\n>>> Loading data and setting correct coordinate system (EPSG:2770)...\")\n",
    "    try:\n",
    "        df_analysis = pd.read_csv(ANALYSIS_DATA_FILE)\n",
    "        gdf_ttwa = gpd.read_file(TTWA_BOUNDARY_FILE)\n",
    "        \n",
    "        gdf_ttwa_projected = gdf_ttwa.to_crs('EPSG:2770')\n",
    "        gdf_analysis = gdf_ttwa_projected.merge(df_analysis, left_on='TTWA11NM', right_on='TTWA_Name', how='inner')\n",
    "        \n",
    "        print(f\"   Data loading and merging successful, {len(gdf_analysis)} TTWAs for spatial analysis.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File {e.filename} not found. Please check path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading or merging: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Run OLS regression and extract residuals ---\n",
    "    print(\"\\n>>> Running Gamma OLS model to obtain residuals...\")\n",
    "    \n",
    "    gdf_analysis['log_in_degree_weighted'] = np.log1p(gdf_analysis['in_degree_weighted'])\n",
    "    \n",
    "    try:\n",
    "        model = smf.ols(formula=MODEL_FORMULA_GAMMA, data=gdf_analysis).fit()\n",
    "        gdf_analysis['residuals_gamma'] = model.resid\n",
    "        print(\"   OLS model (Gamma) run completed, residuals successfully extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running OLS model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Build spatial weight matrix (W) ---\n",
    "    print(\"\\n>>> Building spatial weight matrix (W)...\")\n",
    "    try:\n",
    "        w = Queen.from_dataframe(gdf_analysis)\n",
    "        w.transform = 'r'\n",
    "        print(\"   Spatial weight matrix (W) built successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error building spatial weight matrix: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 5. Calculate Moran's I ---\n",
    "    print(\"\\n>>> Performing Moran's I test on Gamma model residuals...\")\n",
    "    try:\n",
    "        moran_gamma = Moran(gdf_analysis['residuals_gamma'], w)\n",
    "        print(\"   Moran's I calculation completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Moran's I: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 6. Report results ---\n",
    "    print(\"\\n--- Spatial autocorrelation test results (Gamma model) ---\")\n",
    "    print(f\"Moran's I statistic: {moran_gamma.I:.4f}\")\n",
    "    print(f\"p-value (simulation): {moran_gamma.p_sim:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    if moran_gamma.p_sim < 0.05:\n",
    "        print(\"**Conclusion: p-value less than 0.05, Gamma model residuals also show significant spatial autocorrelation.**\")\n",
    "        print(\"   This provides further, independent evidence for your need to use MGWR and other spatial models.\")\n",
    "    else:\n",
    "        print(\"**Conclusion: p-value greater than or equal to 0.05, Gamma model residuals are randomly distributed in space.**\")\n",
    "\n",
    "\n",
    "# --- Run main function ---\n",
    "run_spatial_diagnostics_for_gamma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef69709",
   "metadata": {},
   "source": [
    "##### MGWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc38b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from mgwr.gwr import GWR, MGWR\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mgwr_visualization(gdf, independent_vars, model_name, mgwr_results=None):\n",
    "    \"\"\"\n",
    "    Create MGWR coefficient maps and local R² maps with improved color schemes\n",
    "    \"\"\"\n",
    "    print(f\"Step 3/3: Creating MGWR visualization results...\")\n",
    "    \n",
    "    # Calculate local R² if mgwr_results is provided\n",
    "    if mgwr_results is not None:\n",
    "        print(\"Calculating local R² using alternative method...\")\n",
    "        \n",
    "        # Method 1: Use residuals to calculate local R²\n",
    "        try:\n",
    "            y_obs = gdf['log_in_degree_weighted'].values\n",
    "            y_pred = mgwr_results.mu.flatten() if hasattr(mgwr_results, 'mu') else None\n",
    "            \n",
    "            if y_pred is None:\n",
    "                # Try to get predicted values from fitted values\n",
    "                if hasattr(mgwr_results, 'fitted_values'):\n",
    "                    y_pred = mgwr_results.fitted_values.flatten()\n",
    "                elif hasattr(mgwr_results, 'predy'):\n",
    "                    y_pred = mgwr_results.predy.flatten()\n",
    "                else:\n",
    "                    # Manual calculation using parameters\n",
    "                    X = np.column_stack([np.ones(len(gdf)), \n",
    "                                        gdf[independent_vars[0]].values,\n",
    "                                        gdf[independent_vars[1]].values])\n",
    "                    y_pred = np.sum(mgwr_results.params * X, axis=1)\n",
    "            \n",
    "            # Calculate local R² using moving window approach\n",
    "            from scipy.spatial.distance import cdist\n",
    "            centroids = gdf.geometry.centroid\n",
    "            coords = np.column_stack([centroids.x, centroids.y])\n",
    "            \n",
    "            # Calculate distance matrix\n",
    "            distances = cdist(coords, coords)\n",
    "            \n",
    "            # Use adaptive bandwidth for local R² calculation\n",
    "            bandwidths = distances.mean(axis=1) * 0.5  # Adaptive local window\n",
    "            \n",
    "            local_r2 = []\n",
    "            for i in range(len(y_obs)):\n",
    "                # Find neighbors within bandwidth\n",
    "                neighbors = distances[i] <= bandwidths[i]\n",
    "                if neighbors.sum() > 3:  # Need at least 4 points\n",
    "                    y_local = y_obs[neighbors]\n",
    "                    y_pred_local = y_pred[neighbors]\n",
    "                    \n",
    "                    # Calculate local R²\n",
    "                    ss_res = np.sum((y_local - y_pred_local) ** 2)\n",
    "                    ss_tot = np.sum((y_local - y_local.mean()) ** 2)\n",
    "                    r2_local = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "                    local_r2.append(max(0, min(1, r2_local)))  # Bound between 0 and 1\n",
    "                else:\n",
    "                    local_r2.append(0.5)  # Default value for isolated points\n",
    "            \n",
    "            gdf[f'mgwr_local_R2_{model_name}'] = local_r2\n",
    "            print(f\"   Local R² calculated successfully (range: {np.min(local_r2):.3f} - {np.max(local_r2):.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Failed to calculate local R²: {e}\")\n",
    "            # Fallback: use global R² as approximation\n",
    "            global_r2 = mgwr_results.R2 if hasattr(mgwr_results, 'R2') else 0.5\n",
    "            gdf[f'mgwr_local_R2_{model_name}'] = global_r2\n",
    "            print(f\"   Using global R² ({global_r2:.3f}) as approximation\")\n",
    "    \n",
    "    # Define custom elegant color schemes\n",
    "    def get_diverging_colormap(data_values):\n",
    "        \"\"\"Get appropriate elegant diverging colormap based on data range\"\"\"\n",
    "        vmin, vmax = np.min(data_values), np.max(data_values)\n",
    "        \n",
    "        if vmin >= 0:\n",
    "            # All positive values: use elegant warm colors (light peach to coral)\n",
    "            return 'Oranges', vmin, vmax\n",
    "        elif vmax <= 0:\n",
    "            # All negative values: use elegant cool colors (light blue to navy)\n",
    "            return 'Blues', vmin, vmax\n",
    "        else:\n",
    "            # Mixed values: use elegant diverging colormap (soft blue-white-pink)\n",
    "            abs_max = max(abs(vmin), abs(vmax))\n",
    "            return 'RdYlBu_r', -abs_max, abs_max\n",
    "    \n",
    "    # Create figure with coefficients and local R²\n",
    "    n_vars = len(independent_vars)\n",
    "    fig, axes = plt.subplots(2, n_vars + 1, figsize=(6*(n_vars + 1), 12))\n",
    "    \n",
    "    if n_vars == 1:\n",
    "        axes = axes.reshape(-1, 2)\n",
    "    \n",
    "    # Plot coefficient maps with improved color schemes\n",
    "    for i, var in enumerate(independent_vars):\n",
    "        coeff_col = f'mgwr_coeff_{var}_{model_name}'\n",
    "        if coeff_col in gdf.columns:\n",
    "            coeff_values = gdf[coeff_col].values\n",
    "            \n",
    "            # Get appropriate colormap and normalization\n",
    "            cmap, vmin, vmax = get_diverging_colormap(coeff_values)\n",
    "            \n",
    "            # Coefficient map with elegant colors\n",
    "            im = gdf.plot(column=coeff_col, ax=axes[0, i], legend=True,\n",
    "                         cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                         legend_kwds={'shrink': 0.6, 'label': f'{var} Coefficient', 'aspect': 15},\n",
    "                         edgecolor='lightgray', linewidth=0.2, alpha=0.8)\n",
    "            \n",
    "            axes[0, i].set_title(f'MGWR: {var} Coefficient', fontweight='bold', fontsize=14)\n",
    "            axes[0, i].set_axis_off()\n",
    "            \n",
    "            # Enhanced statistics text with better formatting\n",
    "            mean_coeff = np.mean(coeff_values)\n",
    "            std_coeff = np.std(coeff_values)\n",
    "            median_coeff = np.median(coeff_values)\n",
    "            \n",
    "            # Color-code the statistics based on mean value (softer colors)\n",
    "            text_color = '#8B4513' if mean_coeff > 0 else '#4682B4'  # Saddle brown / Steel blue\n",
    "            bg_color = '#FFF8DC' if mean_coeff > 0 else '#F0F8FF'    # Cornsilk / Alice blue\n",
    "            \n",
    "            stats_text = f'Mean: {mean_coeff:.4f}\\nMedian: {median_coeff:.4f}\\nStd: {std_coeff:.4f}\\nRange: [{np.min(coeff_values):.4f}, {np.max(coeff_values):.4f}]'\n",
    "            \n",
    "            axes[1, i].text(0.5, 0.5, stats_text,\n",
    "                           transform=axes[1, i].transAxes, ha='center', va='center', \n",
    "                           fontsize=11, color=text_color,\n",
    "                           bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=bg_color, alpha=0.8))\n",
    "            axes[1, i].set_title(f'{var} Statistics', fontweight='bold', fontsize=14, color=text_color)\n",
    "            axes[1, i].set_axis_off()\n",
    "    \n",
    "    # Plot local R² map with warm color scheme\n",
    "    r2_col = f'mgwr_local_R2_{model_name}'\n",
    "    if r2_col in gdf.columns:\n",
    "        r2_values = gdf[r2_col].values\n",
    "        \n",
    "        # Use elegant warm colors for R² (cream to soft orange)\n",
    "        gdf.plot(column=r2_col, ax=axes[0, -1], legend=True,\n",
    "                 cmap='YlOrRd', legend_kwds={'shrink': 0.6, 'label': 'Local R²', 'aspect': 15},\n",
    "                 edgecolor='lightgray', linewidth=0.2, alpha=0.8)\n",
    "        axes[0, -1].set_title(f'MGWR: Local R²', fontweight='bold', fontsize=14, color='#CD853F')\n",
    "        axes[0, -1].set_axis_off()\n",
    "        \n",
    "        # R² statistics with enhanced formatting\n",
    "        mean_r2 = np.mean(r2_values)\n",
    "        global_r2 = mgwr_results.R2 if mgwr_results else \"N/A\"\n",
    "        median_r2 = np.median(r2_values)\n",
    "        \n",
    "        r2_text = f'Global R²: {global_r2:.4f}\\nMean Local R²: {mean_r2:.4f}\\nMedian Local R²: {median_r2:.4f}\\nLocal R² Range: [{np.min(r2_values):.4f}, {np.max(r2_values):.4f}]'\n",
    "        \n",
    "        axes[1, -1].text(0.5, 0.5, r2_text,\n",
    "                        transform=axes[1, -1].transAxes, ha='center', va='center', \n",
    "                        fontsize=11, color='#CD853F',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"#FFFACD\", alpha=0.8))\n",
    "        axes[1, -1].set_title('Model Performance', fontweight='bold', fontsize=14, color='#CD853F')\n",
    "        axes[1, -1].set_axis_off()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'mgwr_analysis_with_R2_{model_name}_elegant.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Elegant MGWR analysis with R² map saved: mgwr_analysis_with_R2_{model_name}_elegant.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create separate enhanced local R² map\n",
    "    if r2_col in gdf.columns:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Enhanced R² map with soft gradient background\n",
    "        gdf.plot(column=r2_col, ax=ax, legend=True, cmap='Oranges', \n",
    "                 legend_kwds={'shrink': 0.5, 'label': 'Local R²', 'orientation': 'horizontal', \n",
    "                             'pad': 0.05, 'aspect': 20},\n",
    "                 edgecolor='lightgray', linewidth=0.3, alpha=0.85)\n",
    "        \n",
    "        ax.set_title(f'MGWR Local R² Map - {model_name} Model', \n",
    "                    fontweight='bold', fontsize=18, color='#CD853F', pad=20)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Enhanced text box with gradient background\n",
    "        r2_stats = f'Global R²: {global_r2:.4f}\\nMean Local R²: {mean_r2:.4f}\\nMedian Local R²: {median_r2:.4f}\\nStd Local R²: {np.std(r2_values):.4f}\\nMin-Max: {np.min(r2_values):.3f} - {np.max(r2_values):.3f}'\n",
    "        \n",
    "        # Create a more sophisticated text box with elegant colors\n",
    "        props = dict(boxstyle='round,pad=0.5', facecolor='#FFFACD', alpha=0.9, \n",
    "                    edgecolor='#DEB887', linewidth=2)\n",
    "        ax.text(0.02, 0.98, r2_stats, transform=ax.transAxes, fontsize=13,\n",
    "                verticalalignment='top', bbox=props, color='#8B7355', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'map_local_R2_{model_name}_elegant.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Elegant local R² map saved: map_local_R2_{model_name}_elegant.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Create individual coefficient maps with enhanced styling\n",
    "    for i, var in enumerate(independent_vars):\n",
    "        coeff_col = f'mgwr_coeff_{var}_{model_name}'\n",
    "        if coeff_col in gdf.columns:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "            \n",
    "            coeff_values = gdf[coeff_col].values\n",
    "            cmap, vmin, vmax = get_diverging_colormap(coeff_values)\n",
    "            \n",
    "            # Enhanced individual coefficient map with elegant styling\n",
    "            gdf.plot(column=coeff_col, ax=ax, legend=True,\n",
    "                     cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                     legend_kwds={'shrink': 0.5, 'label': f'{var} Coefficient', \n",
    "                                 'orientation': 'horizontal', 'pad': 0.05, 'aspect': 20},\n",
    "                     edgecolor='lightgray', linewidth=0.3, alpha=0.85)\n",
    "            \n",
    "            # Determine title color based on coefficient mean (elegant colors)\n",
    "            title_color = '#B8860B' if np.mean(coeff_values) > 0 else '#5F9EA0'  # Dark goldenrod / Cadet blue\n",
    "            \n",
    "            ax.set_title(f'MGWR Coefficient Map: {var} - {model_name} Model', \n",
    "                        fontweight='bold', fontsize=18, color=title_color, pad=20)\n",
    "            ax.set_axis_off()\n",
    "            \n",
    "            # Enhanced statistics box with elegant colors\n",
    "            mean_coeff = np.mean(coeff_values)\n",
    "            stats_text = f'Mean: {mean_coeff:.4f}\\nMedian: {np.median(coeff_values):.4f}\\nStd: {np.std(coeff_values):.4f}\\nRange: [{np.min(coeff_values):.4f}, {np.max(coeff_values):.4f}]'\n",
    "            \n",
    "            bg_color = '#FFFACD' if mean_coeff > 0 else '#F0F8FF'  # Lemon chiffon / Alice blue\n",
    "            text_color = '#8B7355' if mean_coeff > 0 else '#4682B4'  # Dark khaki / Steel blue\n",
    "            edge_color = '#DEB887' if mean_coeff > 0 else '#87CEEB'  # Burlywood / Sky blue\n",
    "            \n",
    "            props = dict(boxstyle='round,pad=0.5', facecolor=bg_color, alpha=0.9, \n",
    "                        edgecolor=edge_color, linewidth=2)\n",
    "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=13,\n",
    "                    verticalalignment='top', bbox=props, color=text_color, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            clean_var_name = var.replace('_', ' ').replace('mean', 'Mean')\n",
    "            plt.savefig(f'map_coeff_{clean_var_name}_{model_name}_elegant.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Elegant coefficient map saved: map_coeff_{clean_var_name}_{model_name}_elegant.png\")\n",
    "            plt.close()\n",
    "\n",
    "def run_compatible_mgwr(gdf, dependent_var, independent_vars, model_name):\n",
    "    \"\"\"\n",
    "    More compatible MGWR implementation with proper results return\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Compatibility MGWR Analysis ({model_name}) {'='*20}\")\n",
    "    \n",
    "    # --- 1. Data Preparation ---\n",
    "    y = gdf[dependent_var].values.reshape(-1, 1)\n",
    "    X = gdf[independent_vars].values\n",
    "    \n",
    "    # Add intercept term\n",
    "    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    # Calculate centroid coordinates\n",
    "    centroids = gdf.geometry.centroid\n",
    "    coords = np.column_stack([centroids.x, centroids.y])\n",
    "    \n",
    "    print(f\"Data shapes - Y: {y.shape}, X: {X_with_intercept.shape}, Coordinates: {coords.shape}\")\n",
    "    \n",
    "    # --- 2. Smart Bandwidth Strategy ---\n",
    "    print(\">>> Using smart multiscale bandwidth strategy...\")\n",
    "    \n",
    "    try:\n",
    "        # Strategy 1: Try native multiscale selection directly\n",
    "        print(\"   Strategy 1: Trying native MGWR multiscale...\")\n",
    "        bw_selector = Sel_BW(coords, y, X_with_intercept, multi=True)\n",
    "        multi_bws = bw_selector.search(verbose=False)\n",
    "        \n",
    "        # Check return value type\n",
    "        if hasattr(multi_bws, '__len__') and len(multi_bws) == X_with_intercept.shape[1]:\n",
    "            bws = list(multi_bws)\n",
    "            print(f\"   Successfully obtained multiscale bandwidth: {bws}\")\n",
    "            mgwr_method = \"native_multi\"\n",
    "        else:\n",
    "            raise ValueError(\"Mismatch in bandwidth count\")\n",
    "            \n",
    "    except Exception as e1:\n",
    "        print(f\"   Strategy 1 failed: {e1}\")\n",
    "        \n",
    "        try:\n",
    "            # Strategy 2: Stepwise obtain bandwidth for different variables\n",
    "            print(\"   Strategy 2: Stepwise multiscale bandwidth selection...\")\n",
    "            bws = []\n",
    "            \n",
    "            # Overall model bandwidth as baseline\n",
    "            base_selector = Sel_BW(coords, y, X_with_intercept)\n",
    "            base_bw = base_selector.search()\n",
    "            \n",
    "            # Use larger bandwidth for intercept (global trend)\n",
    "            bws.append(base_bw * 2.0)\n",
    "            \n",
    "            # Calculate bandwidth for each independent variable\n",
    "            for i, var in enumerate(independent_vars):\n",
    "                # Create matrix with intercept and current variable\n",
    "                X_subset = np.column_stack([np.ones(X.shape[0]), X[:, i]])\n",
    "                var_selector = Sel_BW(coords, y, X_subset)\n",
    "                var_bw = var_selector.search()\n",
    "                \n",
    "                # Adjust bandwidth to reflect variable characteristics\n",
    "                if i == 0:  # First variable (usually the main explanatory variable)\n",
    "                    adjusted_bw = var_bw * 0.7  # More local influence\n",
    "                else:  # Second variable\n",
    "                    adjusted_bw = var_bw * 1.1  # Slightly more global influence\n",
    "                    \n",
    "                bws.append(adjusted_bw)\n",
    "                print(f\"   {var} bandwidth: {adjusted_bw:.2f}\")\n",
    "                \n",
    "            print(f\"   Stepwise multiscale bandwidth: {bws}\")\n",
    "            mgwr_method = \"stepwise_multi\"\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   Strategy 2 also failed: {e2}\")\n",
    "            \n",
    "            # Strategy 3: Fixed strategy based on data density\n",
    "            print(\"   Strategy 3: Adaptive fixed bandwidth strategy...\")\n",
    "            n_points = coords.shape[0]\n",
    "            \n",
    "            # Calculate average nearest neighbor distance\n",
    "            from scipy.spatial.distance import pdist\n",
    "            distances = pdist(coords)\n",
    "            avg_distance = np.mean(distances)\n",
    "            \n",
    "            # Adapt based on data density\n",
    "            density_factor = np.sqrt(n_points / 100)  # Normalize to 100 points\n",
    "            \n",
    "            bws = [\n",
    "                avg_distance * density_factor * 1.5,  # Intercept - global\n",
    "                avg_distance * density_factor * 0.6,  # First variable - local\n",
    "                avg_distance * density_factor * 1.0   # Second variable - medium\n",
    "            ]\n",
    "            \n",
    "            print(f\"   Adaptive bandwidth strategy: {bws}\")\n",
    "            mgwr_method = \"adaptive_fixed\"\n",
    "    \n",
    "    # --- Fix: Ensure bws is list or np.ndarray ---\n",
    "    if not isinstance(bws, (list, np.ndarray)) or len(bws) != X_with_intercept.shape[1]:\n",
    "        bws = [bws] * X_with_intercept.shape[1]\n",
    "\n",
    "    # --- 3. Run MGWR Model ---\n",
    "    print(f\">>> Fitting MGWR model using {mgwr_method} method...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure bandwidth is positive\n",
    "        bws = [max(bw, 1.0) for bw in bws]\n",
    "        \n",
    "        # Use selector object instead of bandwidth list\n",
    "        selector = Sel_BW(coords, y, X_with_intercept, multi=True)\n",
    "        selector.search(verbose=True)  # Execute bandwidth search first, results will be saved internally\n",
    "        mgwr_model = MGWR(coords, y, X_with_intercept, selector)\n",
    "        mgwr_results = mgwr_model.fit()\n",
    "        print(\"   MGWR model fitted successfully!\")\n",
    "        print(f\"   Global R²: {mgwr_results.R2:.4f}\")\n",
    "        print(f\"   Adjusted R²: {getattr(mgwr_results, 'adj_R2', 'N/A')}\")\n",
    "        print(f\"   AIC: {mgwr_results.aic:.2f}\")\n",
    "        print(f\"   Effective degrees of freedom: {mgwr_results.tr_S:.2f}\")\n",
    "        \n",
    "        # Store coefficients\n",
    "        param_names = ['Intercept'] + independent_vars\n",
    "        for i, param in enumerate(param_names):\n",
    "            coeff_col = f'mgwr_coeff_{param}_{model_name}'\n",
    "            gdf[coeff_col] = mgwr_results.params[:, i]\n",
    "            coeff_mean = mgwr_results.params[:, i].mean()\n",
    "            coeff_std = mgwr_results.params[:, i].std()\n",
    "            coeff_range = mgwr_results.params[:, i].max() - mgwr_results.params[:, i].min()\n",
    "            print(f\"   {param} coefficient statistics:\")\n",
    "            print(f\"     Mean: {coeff_mean:.4f}, Std: {coeff_std:.4f}, Range: {coeff_range:.4f}\")\n",
    "        \n",
    "        # Store bandwidth info\n",
    "        if hasattr(selector, 'bw'):\n",
    "            if isinstance(selector.bw, (list, np.ndarray)):\n",
    "                bandwidths = {param: selector.bw[i] for i, param in enumerate(param_names)}\n",
    "            else:\n",
    "                bandwidths = {param: selector.bw for param in param_names}\n",
    "            print(f\"Variable bandwidths: {bandwidths}\")\n",
    "            # Store bandwidth information as string\n",
    "            gdf[f'mgwr_bandwidth_{model_name}'] = str(bandwidths)\n",
    "        else:\n",
    "            print(\"No bandwidth information found\")\n",
    "            gdf[f'mgwr_bandwidth_{model_name}'] = \"Not available\"\n",
    "        \n",
    "        gdf[f'mgwr_method_{model_name}'] = mgwr_method\n",
    "        return mgwr_results, mgwr_method\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MGWR fitting failed: {e}\")\n",
    "        print(\"Detailed error information:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def compare_gwr_mgwr(gdf, dependent_var, independent_vars, model_name):\n",
    "    \"\"\"\n",
    "    Compare GWR and MGWR results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} GWR vs MGWR Comparison Analysis ({model_name}) {'='*20}\")\n",
    "    \n",
    "    y = gdf[dependent_var].values.reshape(-1, 1)\n",
    "    X = gdf[independent_vars].values\n",
    "    centroids = gdf.geometry.centroid\n",
    "    coords = np.column_stack([centroids.x, centroids.y])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Standard GWR\n",
    "    print(\">>> Running standard GWR...\")\n",
    "    try:\n",
    "        bw_gwr = Sel_BW(coords, y, X).search()\n",
    "        gwr_model = GWR(coords, y, X, bw_gwr).fit()\n",
    "        \n",
    "        results['GWR'] = {\n",
    "            'model': gwr_model,\n",
    "            'R2': gwr_model.R2,\n",
    "            'AIC': gwr_model.aic,\n",
    "            'bandwidth': bw_gwr\n",
    "        }\n",
    "        \n",
    "        print(f\"   GWR - R²: {gwr_model.R2:.4f}, AIC: {gwr_model.aic:.2f}, Bandwidth: {bw_gwr:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   GWR failed: {e}\")\n",
    "    \n",
    "    # 2. MGWR\n",
    "    print(\">>> Running MGWR...\")\n",
    "    mgwr_results, mgwr_method = run_compatible_mgwr(gdf, dependent_var, independent_vars, model_name)\n",
    "    \n",
    "    if mgwr_results:\n",
    "        results['MGWR'] = {\n",
    "            'model': mgwr_results,\n",
    "            'R2': mgwr_results.R2,\n",
    "            'AIC': mgwr_results.aic,\n",
    "            'method': mgwr_method\n",
    "        }\n",
    "        \n",
    "        # Generate visualization with the results\n",
    "        create_mgwr_visualization(gdf, independent_vars, model_name, mgwr_results)\n",
    "    \n",
    "    # 3. Compare results\n",
    "    if len(results) > 1:\n",
    "        print(f\"\\n>>> Model Comparison Results:\")\n",
    "        print(f\"{'Model':<10} {'R²':<10} {'AIC':<10} {'Note'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for model_name_comp, result in results.items():\n",
    "            note = \"Better fit\" if result['R2'] == max(r['R2'] for r in results.values()) else \"\"\n",
    "            print(f\"{model_name_comp:<10} {result['R2']:<10.4f} {result['AIC']:<10.2f} {note}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # --- Environment Check ---\n",
    "    print(\"=== MGWR Analysis Environment Check ===\")\n",
    "    mgwr_available = check_mgwr_version()\n",
    "    \n",
    "    if not mgwr_available:\n",
    "        print(\"MGWR compatibility issues detected, will use compatibility mode\")\n",
    "    else:\n",
    "        print(\"MGWR environment check passed\")\n",
    "    \n",
    "    # --- File Path Configuration ---\n",
    "    ANALYSIS_DATA_FILE = 'master_analysis_data_final_cleaned.csv' \n",
    "    TTWA_BOUNDARY_FILE = \"boundary/Travel_to_Work_Areas_Dec_2011_FCB_in_United_Kingdom_2022.geojson\"\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"\\n=== Data Loading and Preprocessing ===\")\n",
    "    try:\n",
    "        df_analysis = pd.read_csv(ANALYSIS_DATA_FILE)\n",
    "        gdf_ttwa = gpd.read_file(TTWA_BOUNDARY_FILE)\n",
    "        \n",
    "        # Use British National Grid coordinate system\n",
    "        gdf_ttwa_projected = gdf_ttwa.to_crs('EPSG:27700')\n",
    "        gdf_master = gdf_ttwa_projected.merge(df_analysis, left_on='TTWA11NM', right_on='TTWA_Name', how='inner')\n",
    "        \n",
    "        # Log transformation\n",
    "        gdf_master['log_in_degree_weighted'] = np.log1p(gdf_master['in_degree_weighted'])\n",
    "        \n",
    "        print(f\"Data preparation completed, {len(gdf_master)} TTWAs in total\")\n",
    "        print(f\"Dependent variable statistics: mean={gdf_master['log_in_degree_weighted'].mean():.3f}, std={gdf_master['log_in_degree_weighted'].std():.3f}\")\n",
    "        \n",
    "        # Check data completeness\n",
    "        for var in ['Anisotropy_Lambda_mean', 'Centripetality_Gamma_mean', 'jobs_density']:\n",
    "            if var in gdf_master.columns:\n",
    "                missing = gdf_master[var].isna().sum()\n",
    "                if missing > 0:\n",
    "                    print(f\"Warning: {var} has {missing} missing values\")\n",
    "                    gdf_master[var] = gdf_master[var].fillna(gdf_master[var].median())\n",
    "                    print(f\"Filled missing values in {var} with median\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Data loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- MGWR Analysis ---\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Starting MGWR multiscale geographically weighted regression analysis\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Lambda model MGWR analysis\n",
    "    print(\"\\nLambda Model MGWR Analysis\")\n",
    "    gdf_lambda = gdf_master.copy()\n",
    "    compare_gwr_mgwr(\n",
    "        gdf=gdf_lambda,\n",
    "        dependent_var='log_in_degree_weighted',\n",
    "        independent_vars=['Anisotropy_Lambda_mean', 'jobs_density'],\n",
    "        model_name='Lambda'\n",
    "    )\n",
    "    \n",
    "    # Save Lambda model results\n",
    "    lambda_output = 'mgwr_lambda_results.geojson'\n",
    "    mgwr_cols = [col for col in gdf_lambda.columns if 'mgwr_' in col or col in ['geometry', 'TTWA11NM', 'TTWA_Name']]\n",
    "    gdf_lambda[mgwr_cols].to_file(lambda_output, driver='GeoJSON')\n",
    "    print(f\"Lambda MGWR results saved: {lambda_output}\")\n",
    "    \n",
    "    # Gamma model MGWR analysis\n",
    "    print(\"\\nGamma Model MGWR Analysis\")\n",
    "    gdf_gamma = gdf_master.copy()\n",
    "    compare_gwr_mgwr(\n",
    "        gdf=gdf_gamma,\n",
    "        dependent_var='log_in_degree_weighted',\n",
    "        independent_vars=['Centripetality_Gamma_mean', 'jobs_density'],\n",
    "        model_name='Gamma'\n",
    "    )\n",
    "    \n",
    "    # Save Gamma model results\n",
    "    gamma_output = 'mgwr_gamma_results.geojson'\n",
    "    mgwr_cols = [col for col in gdf_gamma.columns if 'mgwr_' in col or col in ['geometry', 'TTWA11NM', 'TTWA_Name']]\n",
    "    gdf_gamma[mgwr_cols].to_file(gamma_output, driver='GeoJSON')\n",
    "    print(f\"Gamma MGWR results saved: {gamma_output}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MGWR analysis completed!\")\n",
    "    print(\"Generated elegant color scheme map files:\")\n",
    "    print(\"Comprehensive analysis charts:\")\n",
    "    print(\"   - mgwr_analysis_with_R2_Lambda_elegant.png\")\n",
    "    print(\"   - mgwr_analysis_with_R2_Gamma_elegant.png\")\n",
    "    print(\"Elegant local R² maps:\")\n",
    "    print(\"   - map_local_R2_Lambda_elegant.png\")\n",
    "    print(\"   - map_local_R2_Gamma_elegant.png\")\n",
    "    print(\"Elegant coefficient maps:\")\n",
    "    print(\"   - map_coeff_Anisotropy Lambda Mean_Lambda_elegant.png\")\n",
    "    print(\"   - map_coeff_jobs density_Lambda_elegant.png\")\n",
    "    print(\"   - map_coeff_Centripetality Gamma Mean_Gamma_elegant.png\")\n",
    "    print(\"   - map_coeff_jobs density_Gamma_elegant.png\")\n",
    "    print(\"Complete result files:\")\n",
    "    print(\"   - mgwr_lambda_results.geojson\")\n",
    "    print(\"   - mgwr_gamma_results.geojson\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
